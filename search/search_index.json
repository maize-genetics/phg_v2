{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at zrm22@cornell.edu. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"},{"location":"CONTRIBUTING/","title":"Contributing to PHG_v2","text":"<p>Thank you for your interest in contributing to PHG_v2! We welcome contributions from anyone, and are grateful for even the smallest of fixes!</p>"},{"location":"CONTRIBUTING/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Code of Conduct</li> <li>Getting Started</li> <li>How to Contribute<ul> <li>Reporting Bugs</li> <li>Suggesting Enhancements and New Features</li> <li>Submitting Code Changes</li> </ul> </li> <li>Testing</li> </ul>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Please note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>The PHG_v2 project is written in Kotlin and uses the Gradle build system. To get started, you will need to install the following: * Java 17 * Git</p> <p>It is recommended to use an IDE to make any code changes.  Our group prefers using IntelliJ IDEA.</p>"},{"location":"CONTRIBUTING/#how-to-contribute","title":"How to Contribute","text":"<p>For any code changes, you will need to fork the PHG_v2 repository and create a pull request.  For more information on how to do this, please see this guide.</p>"},{"location":"CONTRIBUTING/#reporting-bugs","title":"Reporting Bugs","text":"<p>If you find a bug, please first check the PHG_v2 issue tracker to see  if the bug has already been reported.  If it has not, please create a new issue with the bug report template.</p>"},{"location":"CONTRIBUTING/#suggesting-enhancements-and-new-features","title":"Suggesting Enhancements and New Features","text":"<p>If you have an idea for an enhancement, please create a new issue with the enhancement template in the PHG_v2 issue tracker.</p>"},{"location":"CONTRIBUTING/#submitting-code-changes","title":"Submitting Code Changes","text":"<p>To submit a code change, you first will need to fork the PHG_v2 repository, make your changes to your fork and then submit a Pull Request to the PHG_v2 repository.  For more information on how to do this, please see this guide.</p> <p>When creating the Pull Request, fill out the Pull Request template with the relevant information and a description of the changes you made. Additionally, you can add reviewers to the Pull Request from the PHG team. If you are unsure of who should review, please add @zrm22 and additional reviewers will be assigned. </p> <p>After you have submitted your Pull Request, please verify that all of the automated checks have passed.  If any of the checks have failed, please review the error message and make any necessary changes to your code.  If you are unsure of how to fix the error, please reach out to the PHG team for assistance.</p> <p>A member of the PHG team will review your Pull Request and provide feedback.  If any changes are requested, please make the requested changes and update your Pull Request.  Once the Pull Request has been approved, it will be merged into the PHG_v2 repository on the main branch and a new build will be automatically created and released.</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>TODO: Explain how to run the unit and integration tests for this project.  Maybe refer to a TESTING.md file.</p>"},{"location":"build_and_load/","title":"Building and Loading","text":"<p>Note</p> <p>As of version 2.4.X, the PHG utilizes a new version of AnchorWave (1.2.3).  This changes how ASM coordinates are handled. If you are using old  MAF files generated either from AnchorWave 1.2.2 or from PHGv2  version 2.3 or earlier, please use the <code>--legacy-maf-file</code> flag  for the <code>create-maf-vcf</code> command. It is recommended that you  remove your <code>phgv2-conda</code> Conda environment and rerun the  <code>setup-environment</code> command.</p> <p>In this document, we will discuss the steps needed to:</p> <ol> <li>Set up a working Conda environment containing the required    external software</li> <li>Initialize a TileDB-VCF instance</li> <li>Build VCF data</li> <li>Loading data into TileDB-VCF instances</li> </ol>"},{"location":"build_and_load/#quick-start","title":"Quick start","text":"<ul> <li> <p>Set up the PHGv2 Conda environment:     <pre><code>phg setup-environment\n</code></pre></p> </li> <li> <p>Initialize TileDB instances:     <pre><code>phg initdb \\\n    --db-path /path/to/dbs\n</code></pre></p> </li> <li> <p>Update FASTA headers with sample information:     <pre><code>phg prepare-assemblies \\\n    --keyfile /path/to/keyfile \\\n    --output-dir /path/to/updated/fastas \\\n    --threads 10\n</code></pre></p> </li> <li> <p>Create BED file from GFF for reference range coordinates:     <pre><code>phg create-ranges \\\n    --reference-file /path/to/updated/fastas/ref.fasta \\\n    --gff /path/to/my.gff \\\n    --boundary gene \\\n    --pad 500 \\\n    --range-min-size 500 \\\n    -o /path/to/ranges.bed\n</code></pre></p> </li> <li> <p>Align assemblies:     <pre><code>phg align-assemblies \\\n    --gff /path/to/my.gff \\\n    --reference-file /path/to/updated/fastas/ref.fasta \\\n    --assembly-file-list /path/to/assemblies_list.txt \\\n    -o /path/to/maf_files\n</code></pre></p> </li> <li> <p>Compress FASTA files     <pre><code>phg agc-compress \\\n    --db-path /path/to/dbs \\\n    --reference-file /path/to/updated/fastas/ref.fasta \\\n    --fasta-list /path/to/assemblies_list.txt\n</code></pre></p> </li> <li>Create VCF files     <pre><code># Reference VCF\nphg create-ref-vcf \\\n    --bed /path/to/ranges.bed \\\n    --reference-file /path/to/updated/ref.fasta \\\n    --reference-name B73 \\\n    --db-path /path/to/dbs\n\n# MAF alignments to VCF\nphg create-maf-vcf \\\n    --db-path /path/to/dbs \\\n    --bed /path/to/ranges.bed \\\n    --reference-file /path/to/updated/ref.fasta \\\n    --maf-dir /path/to/maf_files \\\n    -o /path/to/vcf_files\n</code></pre></li> <li>Load data into DBs     <pre><code>phg load-vcf \\\n    --vcf-dir /path/to/vcf_files \\\n    --db-path /path/to/dbs \\\n    --threads 10\n</code></pre></li> </ul>"},{"location":"build_and_load/#detailed-walkthrough","title":"Detailed walkthrough","text":""},{"location":"build_and_load/#preamble","title":"Preamble","text":"<p>For the following steps, I will first make an example directory to house our toy input data. The overall structure of the directory looks like the following</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2514\u2500\u2500 output\n</code></pre> <p>For the following steps, I will be using example small sequence data from the PHGv2 GitHub repository. This is a collection of small raw sequence files that we use for pipeline testing. These will be placed in the <code>data</code> subdirectory while files created by this pipeline will be placed in the <code>output</code> subdirectory. Here, I have downloaded the following  FASTA files:</p> <ul> <li><code>Ref.fa</code></li> <li><code>LineA.fa</code></li> <li><code>LineB.fa</code></li> </ul> <p>...and have renamed and placed them (which is needed to highlight the  functionality of the \"Prepare Assembly FASTA Files\" section later) in the <code>data/</code> directory. </p> <p>Additionally, I have downloaded a GFF file called <code>anchors.gff</code>  which will be used for the \"Create Reference Ranges\" and \"Align Assemblies\" steps. Overall, my example working directory now looks like the following:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u2514\u2500\u2500 output\n</code></pre> <p>This documentation will also assume that the PHGv2 application is  placed in your system's <code>PATH</code> variable (see the installation  documentation for further details).</p>"},{"location":"build_and_load/#set-up-conda-environment","title":"Set up Conda environment","text":"<p>Once you have downloaded the latest release of PHGv2 and have Conda  installed (see the installation documentation),  you will first need to set up a Conda environment to accommodate the  necessary pieces of software for alignment, compression, and storage.  Here is a list of essential pieces of software that will be installed  in your Conda environment:</p> Software Purpose agc Performant FASTA genome compression AnchorWave Sensitive aligner for genomes with high sequence diversity bcftools Utilities for indexing VCF data samtools bgzip compression for VCF data TileDB Performant storage core for array data TileDB-VCF API for storing and querying VCF data <p>Instead of setting this up manually, we can use the  <code>setup-environment</code> command which will automate this process. To run, use the following command:</p> <pre><code>phg setup-environment\n</code></pre> <p>By using the prior default example, the <code>setup-environment</code> command  will extract an internal Conda environment file with the necessary  libraries and programs (see following code block for example file).  Optionally, if you want to add additional libraries and programs  to your PHGv2 Conda environment, you may specify the optional  parameter, <code>--env-file</code>. This will be a path to a local Conda  environment file. You can create the file from scratch (for example,  I will call mine <code>phg_environment.yml</code>) and copy over the contents  in the following block:</p> <pre><code>name: phgv2-conda\nchannels:\n  - conda-forge\n  - bioconda\n  - tiledb\ndependencies:\n  - python=3.8\n  - tiledb-py=0.22\n  - tiledbvcf-py=0.25\n  - anchorwave=1.2.3\n  - bcftools\n  - samtools\n  - agc\n</code></pre> <p>Another option is to pull in the environment file directly from the  GitHub repository:</p> <pre><code>curl https://raw.githubusercontent.com/maize-genetics/phg_v2/main/src/main/resources/phg_environment.yml -o phg_environment.yml\n</code></pre> <p>Once the local YAML file is made, we can pass it to the <code>--env-file</code> parameter:</p> <pre><code>phg setup-environment --env-file phg_environment.yml\n</code></pre> <p>After the setup step is complete, we can activate our environment using the following conda command:</p> <pre><code>conda activate phgv2-conda\n</code></pre> <p>Note</p> <p>It is imperative the Conda environment you create is named  <code>phgv2-conda</code>. This is the default environment name that PHGv2 uses  when executing shell commands from within the software.</p> <p>Note</p> <p>This environment will change as new versions of the software are released. It is recommended to delete any existing phgv2-conda environments and recreate it using the <code>setup-environment</code>  command.  In particular with PHGv2.4, <code>anchorwave</code> is updated to 1.2.3 which represents a fundamental shift in how ASM coordinates are handled. </p> <p>If we look in our example project directory, you will also see two new logging (<code>.log</code>) files which will record all the logging and error output from the PHGv2 command steps:</p> <ul> <li><code>condaCreate_error.log</code></li> <li><code>condaCreate_output.log</code></li> </ul> <p>Note</p> <p>If you need or want to have the Conda environment setup in a non-default location, you must run the <code>conda env create</code> command manually using the <code>--prefix</code> option to specify the name and location of the environment.  This created location should then be passed as the <code>conda-env-prefix</code> parameter for all PHGv2 commands that have this optional parameter.  For this scenario, the <code>.yml</code> file should not contain the  <code>name:</code> section.  An example of the command to create the environment in a non-default location is:</p> <pre><code>conda env create --prefix /path/to/new/conda/env --file phg_environment.yml\n</code></pre>"},{"location":"build_and_load/#running-phg-commands","title":"Running phg commands","text":"<p>PHGv2 has several commands that specify an output directory. This is  depicted as either <code>-o</code> or <code>--output-dir</code> for the parameter names.  Please make sure that these directories exist in your project  before running any commands that require the <code>-o</code> or <code>--output-dir</code>  field!</p> <p>Additionally, the <code>--db-path</code> parameter shows up in many of the PHGv2  commands. It is the path to the directory where the TileDB datasets  are stored. This parameter is an optional parameter for all commands  in which it appears.  If a folder is not specified for the  <code>--db-path</code>, the software will use the current working directory.  When TileDB datasets are required for processing, the <code>--db-path</code>  parameter value will be verified to ensure the required datasets are  present. If they are not present in the <code>--db-path</code> folder (or in the  current working directory) the software with throw an exception.</p> <p>Finally, several commands have the optional parameter `conda-env-prefix'. This parameter is used to specify the location of the conda environment when it is not in the default location and/or is named differently than the default name of 'phgv2-conda'.  If you are using the default conda  environment name and location, you do not need to specify this parameter.</p>"},{"location":"build_and_load/#initialize-tiledb-instances","title":"Initialize TileDB instances","text":"<p>In PHGv2, we leverage TileDB and  TileDB-VCF  for efficient storage and querying of VCF data. For downstream  pipelines, we will need to initialize 2 databases for storing different VCF information: (1) haplotype (hVCF) and (2) genomic variant (gVCF)  information.</p> <p>Similar to setting up our Conda environment from the prior section, we can automate this process using the <code>initdb</code> command:</p> <pre><code>./phg initdb \\\n    --db-path vcf_dbs \\\n    --gvcf-anchor-gap 1000000 \\\n    --hvcf-anchor-gap 1000 \\\n    --conda-env-prefix /path/to/conda/env\n</code></pre> <p>This command takes one required parameter, <code>--db-path</code> which is the  path or subdirectory to where we want to place our databases. In this example project, I will initialize the hVCF and gVCF database  folders in a subdirectory called <code>vcf_dbs</code>.</p> <p>Three optional parameters may also be set.</p> <p>Parameters <code>--gvcf-anchor-gap</code> and <code>--hvcf-anchor-gap</code> define the distance between anchors in the two TileDB-VCF sparse arrays. Smaller values enable faster data retrieval. However, if there are non-symbolic variants that span many anchors (for example, very large deletions), then the <code>load-vcf</code> command will require a large amount of RAM to process variant information for each assembly. More information can be found in the TileDB-VCF docs.</p> <p>Optional parameter <code>--conda-env-prefix</code> is the path to the Conda env directory that contains the conda environment needed to run phg. This parameter is used when the conda environment to activate does not reside in the default location.</p> <p>After initialization is complete, we will have two empty TileDB-VCF instances and a <code>temp</code> directory in our <code>vcf_dbs</code> subdirectory:</p> Directory Purpose <code>gvcf_dataset</code> Genomic variant (gVCF) database storage <code>hvcf_dataset</code> Haplotype variant (hVCF) database storage <code>temp</code> Creation output and error logging files <p>For reference, my example working directory now looks like this:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2514\u2500\u2500 vcf_dbs *\n    \u251c\u2500\u2500 gvcf_dataset/ *\n    \u251c\u2500\u2500 hvcf_dataset/ *\n    \u2514\u2500\u2500 temp/ *\n</code></pre>"},{"location":"build_and_load/#prepare-assembly-fasta-files","title":"Prepare Assembly FASTA files","text":"<p>Before we begin our alignment and VCF creation steps, we must first process our \"raw\" FASTA files. This section will accomplish two goals:</p> <ol> <li>Copy and rename FASTA files via user-provided sample name IDs.</li> <li>Add a sample name tag to the ID lines of each FASTA file.</li> </ol> <p>Warning</p> <p>Updated assembly and reference FASTA files will need to be used as input for all downstream commands that take assembly  or reference FASTA files as input (e.g., <code>agc-compress</code>,  <code>align-assemblies</code>, etc.). This ensures consistent sample names  across the pipeline!</p> <p>To better explain the first goal, let's use an example. A file named <code>Zm-CML52-NAM-1.0.fa</code> would be copied to a new one named <code>CML52.fa</code>. This action is based on a keyfile (which we will discuss later in the parameters section) provided by the user. The keyfile would list \"CML52\" as the sample name for this FASTA shown below:</p> <pre><code>## A keyfile example (disregard '#' comments for your actual keyfile):\n\n# file name           # new name\nZm-CML52-NAM-1.0.fa   CML52\n</code></pre> <p>The reason for this change is the AGC compression tool stores the file name (minus extension) as the sample name when creating the compressed file.  This keeps the AGC sample name consistent with sample names that are stored in the VCF files. With consistent sample names, sequence and variants may be associated.</p> <p>For the second goal: As of the current date of this document, the return methods of AGC will not keep track of sample IDs when returning sequence information from the compressed file unless you explicitly state the sample information in the header lines of the FASTA files you wish to compress for the PHGv2 databases.</p> <p>To explain this further, let's imagine that we have two FASTA files: <code>LineA.fa</code> and <code>LineB.fa</code>. These files contain information for only chromosome 1 and have a simple header that denotes this sequence name (e.g. <code>chr1</code>):</p> <pre><code>$ head LineA.fa\n&gt;chr1\nATGCGTACGCGCACCG\n\n$ head LineB.fa\n&gt;chr1\nATGCGTTCGCCTTCCG\n</code></pre> <p>After we compress this information using the <code>agc-compress</code> command, we will have a file called <code>assemblies.agc</code>. PHGv2 leverages this compressed sequence file when creating VCF data (see the \"Create VCF files\" section for further details). The issue arrives when we query the compressed data using AGC's <code>getctg</code> command. When we query this example archive, we will get the following information:</p> <pre><code>$ agc getctg assemblies.agc chr1@LineA chr1@LineB &gt; queries.fa\n\n$ cat queries.fa\n&gt;chr1\nATGCGTACGCGCACCG\n&gt;chr1\nATGCGTTCGCCTTCCG\n</code></pre> <p>As you can see from the above example output, we now have no means to efficiently track where each sequence is coming from due to the lack of header information from our prior FASTA files. We can remedy this by adding sample information to the headers:</p> <pre><code>$ head LineA.fa\n&gt;chr1 sampleName=LineA\nATGCGTACGCGCACCG\n\n$ head LineB.fa\n&gt;chr1 sampleName=LineB\nATGCGTTCGCCTTCCG\n</code></pre> <p>Now, when we compress and query using AGC, we get enhanced sample ID tracking:</p> <pre><code>$ ./agc getctg assemblies_new_headers.agc chr1@LineA chr1@LineB &gt; queries.fa\n\n$ head queries.fa\n&gt;chr1 sampleName=LineA\nATGCGTACGCGCACCG\n&gt;chr1 sampleName=LineB\nATGCGTTCGCCTTCCG\n</code></pre> <p>While we can manually modify the header lines of our FASTA file, this can become tedious and prone to a new set of downstream errors. To automate this, PHGv2 provides a command to append sample information to the headers of each FASTA file called <code>prepare-assemblies</code>:</p> <pre><code>phg prepare-assemblies \\\n    --keyfile data/annotation_keyfile.txt \\\n    --threads 10 \\\n    --output-dir output/updated_assemblies\n</code></pre> <p>This command takes 3 parameters:</p> <ul> <li> <p><code>--keyfile</code> - A tab-delimited   keyfile containing two columns:</p> Column Value <code>1</code> Path to FASTA file you would like annotated (this is similar to the text files used to point to the FASTA file paths in the <code>agc-compress</code> and <code>align-assemblies</code> commands). <code>2</code> Name of the sample that will be (1) appended to each header line and (2) the name of the newly generated FASTA file. <ul> <li> <p>My example <code>annotation_keyfile.txt</code> (which is placed in the    <code>data/</code> subdirectory) would look like this:</p> <pre><code>data/Ref-v5.fa Ref\ndata/LineA-final-01.fa   LineA\ndata/LineB-final-04.fa   LineB\n</code></pre> </li> <li> <p>\u26a0\ufe0f Warning All sample assemblies (including your reference assembly)   that you would want processed need to be included in this keyfile.</p> </li> </ul> </li> <li> <p><code>--threads</code> - Optional number of threads to update multiple   FASTA files in parallel. Defaults to <code>1</code>.</p> </li> <li><code>-o</code> - Output directory for the newly updated FASTA files</li> </ul> <p>Note</p> <p>FASTA input files can be either uncompressed or compressed. The output from the <code>prepare-assemblies</code> command will be new FASTA files that are uncompressed.  While AGC accepts compressed FASTA files, the <code>align-assemblies</code> command uses AnchorWave which requires uncompressed FASTA files.</p> <p>Once finished, this command will produce FASTA files with the name of the sample from the keyfile appended to each header line. For example, in our hypothetical FASTA files, our headers go from this:</p> <pre><code>$ head LineA.fa\n&gt;chr1\nATGCGTACGCGCACCG\n</code></pre> <p>to this:</p> <pre><code>$ head LineA.fa\n&gt;chr1 sampleName=LineA\nATGCGTACGCGCACCG\n</code></pre> <p>Note</p> <p>This command will just append the name of a file to the end of the FASTA headers. For example, if we had a more detailed header:</p> <pre><code>chr1 pos=1:16\nATGCGTACGCGCACCG\n</code></pre> <p>...the header would become:</p> <pre><code>chr1 pos=1:16 sampleName=LineA\nATGCGTACGCGCACCG\n</code></pre> <p>Now that we are finished preparing samples, my example working directory looks like the following with a newly formed subdirectory called <code>updated_assemblies</code> under the <code>output</code> directory:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 annotation_keyfile.txt *\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 updated_assemblies *\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa *\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa *\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa *\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u2514\u2500\u2500 temp/\n</code></pre> <p>For further steps, we will be using the updated assemblies from the <code>output/updated_assemblies</code> directory path.</p>"},{"location":"build_and_load/#compress-fasta-files","title":"Compress FASTA files","text":"<p>For optimal storage of sequence information, we can convert our \"plain-text\" FASTA files into a more compact representation using compression. For PHGv2, we can use a command called <code>agc-compress</code>, which is a wrapper for the Assembled Genomes Compressor (AGC). AGC provides performant and efficient compression ratios for our assembly genomes. Like AnchorWave, AGC is also installed during the Conda environment setup phase, so there is no need to install this manually.</p> <p>To run the compression step, we can call the <code>agc-compress</code> command:</p> <pre><code>./phg agc-compress \\\n    --db-path vcf_dbs \\\n    --fasta-list data/assemblies_list.txt \\\n    --reference-file output/updated_assemblies/Ref.fa \\\n    --conda-env-prefix /path/to/conda/env\n</code></pre> <p>This command takes in 3 parameters: * <code>--db-path</code> - path to directory storing the TileDB instances. The   AGC compressed genomes will be placed here on completion.</p> <p>Note</p> <p>The directory specified here should be the same directory used to initialize the TileDB instances in the database initialization (<code>initdb</code>) step.</p> <ul> <li><code>--fasta-list</code> - List of annotated assembly FASTA genomes to   compress.</li> </ul> <p>Note</p> <p>The list specified in <code>--fasta-list</code> should be the list of FASTA files output from the <code>prepare-assemblies</code> command (see the \"Prepare Assembly FASTA files\" section for further details).</p> <ul> <li><code>--reference-file</code> - Reference FASTA genome processed with   the <code>prepare-assemblies</code> command.</li> </ul> <p>In addition, <code>conda-env-prefix</code> is an optional parameter that specifies the path to the Conda directory that contains the conda environment needed to run phg. If not set, conda env <code>phgv2-conda</code> in the defauilt location will be used.</p> <p>After compression is finished, we can navigate to the directory containing the TileDB instances. In my case, this would be the subdirectory, <code>vcf_dbs</code>. Here, you will see a new file created: <code>assemblies.agc</code>. This is the compressed AGC file containing our assemblies. This file will be used later to query for haplotype sequence regions and composite genome creation. Our example working directory now looks like this:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 annotation_keyfile.txt\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u2514\u2500\u2500 updated_assemblies\n\u2502       \u251c\u2500\u2500 Ref.fa\n\u2502       \u251c\u2500\u2500 LineA.fa\n\u2502       \u2514\u2500\u2500 LineB.fa\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc *\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u2514\u2500\u2500 temp/\n</code></pre> <p>When running the <code>agc-compress</code> command, the software will determine if the \"create\" or \"append\" AGC command should be used. If the <code>assemblies.agc</code> file is not present in the db-path directory, the software will use the \"create\" command to compress and load the FASTAs. If the <code>assemblies.agc</code> file is present in the db-path directory, the software will use the \"append\" command to compress and load the FASTAs to the existing <code>assemblies.agc</code> file. It skips FASTA files whose \"name\" portion (file name without extension) is already represented as a sample name in the <code>assemblies.agc</code> file, adding only the new FASTAs to this file.</p>"},{"location":"build_and_load/#create-reference-ranges","title":"Create reference ranges","text":"<p>Next, we must define ordered sequences of genic and inter-genic  ranges across the reference genome. These ordered ranges, which we  will call \"reference ranges\", are used to specify haplotype sequence  coordinate information for the haplotype and genomic variant call  format data. These genomic positions are structured using the  BED format.</p> <p>Generally, this data is not readily available as BED files. PHGv2 can generate this file directly from  GFF-formatted  data using the <code>create-ranges</code> command:</p> <pre><code>./phg create-ranges \\\n    --gff data/anchors.gff \\\n    --reference-file output/updated_assemblies/Ref.fa \\\n    --boundary gene \\\n    --pad 500 \\\n    --range-min-size 500 \\\n    -o output/ref_ranges.bed\n</code></pre> <p>This command uses several parameters:</p> <ul> <li><code>--gff</code> - Our reference GFF file of interest. Since    reference ranges are usually established by genic/intergenic    regions, we can leverage coordinate and feature information from   GFF files.</li> <li><code>--reference-file</code> - Our reference genome in    FASTA format. This   is needed for determining the intergenic regions near the ends of    chromosomes.</li> <li> <p><code>--boundary</code> - How do you want to define the boundaries of the    reference ranges. Currently, these can be defined as either   <code>gene</code> or <code>cds</code> regions:</p> <p></p> <ul> <li>In the above figure, if <code>--boundary</code> is set to <code>gene</code>, the start   and end positions are at the UTR regions for each gene feature from   the GFF file, while <code>cds</code> will begin and end at the open reading    frame. By default, the <code>cds</code> option will try to identify the   transcript with the longest open reading frame if there are   multiple transcript \"children\" for a gene \"parent\" in the GFF   file.</li> </ul> </li> <li> <p><code>--pad</code> - The number of base pairs you would like to flank regions:</p> <p></p> <ul> <li>For example, if we were to set the <code>--pad</code> parameter to <code>500</code>,   we would extend the region 500 base pairs upstream and downstream   of the defined boundary (in this case, <code>gene</code>).</li> </ul> </li> </ul> <p>Note</p> <p>There is a possibility that overlaps of regions will occur. If this does happen, <code>create-ranges</code> will identify any overlapping regions and merge regions together:</p> <pre><code>[---Overlap 1----]\n           [-------Overlap 2-------]\n====================================\n[---------Merged overlap-----------]\n</code></pre> <ul> <li><code>--range-min-size</code> - The minimum size for each range in the BED    file. This parameter is optional with a default of 500 base pairs.    For example, if we were to set the <code>--range-min-size</code> parameter to    <code>500</code>, any region that is less than 500 base pairs in length will    be merged with the previous region on that contig.  If the first    region of a contig is less than the specified minimum size, it will    be merged with the next region on that contig.  This merging is    done in <code>create-ranges</code> after the gene or CDS region has been    created and padding has been applied.</li> <li><code>-o</code> - Name for the output BED file.</li> </ul> <p>In the above example, I am using test data from the  PHGv2 GitHub repository. After the command is finished, we have produced a BED file (which in my case, is located in a subdirectory labelled <code>output</code>). This BED  file contains 6 columns of information:</p> Column Value <code>1</code> Sequence name <code>2</code> Start position (bp) <code>3</code> End position (bp) <code>4</code> Range ID <code>5</code> Score (always <code>0</code>) <code>6</code> Strand information <p>Note</p> <p>Column <code>4</code> will either be gene or cds name depending on the boundary input.  If regions overlapped and were merged, the name is a comma  separated string of all the genes/CDS features included in this BED  region.</p> <p>Note</p> <p>While a BED file of reference ranges is required for this  pipeline, it does not have to be created via the <code>create-ranges</code>  command as long as it is reflective of the features found within  the reference genome used in the pipeline:</p> <ul> <li>Same contig/chromosome IDs</li> <li>Coordinates are 0-based, inclusive/exclusive</li> <li>Coordinates do not exceed contig boundaries</li> <li>Coordinates do not overlap</li> <li>Contains the first three BED columns:<ul> <li><code>1</code> - Sequence name</li> <li><code>2</code> - Start position (bp)</li> <li><code>3</code> - End position (bp)</li> </ul> </li> </ul> <p>While we only need the first three columns, columns <code>4</code> through <code>6</code> provide additional biologically relevant information that you may also want to include!</p> <p>Now that we have a BED file, our example working directory now looks like the following:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 annotation_keyfile.txt\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 ref_ranges.bed *\n\u2502   \u2514\u2500\u2500 updated_assemblies\n\u2502       \u251c\u2500\u2500 Ref.fa\n\u2502       \u251c\u2500\u2500 LineA.fa\n\u2502       \u2514\u2500\u2500 LineB.fa\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u2514\u2500\u2500 temp/\n</code></pre>"},{"location":"build_and_load/#align-assemblies","title":"Align assemblies","text":"<p>Next, we must align our collection of genome assemblies against a single reference genome in order to have a common coordinate system across all genomes within our PHG databases. While whole genome alignment can be performed in a multitude of ways, PHGv2 provides an opinionated wrapper to AnchorWave,  which uses an Anchored Wavefront alignment approach for  genomes with high sequence diversity, extensive structural  polymorphism, or whole-genome duplications. Since this software is already set up during the Conda environment step, there is no need to install this manually.</p> <p>Note</p> <p>For best results with imputation and rare allele calling pipelines,  please use high quality assemblies that have been run through  the <code>prepare-assemblies</code> command.</p> <p>To run the aligner step, we can call the <code>align-assemblies</code> command:</p> <pre><code>./phg align-assemblies \\\n    --gff data/anchors.gff \\\n    --reference-file output/updated_assemblies/Ref.fa \\\n    --assembly-file-list data/assemblies_list.txt \\\n    --total-threads 20 \\\n    --in-parallel 2 \\\n    -o output/alignment_files \\\n    --conda-env-prefix /path/to/conda/env\n</code></pre> <p>Note</p> <p>The following example workflow is for users who have access to a single machine. If you are running this on an  HPC system that leverages the  SLURM job scheduler, please follow the guidelines found in the  \"SLURM Usage\" documentation to maximize computational efficiency.</p>"},{"location":"build_and_load/#align-assemblies-parameters","title":"Align-assemblies parameters","text":"<p>This <code>align-assemblies</code> command uses several required parameters:</p> <ul> <li><code>--gff</code> - GFF file for the reference genome. This is used to   identify full-length coding sequences to use as anchors</li> <li> <p><code>--reference-file</code> - The reference genome in    FASTA format.</p> <ul> <li> <p>\u2139\ufe0f Note   The path to the reference genome should be the updated version   that was created during the <code>prepare-assemblies</code> command.</p> </li> </ul> </li> <li> <p><code>--assembly-file-list</code> Or <code>--assembly-file</code> - Either a single    annotated file may be specified, or a text file containing a list    of annotated assembly genomes (see the \"Prepare Assembly FASTA files\"    section for further details). The single assembly or the contents    of the assembly list file should be either full or relative paths   to each uncompressed assembly you would like to align.   For example, since I am using the example data found on the    PHGv2 GitHub repository,   I can create a text file called <code>assemblies_list.txt</code> (placed in    the <code>data/</code> subdirectory) and populate it with the following lines:</p> <p><pre><code>output/updated_assemblies/LineA.fa\noutput/updated_assemblies/LineB.fa\n</code></pre> Here, I am planning on aligning two genomes called <code>LineA</code> and  <code>LineB</code>. Since these are created with the <code>prepare-assemblies</code> command  and the output is located in a subdirectory called  <code>output/updated_assemblies/</code> relative to my working directory, I  will also add that to the path.</p> <ul> <li> <p>\u26a0\ufe0f Warning   This text list should not contain the path to the reference   genome since this is recognized in the <code>--reference-file</code> flag.</p> </li> </ul> </li> <li> <p><code>-o</code> - The name of the directory for the alignment outputs.</p> </li> </ul>"},{"location":"build_and_load/#align-assemblies-optional-parameters","title":"Align-assemblies optional parameters","text":"<p>In addition to the above parameters, there are several optional  parameters. When values are not specified for these parameters, default values are calculated by the software based on the system  processor and memory configuration:</p> <ul> <li><code>--total-threads</code> - How many threads would you like to allocate for   the alignment step? More information about this step and the    <code>--in-parallel</code> step can be found in the following    Details - threads and parallelization    section.</li> <li><code>--in-parallel</code> - How many genomes would you like to run in    parallel? More information about this step and the <code>--total-threads</code>    step can be found in the following    \"Details - threads and parallelization\"    section.</li> <li><code>--conda-env-prefix</code> - Specifies the path to the Conda directory    that contains the Conda environment needed to run the PHG software.    If not set, a Conda environment labelled <code>phgv2-conda</code> will be used    in the default location.</li> <li><code>--just-ref-prep</code> - Specifies whether to run only the preliminary    steps of aligning the reference genome to the GFF CDS, creating the    <code>reference-sam</code> and <code>reference-cds-fasta</code> files needed for aligning    the individual assemblies. This is desirable when a list of assembly    files will be run through a SLURM job, and we don't want to    unnecessarily create these 2 files multiple times. If set to <code>true</code>,    the software stops after creating these two files, which will be    written to the user supplied output directory. The default value is    <code>false</code>. More information about these parameters can be found in the    supplemental \"SLURM Usage\" documentation.</li> <li><code>--reference-sam</code> - If this is specified, the parameter    <code>--reference-cds-fasta</code> must also be supplied. When both are    supplied, the software skips the creation of these files and uses    those supplied by the user. This is desirable when the user is    running multiple assembly alignments from a SLURM data-array option    and does not wish to realign the reference multiple times. If    specified, but <code>--reference-cds-fasta</code> is not, the software will    throw an exception. More information about these parameters can be   found in the supplemental \"SLURM Usage\"    documentation.</li> <li><code>--reference-cds-fasta</code> - If this is specified, the parameter   <code>--reference-sam</code> must also be supplied. When both are supplied, the    software skips the creation of these files and uses those supplied    by the user. This is desirable when the user is running multiple    assembly alignments from a SLURM data-array option and does not    wish to realign the reference multiple times. If specified, but    <code>--reference-sam</code> is not, the software will throw an exception.    More information about these parameters can be found in the    supplemental \"SLURM Usage\" documentation.</li> </ul> <p>Warning</p> <p>The directory that you specify in the output (<code>-o</code>) section must be an existing directory.</p> <p>Once alignment is finished, and you have navigated into the output  directory (in my case, this would be <code>output/alignment_files</code>), you will see several different file types. In addition to various  logging files (<code>.log</code>) for the AnchorWave procedures, you will notice  that each assembly will have a collection of different file types:</p> File extension Property <code>.sam</code> sequence alignment map (SAM) file <code>.maf</code> multiple alignment format (MAF) file <code>.anchorspro</code> alignment blocks between reference and assembly genomes (used for dot plot generation) <p>The MAF files from this output will be used in the VCF creation step.  Additionally, dot plots  will be generated for each sample/reference alignment as <code>.svg</code>  files. Now that alignment is completed, our example working directory  looks as follows (NOTE: I will collapse <code>alignment_files</code> for  future steps):</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 annotation_keyfile.txt\n\u2502   \u251c\u2500\u2500 assemblies_list.txt *\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files\n\u2502   \u2502   \u251c\u2500\u2500 anchorwave_gff2seq_error.log *\n\u2502   \u2502   \u251c\u2500\u2500 anchorwave_gff2seq_output.log *\n\u2502   \u2502   \u251c\u2500\u2500 LineA.maf *\n\u2502   \u2502   \u251c\u2500\u2500 LineA.sam *\n\u2502   \u2502   \u251c\u2500\u2500 LineA.svg *\n\u2502   \u2502   \u251c\u2500\u2500 LineA_Ref.anchorspro *\n\u2502   \u2502   \u251c\u2500\u2500 LineB.maf *\n\u2502   \u2502   \u251c\u2500\u2500 LineB.sam *\n\u2502   \u2502   \u251c\u2500\u2500 LineB.svg *\n\u2502   \u2502   \u251c\u2500\u2500 LineB_Ref.anchorspro *\n\u2502   \u2502   \u251c\u2500\u2500 minimap2_LineA_error.log *\n\u2502   \u2502   \u251c\u2500\u2500 minimap2_LineA_output.log *\n\u2502   \u2502   \u251c\u2500\u2500 minimap2_LineB_error.log *\n\u2502   \u2502   \u251c\u2500\u2500 minimap2_LineB_output.log *\n\u2502   \u2502   \u251c\u2500\u2500 minimap2_Ref_error.log *\n\u2502   \u2502   \u251c\u2500\u2500 minimap2_Ref_output.log *\n\u2502   \u2502   \u251c\u2500\u2500 proali_LineA_outputAndError.log *\n\u2502   \u2502   \u251c\u2500\u2500 proali_LineB_outputAndError.log *\n\u2502   \u2502   \u251c\u2500\u2500 ref.cds.fasta *\n\u2502   \u2502   \u2514\u2500\u2500 Ref.sam *\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u2514\u2500\u2500 updated_assemblies\n\u2502       \u251c\u2500\u2500 Ref.fa\n\u2502       \u251c\u2500\u2500 LineA.fa\n\u2502       \u2514\u2500\u2500 LineB.fa\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u2514\u2500\u2500 temp/\n</code></pre>"},{"location":"build_and_load/#internal-anchorwave-and-minimap2-commands","title":"Internal AnchorWave and minimap2 commands","text":"<p>While PHGv2 is flexible in terms of how data is processed, we have taken several opinionated steps with how the AnchorWave aligner runs in the <code>align-assemblies</code> command. If you are interested with what parameters are utilized, search for <code>ProcessBuilder()</code> methods within our <code>AlignAssemblies.kt</code> source code or review the following code blocks:</p> <ul> <li>Run AnchorWave's <code>gff2seq</code> command:   <pre><code># Get the longest full-length CDS for each gene\nanchorwave gff2seq \\\n    -r &lt;'--reference-file' parameter&gt; \\\n    -i &lt;'--gff' parameter&gt; \\\n    -o ref.cds.fasta # directed to '-o' path\n</code></pre></li> <li>Run minimap2   <pre><code># AnchorWave liftover steps (ref CDS to query)\nminimap2 \\\n    -x \"splice\" \\\n    -t &lt; '--total-threads' parameter &gt; \\\n    -k 12 \\ \n    -a \\\n    -p 0.4 \\\n    -N 20 \\\n    &lt;'--reference-file' parameter OR sample in '--assemblies' parameter&gt; \\\n    ref.cds.fasta \\      # from prior 'gff2seq' step\n    -o &lt;sample_name&gt;.sam # directed to '-o' path\n</code></pre></li> <li>Run AnchorWave's <code>proali</code> command:   <pre><code>anchorwave proali \\\n    -i &lt;'--gff' parameter&gt; \\\n    -r &lt;'--reference-file' parameter&gt; \\\n    -as ref.cds.fa \\\n    -a &lt;assembly_sample.sam&gt; \\\n    -ar &lt;ref_assembly.sam&gt; \\\n    -s &lt;assembly_sample.fa&gt; \\\n    -n &lt;assembly_ref.anchorspro&gt; \\\n    -R 1 \\    # manually set via --ref-max-align-cov\n    -Q 1 \\    # manually set via --query-max-align-cov\n    -t &lt;'--total-threads' / '--in-parallel' parameter&gt; \\\n    -o &lt;assembly_sample.maf&gt;\n</code></pre></li> </ul> <p>Note</p> <p>In the above <code>proali</code> command, the maximum reference genome  alignment coverage (<code>-R</code>) and maximum query genome coverage (<code>-Q</code>) can be manually set using the <code>--ref-max-align-cov</code> and  <code>--query-max-algin-cov</code> commands, respectively. If these are not set, they will both default to the value of <code>1</code> as shown in the prior <code>-R</code> and <code>-Q</code> parameters.</p>"},{"location":"build_and_load/#details-threads-and-parallelization","title":"Details - threads and parallelization","text":"<p>Aligning with AnchorWave is memory intensive and can be slow.  Processing speed may be increased by using multiple threads for each  alignment, and/or by running multiple alignments in parallel. The  amount of memory each thread takes is dependent on the processor  type. The table below shows the memory usage for a single alignment  based on processor type:</p> Processor peak memory (GB) wall time SSE2 20.1 26:47:17 SSE4.1 20.6 24:05:07 AVX2 20.1 21:40:00 AVX512 20.1 18:31:39 ARM 34.2 18:08:57 <p>The <code>--total-threads</code> parameter indicates the total number of threads  available for system use. The <code>--in-parallel</code> parameter controls the  number of alignments run in parallel.  When these values are not  specified, the software will compute the optimal values based on the  system processor and memory configuration. </p> <p>The number of threads that may be run in parallel is limited by the  amount of memory available. The system is queried for memory and  processor information. The number of threads that may be run in  parallel is determined by \"system memory\" / \"peak memory\" from the  table above. To generalize the calculation, we divide memory  available (GiB) by 21 and round down to the nearest integer.</p> <p>For example, if the system has 512 GB of memory, 80 processors and 5  assemblies that need aligning, the maximum number of threads that could be run in parallel is 24 (512/21). The number of potential  parallel alignments with the threads allocated for each is shown in  the table below:</p> Alignments in parallel Threads per alignment 5 4 4 6 3 8 2 12 1 24 <p>The software will select a pairing that maximizes the number of  alignments run in parallel while utilizing multiple threads, opting  for a value in the middle. In the case above with 5 assemblies and a  possibility of 24 concurrent threads, the system will choose to run 3  alignments in parallel with 8 threads each.  The total number of  threads used will be 24 (3 * 8).</p> <p>User defined values for in-parallel and total-threads are considered  along with the number of assemblies to align and system capacities,  when determining the AnchorWave setup.</p>"},{"location":"build_and_load/#create-vcf-files","title":"Create VCF files","text":"<p>Now that we have (1) created alignments of our assemblies against a single reference genome and (2) created compressed representations of our assembly genomes, we can now create VCF information to populate our TileDB instances. This process is performed using two commands:</p> <ol> <li>Create hVCF data from reference genome:</li> </ol> <pre><code>phg create-ref-vcf \\\n    --bed output/ref_ranges.bed \\\n    --reference-file output/updated_assemblies/Ref.fa \\\n    --reference-name Ref \\\n    --db-path vcf_dbs \\\n    --conda-env-prefix /path/to/conda/env\n</code></pre> <ol> <li>Create hVCF and gVCF data from assembly alignments against reference    genome:</li> </ol> <pre><code>phg create-maf-vcf \\\n    --db-path vcf_dbs \\\n    --bed output/ref_ranges.bed \\\n    --reference-file output/updated_assemblies/Ref.fa \\\n    --maf-dir output/alignment_files \\\n    -o output/vcf_files \\\n    --conda-env-prefix /path/to/conda/env\n</code></pre> <ol> <li>(OPTIONAL!) Create hVCF from existing PHGv1 created gVCF files.     Use instead of create-maf-vcf if you have previously created gVCF     files from PHGv1 and want to create hVCF files:</li> </ol> <pre><code>phg gvcf2hvcf \\\n    --db-path vcf_dbs \\\n    --bed output/ref_ranges.bed \\\n    --reference-file output/updated_assemblies/Ref.fa \\\n    --gvcf-dir output/gvcf_files \n</code></pre> <p>Tip</p> <p>For more information about the haplotype VCF (hVCF) specification, please refer to the hVCF specification documentation.</p> <p>VCF creation is split up into two separate commands since the reference genome and aligned assemblies require different sets of input data. An additional optional command is available to convert existing PHG created gVCF files to hVCF files:</p>"},{"location":"build_and_load/#inputs-for-the-create-ref-vcf-command","title":"Inputs for the <code>create-ref-vcf</code> command","text":"<ul> <li><code>--bed</code> - a BED file containing ordered reference ranges (see   the \"Create reference ranges\" section for further    details). This is used to define the positional information of the    VCF.</li> <li><code>--reference-file</code> - processed reference FASTA genome (from    <code>prepare-assemblies</code>) used for creating MD5 hashes of sequence    information guided by reference range positional data from the    BED file used in the <code>--bed</code> parameter.</li> <li><code>--reference-name</code> - the name of the reference sample. <ul> <li> <p>\u2139\ufe0f Note   The name given here should be the same name given in the   keyfile during the Prepare Assembly FASTA Files   step.</p> </li> </ul> </li> <li><code>--db-path</code> - path to PHG database directory for VCF storage.</li> </ul> <p>OPTIONAL parameters: There are 2 optional parameters to <code>create-ref-vcf</code>:</p> <ul> <li><code>--reference-url</code> - URL where the reference FASTA file can be   downloaded. This will be added to the VCF header information.</li> <li><code>--conda-env-prefix</code> - path to the Conda directory that contains the conda environment needed to run phg.  If not set, env <code>phgv2-conda</code> in the defauilt location will be used.</li> </ul> <p>Once the command is complete, and you have navigated into the  <code>--db-path</code> parameter directory (in my case, <code>vcf_dbs/</code>), you will  see a subfolder named <code>hvcf_files</code> with two files:</p> File Description <code>&lt;ref_name&gt;.h.vcf.gz</code> compressed reference haplotype VCF (hVCF) file <code>&lt;ref_name&gt;.h.vcf.gz.csi</code> coordinate sorted index (CSI) of the reference hVCF file <p>Here, <code>&lt;ref_name&gt;</code> is the name of the reference genome provided using the <code>--reference-name</code> parameter. Since I defined this parameter as <code>Ref</code> in the above example, my two files would be:</p> <ul> <li><code>Ref.h.vcf.gz</code></li> <li><code>Ref.h.vcf.gz.csi</code></li> </ul> <p>There will also be a subdirectory named <code>reference</code> with two files.  The bed file used to create the reference hVCF and the reference  FASTA are stored here for future reference.</p> File Description <code>&lt;ref_name&gt;.bed</code> BED file used to create the reference hVCF <code>&lt;ref_name&gt;.fa</code> reference FASTA file <p>In addition to creating the files, the <code>create-ref-vcf</code> command will load the reference hVCF file to the <code>hvcf_dataset</code> TileDB instance.</p>"},{"location":"build_and_load/#inputs-for-the-create-maf-vcf-command","title":"Inputs for the <code>create-maf-vcf</code> command","text":"<ul> <li><code>--db-path</code> - Path to the directory containing the TileDB    instances. This is needed to access the AGC compressed assembly   genome information found in the <code>assemblies.agc</code> file (see the    \"Compress FASTA files\" section for further details).</li> <li><code>--bed</code> - A BED file containing ordered reference ranges (see   the \"Create reference ranges\" section for further    details). This is used to define the positional information of the    VCF in relation to the reference genome.</li> <li><code>--reference-file</code> - Reference FASTA genome used for creating   MD5 hashes of sequence information guided by reference range   positional data from the BED file used in the <code>--bed</code> parameter.   hashed sequence data will place in the <code>##ALT</code> tag's <code>RefRange</code>   key.<ul> <li> <p>\u2139\ufe0f Note   This should be the processed reference assembly generated from   the \"Prepare Assembly FASTA Files\"   step.</p> </li> </ul> </li> <li><code>--maf-dir</code> - Directory containing the MAF files generated using   the <code>align-assemblies</code> command (see the    \"Align assemblies\" section for further    details).</li> <li><code>-o</code> - Output directory for the VCF data.</li> </ul> <p><code>create-maf-vcf</code> provides two optional parameters for metrics  data (see the \"QC Metrics\" documentation for further information):</p> <ul> <li><code>--metrics-file</code> - Name of the output file for the VCF metrics table     if left blank, the table will be written to the output directory     and will be named <code>VCFMetrics.tsv</code>.</li> <li><code>--skip-metrics</code> - If this flag is set, QC metrics will not be    calculated</li> </ul> <p>In addition, <code>--conda-env-prefix</code> is an optional parameter that specifies the path to the Conda directory that contains the conda environment needed to run phg. If not set, conda env <code>phgv2-conda</code> in the defauilt location will be used.</p> <p>There is also an optional parameter: <code>--legacy-maf-file</code>.  This flag can be added to the command to be able to process MAF files created from anchorwave 1.2.2 or earlier.  If this is not set and  an old file is being processed, the Assembly coordinates in the GVCF will be incorrect.</p> <p>Once the command is complete, and you have navigated into the output directory (in my case, <code>output/vcf_files</code>), you will see a collection of different file types for each sample:</p> File Description <code>&lt;sample_name&gt;.h.vcf.gz</code> compressed reference haplotype VCF (hVCF) file <code>&lt;sample_name&gt;.h.vcf.gz.csi</code> coordinate sorted index (CSI) of the reference hVCF file <code>&lt;sample_name&gt;.g.vcf.gz</code> compressed reference genomic VCF (gVCF) file <code>&lt;sample_name&gt;.g.vcf.gz.csi</code> coordinate sorted index (CSI) of the reference gVCF file <p>Here, <code>&lt;sample_name&gt;</code> would be the name of each sample that was aligned to the reference genome.</p>"},{"location":"build_and_load/#inputs-for-the-gvcf2hvcf-command-optional","title":"Inputs for the <code>gvcf2hvcf</code> command (OPTIONAL!)","text":"<p>Note</p> <p>The <code>gvcf2hvcf</code> command should only be used if you have preexisting gVCF data created from historic PHGv1 steps!</p> <ul> <li><code>--db-path</code> - Path to the directory containing the TileDB   instances. This is needed to access the AGC compressed assembly   genome information found in the <code>assemblies.agc</code> file (see the   \"Compress FASTA files\" section for further details).</li> <li><code>--bed</code> - A BED file containing ordered reference ranges (see   the \"Create reference ranges\" section for further   details). This is used to define the positional information of the   VCF in relation to the reference genome.</li> <li><code>--reference-file</code> - Reference FASTA genome used for creating   MD5 hashes of sequence information guided by reference range   positional data from the BED file used in the <code>--bed</code> parameter.   hashed sequence data will place in the <code>##ALT</code> tag's <code>RefRange</code>   key.</li> <li><code>--gvcf-dir</code> - Directory containing gVCF files generated from either   a PHGv1 <code>MAFToGVCFPlugin</code>  command or a BioKotlin    <code>MAFToGVCF.createGVCFfromMAF()</code> command.</li> </ul> <p>Now that we have created our hVCF and gVCF files using the  <code>create-ref-vcf</code> and <code>create-maf-vcf</code> commands, our directory  structure now looks like the following:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 annotation_keyfile.txt\n\u2502   \u251c\u2500\u2500 assemblies_list.txt\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u2514\u2500\u2500 vcf_files *\n\u2502       \u251c\u2500\u2500 LineA.h.vcf.gz *\n\u2502       \u251c\u2500\u2500 LineA.h.vcf.gz.csi *\n\u2502       \u251c\u2500\u2500 LineA.g.vcf.gz *\n\u2502       \u251c\u2500\u2500 LineA.g.vcf.gz.csi *\n\u2502       \u251c\u2500\u2500 LineB.h.vcf.gz *\n\u2502       \u251c\u2500\u2500 LineB.h.vcf.gz.csi *\n\u2502       \u251c\u2500\u2500 LineB.g.vcf.gz *\n\u2502       \u251c\u2500\u2500 LineB.g.vcf.gz.csi *\n\u2502       \u2514\u2500\u2500 VCFMetrics.tsv *\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files *\n    \u2502   \u251c\u2500\u2500 Ref.h.vcf.gz *\n    \u2502   \u2514\u2500\u2500 Ref.h.vcf.gz.csi *\n    \u251c\u2500\u2500 reference *\n    \u2502   \u251c\u2500\u2500 Ref.bed *\n    \u2502   \u2514\u2500\u2500 Ref.fa *\n    \u2514\u2500\u2500 temp/\n</code></pre>"},{"location":"build_and_load/#load-vcf-data-into-dbs","title":"Load VCF data into DBs","text":"<p>After VCF files are created, we can finally load the information into our empty TileDB instances. Instead of manually performing this action, we can use the <code>load-vcf</code> command to automatically load hVCF and gVCF files into their respective TileDB directories:</p> <pre><code>./phg load-vcf \\\n    --vcf-dir output/vcf_files \\\n    --db-path vcf_dbs \\\n    --threads 10 \\\n    --conda-env-prefix /path/to/conda/env\n</code></pre> <p>This command takes three parameters:</p> <ul> <li><code>--vcf-dir</code> - Directory containing <code>h.vcf.gz</code> and/or <code>g.vcf.gz</code>    files made from the <code>create-ref-vcf</code> and <code>create-maf-vcf</code> commands   (see the \"Create VCF files\" section for    further details). In my example case this is a subdirectory   called <code>output/vcf_files</code>.</li> <li><code>--db-path</code> - Path to the directory containing the TileDB instances.</li> <li><code>--threads</code> - Number of threads for use by the TileDB loading   procedure.</li> </ul> <p>Optional Parameter:</p> <ul> <li><code>--conda-env-prefix</code> - path to the Conda directory that contains the conda environment needed to run phg.  If not set, env <code>phgv2-conda</code> in the defauilt location will be used.</li> </ul> <p>After VCF files have been loaded, the directory containing the TileDB instances (in our case, <code>vcf_dbs/</code>) can be compressed for backups</p>"},{"location":"build_and_load/#where-to-go-from-here","title":"Where to go from here?","text":"<ul> <li>QC Metrics</li> <li>Imputation and Path Finding</li> </ul>"},{"location":"citations/","title":"References","text":"<p>On this page, we highlight articles referencing the use of the Practical Haplotype Graph system. This page is separatated into three sections:</p> <ul> <li>Core Publications - publications from the   Buckler Lab detailing the use and   creation of the PHG in various plant systems.</li> <li>Publications leveraging PHG resources - publications that   utilize the PHG or resources generated from the PHG in various   plant systems.</li> <li>Publications discussing the PHG - publications that discuss   the PHG in various article formats (e.g., book chapters, reviews,   etc.)</li> </ul>"},{"location":"citations/#core-publications","title":"Core Publications","text":"<p>The Practical Haplotype Graph, a platform for storing and using pangenomes for imputation</p>  Summary Plant Species Citation Citation (BibTeX) <ul> <li>Details the overview, architecture, and use-cases of version 1 of the Practical   Haplotype Graph</li> <li>Highlights data compression and imputation capabilities with low coverage sequence data</li> <li>Highlights expandability with open source tools such as the Breeding API   and rPHG</li> </ul> <p>\\(\\quad\\) Maize (Zea mays)</p> <p>Bradbury, P J and Casstevens, T and Jensen, S E and Johnson, L C and Miller, Z R and Monier, B and Romay, M C and Song, B and Buckler, E S (2022). The Practical Haplotype Graph, a platform for storing and using pangenomes for imputation. Bioinformatics. DOI: 10.1093/bioinformatics/btac410</p> <pre><code>@article{10.1093/bioinformatics/btac410,\n    author = {Bradbury, P J and Casstevens, T and Jensen, S E and Johnson, L C and Miller, Z R and Monier, B and Romay, M C and Song, B and Buckler, E S},\n    title = \"{The Practical Haplotype Graph, a platform for storing and using pangenomes for imputation}\",\n    journal = {Bioinformatics},\n    volume = {38},\n    number = {15},\n    pages = {3698-3702},\n    year = {2022},\n    month = {06},\n    abstract = \"{Pangenomes provide novel insights for population and quantitative genetics, genomics and breeding not available from studying a single reference genome. Instead, a species is better represented by a pangenome or collection of genomes. Unfortunately, managing and using pangenomes for genomically diverse species is computationally and practically challenging. We developed a trellis graph representation anchored to the reference genome that represents most pangenomes well and can be used to impute complete genomes from low density sequence or variant data.The Practical Haplotype Graph (PHG) is a pangenome pipeline, database (PostGRES \\&amp;amp; SQLite), data model (Java, Kotlin or R) and Breeding API (BrAPI) web service. The PHG has already been able to accurately represent diversity in four major crops including maize, one of the most genomically diverse species, with up to 1000-fold data compression. Using simulated data, we show that, at even 0.1\u00d7 coverage, with appropriate reads and sequence alignment, imputation results in extremely accurate haplotype reconstruction. The PHG is a platform and environment for the understanding and application of genomic diversity.All resources listed here are freely available. The PHG Docker used to generate the simulation results is https://hub.docker.com/ as maizegenetics/phg:0.0.27. PHG source code is at https://bitbucket.org/bucklerlab/practicalhaplotypegraph/src/master/. The code used for the analysis of simulated data is at https://bitbucket.org/bucklerlab/phg-manuscript/src/master/. The PHG database of NAM parent haplotypes is in the CyVerse data store (https://de.cyverse.org/de/) and named/iplant/home/shared/panzea/panGenome/PHG\\_db\\_maize/phg\\_v5Assemblies\\_20200608.db.Supplementary data are available at Bioinformatics online.}\",\n    issn = {1367-4803},\n    doi = {10.1093/bioinformatics/btac410},\n    url = {https://doi.org/10.1093/bioinformatics/btac410},\n    eprint = {https://academic.oup.com/bioinformatics/article-pdf/38/15/3698/49884088/btac410.pdf},\n}\n</code></pre> <p>Genome-wide imputation using the practical haplotype graph in the heterozygous crop cassava</p>  Summary Plant Species Citation Citation (BibTeX) <ul> <li>Using the PHG improves genotype imputation accuracy in cassava, especially for rare and heterozygous alleles.</li> <li>Using IBD (Identity-by-Descent) to sample phased haplotypes leads to more accurate imputation than computational methods.</li> <li>High genomic prediction accuracy from low-depth sequencing, supporting more efficient plant breeding.</li> </ul> <p>\\(\\quad\\) Cassava (Manihot esculenta)</p> <p>Long, Evan M and Bradbury, Peter J and Romay, M Cinta and Buckler, Edward S and Robbins, Kelly R (2021). Genome-wide imputation using the practical haplotype graph in the heterozygous crop cassava. G3 Genes|Genomes|Genetics. DOI: 10.1093/g3journal/jkab383</p> <pre><code>@article{10.1093/g3journal/jkab383,\n    author = {Long, Evan M and Bradbury, Peter J and Romay, M Cinta and Buckler, Edward S and Robbins, Kelly R},\n    title = \"{Genome-wide imputation using the practical haplotype graph in the heterozygous crop cassava}\",\n    journal = {G3 Genes|Genomes|Genetics},\n    volume = {12},\n    number = {1},\n    pages = {jkab383},\n    year = {2021},\n    month = {11},\n    abstract = \"{Genomic applications such as genomic selection and genome-wide association have become increasingly common since the advent of genome sequencing. The cost of sequencing has decreased in the past two decades; however, genotyping costs are still prohibitive to gathering large datasets for these genomic applications, especially in nonmodel species where resources are less abundant. Genotype imputation makes it possible to infer whole-genome information from limited input data, making large sampling for genomic applications more feasible. Imputation becomes increasingly difficult in heterozygous species where haplotypes must be phased. The practical haplotype graph (PHG) is a recently developed tool that can accurately impute genotypes, using a reference panel of haplotypes. We showcase the ability of the PHG to impute genomic information in the highly heterozygous crop cassava (Manihot esculenta). Accurately phased haplotypes were sampled from runs of homozygosity across a diverse panel of individuals to populate PHG, which proved more accurate than relying on computational phasing methods. The PHG achieved high imputation accuracy, using sparse skim-sequencing input, which translated to substantial genomic prediction accuracy in cross-validation testing. The PHG showed improved imputation accuracy, compared to a standard imputation tool Beagle, especially in predicting rare alleles.}\",\n    issn = {2160-1836},\n    doi = {10.1093/g3journal/jkab383},\n    url = {https://doi.org/10.1093/g3journal/jkab383},\n    eprint = {https://academic.oup.com/g3journal/article-pdf/12/1/jkab383/42340789/jkab383.pdf},\n}\n</code></pre> <p>A Maize Practical Haplotype Graph Leverages Diverse NAM Assemblies</p>  Summary Plant Species Citation Citation (BibTeX) <ul> <li>The PHG was used to create a pangenome representation of maize by leveraging 27 high-quality   genome assemblies, capturing substantial structural diversity in maize.</li> <li>Successfully imputed genotypes with high accuracy. For related lines, the accuracy was over   99%, while for unrelated lines, it reached up to 95% using whole-genome sequencing (WGS).</li> <li>Highly space-efficient, storing data in a manner 30,000 times smaller than typical genotype   files, making it a powerful tool for handling large-scale genomic data.</li> </ul> <p>\\(\\quad\\) Maize (Zea mays)</p> <p>Franco, Jose A. Valdes and Gage, Joseph L. and Bradbury, Peter J. and Johnson, Lynn C. and Miller, Zachary R. and Buckler, Edward S. and Romay, M. Cinta (2020). A Maize Practical Haplotype Graph Leverages Diverse NAM Assemblies. bioRxiv. DOI: 10.1101/2020.08.31.268425</p> <pre><code>@article {Franco2020.08.31.268425,\n    author = {Franco, Jose A. Valdes and Gage, Joseph L. and Bradbury, Peter J. and Johnson, Lynn C. and Miller, Zachary R. and Buckler, Edward S. and Romay, M. Cinta},\n    title = {A Maize Practical Haplotype Graph Leverages Diverse NAM Assemblies},\n    elocation-id = {2020.08.31.268425},\n    year = {2020},\n    doi = {10.1101/2020.08.31.268425},\n    publisher = {Cold Spring Harbor Laboratory},\n    abstract = {As a result of millions of years of transposon activity, multiple rounds of ancient polyploidization, and large populations that preserve diversity, maize has an extremely structurally diverse genome, evidenced by high-quality genome assemblies that capture substantial levels of both tropical and temperate diversity. We generated a pangenome representation (the Practical Haplotype Graph, PHG) of these assemblies in a database, representing the pangenome haplotype diversity and providing an initial estimate of structural diversity. We leveraged the pangenome to accurately impute haplotypes and genotypes of taxa using various kinds of sequence data, ranging from WGS to extremely-low coverage GBS. We imputed the genotypes of the recombinant inbred lines of the NAM population with over 99\\% mean accuracy, while unrelated germplasm attained a mean imputation accuracy of 92 or 95\\% when using GBS or WGS data, respectively. Most of the imputation errors occur in haplotypes within European or tropical germplasm, which have yet to be represented in the maize PHG database. Also, the PHG stores the imputation data in a 30,000-fold more space-efficient manner than a standard genotype file, which is a key improvement when dealing with large scale data.Competing Interest StatementThe authors have declared no competing interest.},\n    URL = {https://www.biorxiv.org/content/early/2020/09/28/2020.08.31.268425},\n    eprint = {https://www.biorxiv.org/content/early/2020/09/28/2020.08.31.268425.full.pdf},\n    journal = {bioRxiv}\n}\n</code></pre> <p>A sorghum practical haplotype graph facilitates genome\u2010wide imputation and cost\u2010effective genomic prediction</p>  Summary Plant Species Citation Citation (BibTeX) <ul> <li>The PHG successfully imputed genome-wide variants from only 0.01x   sequence coverage with minimal loss in accuracy, making it highly   cost-effective for genomic prediction.</li> <li>The PHG can combine data from different sequencing platforms,   unifying genotype calls, and facilitating easier comparison across   breeding programs.</li> <li>The PHG enables cost-effective genomic selection, with prediction   accuracies comparable to higher-cost methods like   genotyping-by-sequencing (GBS) or rhAmpSeq, thus supporting faster   and cheaper breeding programs.</li> </ul> <p>\\(\\quad\\) Sorghum (Sorghum bicolor)</p> <p>Jensen, Sarah E. and Charles, Jean Rigaud and Muleta, Kebede and Bradbury, Peter J. and Casstevens, Terry and Deshpande, Santosh P. and Gore, Michael A. and Gupta, Rajeev and Ilut, Daniel C. and Johnson, Lynn and Lozano, Roberto and Miller, Zachary and Ramu, Punna and Rathore, Abhishek and Romay, M. Cinta and Upadhyaya, Hari D. and Varshney, Rajeev K. and Morris, Geoffrey P. and Pressoir, Gael and Buckler, Edward S. and Ramstein, Guillaume P. (2020). A sorghum practical haplotype graph facilitates genome\u2010wide imputation and cost\u2010effective genomic prediction. The Plant Genome. DOI: 10.1002/tpg2.20009</p> <pre><code>@article{https://doi.org/10.1002/tpg2.20009,\n    author = {Jensen, Sarah E. and Charles, Jean Rigaud and Muleta, Kebede and Bradbury, Peter J. and Casstevens, Terry and Deshpande, Santosh P. and Gore, Michael A. and Gupta, Rajeev and Ilut, Daniel C. and Johnson, Lynn and Lozano, Roberto and Miller, Zachary and Ramu, Punna and Rathore, Abhishek and Romay, M. Cinta and Upadhyaya, Hari D. and Varshney, Rajeev K. and Morris, Geoffrey P. and Pressoir, Gael and Buckler, Edward S. and Ramstein, Guillaume P.},\n    title = {A sorghum practical haplotype graph facilitates genome-wide imputation and cost-effective genomic prediction},\n    journal = {The Plant Genome},\n    volume = {13},\n    number = {1},\n    pages = {e20009},\n    doi = {https://doi.org/10.1002/tpg2.20009},\n    url = {https://acsess.onlinelibrary.wiley.com/doi/abs/10.1002/tpg2.20009},\n    eprint = {https://acsess.onlinelibrary.wiley.com/doi/pdf/10.1002/tpg2.20009},\n    abstract = {Abstract Successful management and utilization of increasingly large genomic datasets is essential for breeding programs to accelerate cultivar development. To help with this, we developed a Sorghum bicolor Practical Haplotype Graph (PHG) pangenome database that stores haplotypes and variant information. We developed two PHGs in sorghum that were used to identify genome-wide variants for 24 founders of the Chibas sorghum breeding program from 0.01x sequence coverage. The PHG called single nucleotide polymorphisms (SNPs) with 5.9\\% error at 0.01x coverage\u2014only 3\\% higher than PHG error when calling SNPs from 8x coverage sequence. Additionally, 207 progenies from the Chibas genomic selection (GS) training population were sequenced and processed through the PHG. Missing genotypes were imputed from PHG parental haplotypes and used for genomic prediction. Mean prediction accuracies with PHG SNP calls range from .57\u2013.73 and are similar to prediction accuracies obtained with genotyping-by-sequencing or targeted amplicon sequencing (rhAmpSeq) markers. This study demonstrates the use of a sorghum PHG to impute SNPs from low-coverage sequence data and shows that the PHG can unify genotype calls across multiple sequencing platforms. By reducing input sequence requirements, the PHG can decrease the cost of genotyping, make GS more feasible, and facilitate larger breeding populations. Our results demonstrate that the PHG is a useful research and breeding tool that maintains variant information from a diverse group of taxa, stores sequence data in a condensed but readily accessible format, unifies genotypes across genotyping platforms, and provides a cost-effective option for genomic selection.},\n    year = {2020}\n}\n</code></pre>"},{"location":"citations/#publications-leveraging-phg-resources","title":"Publications leveraging PHG resources","text":"CassavaMaizeSorghumWheat <ul> <li>Long, Evan M and Bradbury, Peter J and Romay, M Cinta and Buckler, Edward S and Robbins, Kelly R (2021). Genome-wide imputation using the practical haplotype graph in the heterozygous crop cassava. G3 Genes|Genomes|Genetics. DOI: 10.1093/g3journal/jkab383</li> </ul> <ul> <li>Fernandes, Igor K. and Vieira, Caio C. and Dias, Kaio O. G. and Fernandes, Samuel B. (2024). Using machine learning to integrate genetic and environmental data to model genotype-by-environment interactions. bioRxiv. DOI: 10.1101/2024.02.08.579534</li> <li>Stitzer, Michelle C. and Khaipho-Burch, Merritt B. and Hudson, Asher I. and Song, Baoxing and Valdez-Franco, Jose Arcadio and Ramstein, Guillaume and Feschotte, Cedric and Buckler, Edward S. (2023). Transposable element abundance subtly contributes to lower fitness in maize. bioRxiv. DOI: 10.1101/2023.09.18.557618</li> <li>Lima, Dayane Cristina and Aviles, Alejandro Castro and Alpers, Ryan Timothy and McFarland, Bridget A. and Kaeppler, Shawn and Ertl, David and Romay, Maria Cinta and Gage, Joseph L. and Holland, James and Beissinger, Timothy and Bohn, Martin and Buckler, Edward and Edwards, Jode and Flint-Garcia, Sherry and Hirsch, Candice N. and Hood, Elizabeth and Hooker, David C. and Knoll, Joseph E. and Kolkman, Judith M. and Liu, Sanzhen and McKay, John and Minyo, Richard and Moreta, Danilo E. and Murray, Seth C. and Nelson, Rebecca and Schnable, James C. and Sekhon, Rajandeep S. and Singh, Maninder P. and Thomison, Peter and Thompson, Addie and Tuinstra, Mitchell and Wallace, Jason and Washburn, Jacob D. and Weldekidan, Teclemariam and Wisser, Randall J. and Xu, Wenwei and de Leon, Natalia (2023). 2018\u20132019 field seasons of the Maize Genomes to Fields (G2F) G x E project. BMC Genomic Data. DOI: 10.1186/s12863-023-01129-2</li> <li>Giri, Anju and Khaipho-Burch, Merritt and Buckler, Edward S. and Ramstein, Guillaume P. (2021). Haplotype Associated RNA Expression (HARE) Improves Prediction of Complex Traits in Maize. PLOS Genetics. DOI: 10.1371/journal.pgen.1009568</li> <li>Franco, Jose A. Valdes and Gage, Joseph L. and Bradbury, Peter J. and Johnson, Lynn C. and Miller, Zachary R. and Buckler, Edward S. and Romay, M. Cinta (2020). A Maize Practical Haplotype Graph Leverages Diverse NAM Assemblies. bioRxiv. DOI: 10.1101/2020.08.31.268425</li> </ul> <ul> <li>Somegowda, Vinutha Kanuganhalli and Diwakar Reddy, S.E. and Gaddameedi, Anil and Kiranmayee, K.N.S. Usha and Naravula, Jalaja and Kavi Kishor, P.B. and Penna, Suprasanna (2024). Genomics breeding approaches for developing Sorghum bicolor lines with stress resilience and other agronomic traits. Current Plant Biology. DOI: 10.1016/j.cpb.2023.100314</li> <li>Jensen, Sarah E. and Charles, Jean Rigaud and Muleta, Kebede and Bradbury, Peter J. and Casstevens, Terry and Deshpande, Santosh P. and Gore, Michael A. and Gupta, Rajeev and Ilut, Daniel C. and Johnson, Lynn and Lozano, Roberto and Miller, Zachary and Ramu, Punna and Rathore, Abhishek and Romay, M. Cinta and Upadhyaya, Hari D. and Varshney, Rajeev K. and Morris, Geoffrey P. and Pressoir, Gael and Buckler, Edward S. and Ramstein, Guillaume P. (2020). A sorghum practical haplotype graph facilitates genome\u2010wide imputation and cost\u2010effective genomic prediction. The Plant Genome. DOI: 10.1002/tpg2.20009</li> </ul> <ul> <li>Wang, Hongliang and Bernardo, Amy and St. Amand, Paul and Bai, Guihua and Bowden, Robert L. and Guttieri, Mary J. and Jordan, Katherine W. (2023). Skim exome capture genotyping in wheat. The Plant Genome. DOI: 10.1002/tpg2.20381</li> <li>Jordan, Katherine W and Bradbury, Peter J and Miller, Zachary R and Nyine, Moses and He, Fei and Fraser, Max and Anderson, Jim and Mason, Esten and Katz, Andrew and Pearce, Stephen and Carter, Arron H and Prather, Samuel and Pumphrey, Michael and Chen, Jianli and Cook, Jason and Liu, Shuyu and Rudd, Jackie C and Wang, Zhen and Chu, Chenggen and Ibrahim, Amir M H and Turkus, Jonathan and Olson, Eric and Nagarajan, Ragupathi and Carver, Brett and Yan, Liuling and Taagen, Ellie and Sorrells, Mark and Ward, Brian and Ren, Jie and Akhunova, Alina and Bai, Guihua and Bowden, Robert and Fiedler, Jason and Faris, Justin and Dubcovsky, Jorge and Guttieri, Mary and Brown-Guedira, Gina and Buckler, Ed and Jannink, Jean-Luc and Akhunov, Eduard D (2021). Development of the Wheat Practical Haplotype Graph database as a resource for genotyping data storage and genotype imputation. G3 Genes|Genomes|Genetics. DOI: 10.1093/g3journal/jkab390</li> </ul>"},{"location":"citations/#publications-discussing-the-phg","title":"Publications discussing the PHG","text":"General DiscussionBook ChaptersReviews <ul> <li>Song, Baoxing and Buckler, Edward S. and Stitzer, Michelle C. (2024). New whole-genome alignment tools are needed for tapping into plant diversity. Trends in Plant Science. DOI: 10.1016/j.tplants.2023.08.013</li> <li>Coletta, Rafael Della and Fernandes, Samuel B. and Monnahan, Patrick J. and Mikel, Mark A. and Bohn, Martin O. and Lipka, Alexander E. and Hirsch, Candice N. (2023). Importance of genetic architecture in marker selection decisions for genomic prediction. bioRxiv. DOI: 10.1101/2023.02.28.530521</li> <li>da Costa Lima Moraes, Aline and Mollinari, Marcelo and Ferreira, Rebecca Caroline Ulbricht and Aono, Alexandre and de Castro Lara, Let\u00edcia Aparecida and Pessoa-Filho, Marco and Barrios, Sanzio Carvalho Lima and Garcia, Antonio Augusto Franco and do Valle, Cacilda Borges and de Souza, Anete Pereira and Vigna, Bianca Baccili Zanotto (2023). Advances in genomic characterization ofUrochloa humidicola: exploring polyploid inheritance and apomixis. bioRxiv. DOI: 10.1101/2023.08.31.555743</li> <li>Ruperao, Pradeep and Gandham, Prasad and Odeny, Damaris A. and Mayes, Sean and Selvanayagam, Sivasubramani and Thirunavukkarasu, Nepolean and Das, Roma R. and Srikanda, Manasa and Gandhi, Harish and Habyarimana, Ephrem and Manyasa, Eric and Nebie, Baloua and Deshpande, Santosh P. and Rathore, Abhishek (2023). Exploring the sorghum race level diversity utilizing 272 sorghum accessions genomic resources. Frontiers in Plant Science. DOI: 10.3389/fpls.2023.1143512</li> <li>Brown, Pat J. (2023). Haplotyping interspecific hybrids by dual alignment to both parental genomes. The Plant Genome. DOI: 10.1002/tpg2.20324</li> <li>Aylward, Anthony J. and Petrus, Semar and Mamerto, Allen and Hartwick, Nolan T. and Michael, Todd P. (2023). PanKmer:k-mer based and reference-free pangenome analysis. bioRxiv. DOI: 10.1101/2023.03.31.535143</li> <li>Tanaka, Ryokei and Wu, Di and Li, Xiaowei and Tibbs\u2010Cortes, Laura E. and Wood, Joshua C. and Magallanes\u2010Lundback, Maria and Bornowski, Nolan and Hamilton, John P. and Vaillancourt, Brieanne and Li, Xianran and Deason, Nicholas T. and Schoenbaum, Gregory R. and Buell, C. Robin and DellaPenna, Dean and Yu, Jianming and Gore, Michael A. (2022). Leveraging prior biological knowledge improves prediction of tocochromanols in maize grain. The Plant Genome. DOI: 10.1002/tpg2.20276</li> <li>Bayer, Philipp E. and Petereit, Jakob and Durant, \u00c9loi and Monat, C\u00e9cile and Rouard, Mathieu and Hu, Haifei and Chapman, Brett and Li, Chengdao and Cheng, Shifeng and Batley, Jacqueline and Edwards, David (2022). Wheat Panache - a pangenome graph database representing presence/absence variation across 16 bread wheat genomes. NA. DOI: 10.1101/2022.02.23.481560</li> <li>Cerioli, Tommaso and Hernandez, Christopher O. and Angira, Brijesh and McCouch, Susan R. and Robbins, Kelly R. and Famoso, Adam N. (2022). Development and validation of an optimized marker set for genomic selection in southern U.S. rice breeding programs. The Plant Genome. DOI: 10.1002/tpg2.20219</li> <li>Tinker, Nicholas A. and Wight, Charlene P. and Bekele, Wubishet A. and Yan, Weikai and Jellen, Eric N. and Renhuldt, Nikos Tsardakas and Sirijovski, Nick and Lux, Thomas and Spannagl, Manuel and Mascher, Martin (2022). Genome analysis in Avena sativa reveals hidden breeding barriers and opportunities for oat improvement. Communications Biology. DOI: 10.1038/s42003-022-03256-5</li> <li>Singh, Pummi and Huang, Shun-Yuan and Hernandez, Alvaro G. and Adhikari, Pragya and Jamann, Tiffany M. and Mideros, Santiago X. (2022). Genomic regions associated with virulence in Setosphaeria turcica identified by linkage mapping in a biparental population. Fungal Genetics and Biology. DOI: 10.1016/j.fgb.2021.103655</li> <li>Vaughn, Justin N. and Branham, Sandra E. and Abernathy, Brian and Hulse-Kemp, Amanda M. and Rivers, Adam R. and Levi, Amnon and Wechter, William P. (2022). Graph-based pangenomics maximizes genotyping density and reveals structural impacts on fungal resistance in melon. Nature Communications. DOI: 10.1038/s41467-022-35621-7</li> <li>Schneider, Michael and Casale, Federico and Stich, Benjamin (2022). Accurate recombination estimation from pooled genotyping and sequencing: a case study on barley. BMC Genomics. DOI: 10.1186/s12864-022-08701-7</li> <li>Wang, Shuo and Qian, Yong-Qing and Zhao, Ru-Peng and Chen, Ling-Ling and Song, Jia-Ming (2022). Graph-based pan-genomes: increased opportunities in plant genomics. Journal of Experimental Botany. DOI: 10.1093/jxb/erac412</li> <li>Boatwright, J. Lucas and Sapkota, Sirjan and Jin, Hongyu and Schnable, James C. and Brenton, Zachary and Boyles, Richard and Kresovich, Stephen (2022). Sorghum Association Panel\u00a0whole\u2010genome sequencing establishes cornerstone resource for dissecting genomic diversity. The Plant Journal. DOI: 10.1111/tpj.15853</li> <li>Zhao, Yikun and Tian, Hongli and Li, Chunhui and Yi, Hongmei and Zhang, Yunlong and Li, Xiaohui and Zhao, Han and Huo, Yongxue and Wang, Rui and Kang, Dingming and Lu, Yuncai and Liu, Zhihao and Liang, Ziyue and Xu, Liwen and Yang, Yang and Zhou, Ling and Wang, Tianyu and Zhao, Jiuran and Wang, Fengge (2022). HTPdb and HTPtools: Exploiting maize haplotype-tag polymorphisms for germplasm resource analyses and genomics-informed breeding. Plant Communications. DOI: 10.1016/j.xplc.2022.100331 Schneider, Michael and Shrestha, Asis and Ballvora, Agim and L\u00e9on, Jens (2022). High-throughput estimation of allele frequencies using combined pooled-population sequencing and haplotype-based data processing. Plant Methods*. DOI: 10.1186/s13007-022-00852-8</li> <li>Strable, Josh (2021). Developmental genetics of maize vegetative shoot architecture. Molecular Breeding. DOI: 10.1007/s11032-021-01208-1</li> <li>Reynolds, Matthew P and Lewis, Janet M and Ammar, Karim and Basnet, Bhoja R and Crespo-Herrera, Leonardo and Crossa, Jos\u00e9 and Dhugga, Kanwarpal S and Dreisigacker, Susanne and Juliana, Philomin and Karwat, Hannes and Kishii, Masahiro and Krause, Margaret R and Langridge, Peter and Lashkari, Azam and Mondal, Suchismita and Payne, Thomas and Pequeno, Diego and Pinto, Francisco and Sansaloni, Carolina and Schulthess, Urs and Singh, Ravi P and Sonder, Kai and Sukumaran, Sivakumar and Xiong, Wei and Braun, Hans J (2021). Harnessing translational research in wheat for climate resilience. Journal of Experimental Botany. DOI: 10.1093/jxb/erab256</li> <li>Pook, Torsten and Nemri, Adnane and Gonzalez Segovia, Eric Gerardo and Valle Torres, Daniel and Simianer, Henner and Schoen, Chris-Carolin (2021). Increasing calling accuracy, coverage, and read-depth in sequence data by the use of haplotype blocks. PLOS Genetics. DOI: 10.1371/journal.pgen.1009944</li> <li>Li, Jeremiah H. and Mazur, Chase A. and Berisa, Tomaz and Pickrell, Joseph K. (2021). Low-pass sequencing increases the power of GWAS and decreases measurement error of polygenic risk scores compared to genotyping arrays. Genome Research. DOI: 10.1101/gr.266486.120</li> <li>Tao, Yongfu and Luo, Hong and Xu, Jiabao and Cruickshank, Alan and Zhao, Xianrong and Teng, Fei and Hathorn, Adrian and Wu, Xiaoyuan and Liu, Yuanming and Shatte, Tracey and Jordan, David and Jing, Haichun and Mace, Emma (2021). Extensive variation within the pan-genome of cultivated and wild sorghum. Nature Plants. DOI: 10.1038/s41477-021-00925-x</li> <li>Wolfe, Marnin D and Chan, Ariel W and Kulakow, Peter and Rabbi, Ismail and Jannink, Jean-Luc (2021). Genomic mating in outbred species: predicting cross usefulness with additive and total genetic covariance matrices. Genetics. DOI: 10.1093/genetics/iyab122</li> <li>Ruperao, Pradeep and Thirunavukkarasu, Nepolean and Gandham, Prasad and Selvanayagam, Sivasubramani and Govindaraj, Mahalingam and Nebie, Baloua and Manyasa, Eric and Gupta, Rajeev and Das, Roma Rani and Odeny, Damaris A. and Gandhi, Harish and Edwards, David and Deshpande, Santosh P. and Rathore, Abhishek (2021). Sorghum Pan-Genome Explores the Functional Utility for Genomic-Assisted Breeding to Accelerate the Genetic Gain. Frontiers in Plant Science. DOI: 10.3389/fpls.2021.666342</li> <li>Nyine, Moses and Adhikari, Elina and Clinesmith, Marshall and Aiken, Robert and Betzen, Bliss and Wang, Wei and Davidson, Dwight and Yu, Zitong and Guo, Yuanwen and He, Fei and Akhunova, Alina and Jordan, Katherine W. and Fritz, Allan K. and Akhunov, Eduard (2021). The Haplotype-Based Analysis of Aegilops tauschii Introgression Into Hard Red Winter Wheat and Its Impact on Productivity Traits. Frontiers in Plant Science. DOI: 10.3389/fpls.2021.716955</li> <li>Cerioli, Tommaso and Hernandez, Christopher and Angira, Brijesh and McCouch, Susan and Robbins, Kelly and Famoso, Adam (2021). Development and validation of an optimized marker set for genomic selection in Southern U. S. rice breeding programs. NA. DOI: 10.1002/essoar.10508975.1</li> <li>Cooper, M and Powell, O and Voss-Fels, K P and Messina, C D and Gho, C and Podlich, D W and Technow, F and Chapman, S C and Beveridge, C A and Ortiz-Barrientos, D and Hammer, G L (2020). Modelling selection response in plant-breeding programs using crop models as mechanistic gene-to-phenotype (CGM-G2P) multi-trait link functions. in silico Plants. DOI: 10.1093/insilicoplants/diaa016</li> <li>Torkamaneh, Davoud and Laroche, J\u00e9r\u00f4me and Valliyodan, Babu and O\u2019Donoughue, Louise and Cober, Elroy and Rajcan, Istvan and Vilela Abdelnoor, Ricardo and Sreedasyam, Avinash and Schmutz, Jeremy and Nguyen, Henry T. and Belzile, Fran\u00e7ois (2020). Soybean (Glycine max) Haplotype Map (GmHapMap): a universal resource for soybean translational and functional genomics. Plant Biotechnology Journal. DOI: 10.1111/pbi.13466</li> </ul> <ul> <li>Naik, Yogesh Dashrath and Zhao, Chuanzhi and Channale, Sonal and Nayak, Spurthi N. and Bhutia, Karma L. and Gautam, Ashish and Kumar, Rakesh and Niranjan, Vidya and Shah, Trushar M. and Mott, Richard and Punnuri, Somashekhar and Pandey, Manish K. and Wang, Xingjun and Varshney, Rajeev K. and Thudi, Mahendar (2024). Bioinformatics for Plant Genetics and Breeding Research. Frontiers Technologies for Crop Improvement. DOI: 10.1007/978-981-99-4673-0_3</li> <li>Kaur, Ishveen and Relan, Ashima and Saini, Dinesh Kumar and Kaur, Gurleen and Biswas, Anju and Singh, Lovepreet and Kaur, Shivreet and Sandhu, Karansher Singh (2023). Revisiting the Genomic Approaches in the Cereals and the Path Forward. Smart Plant Breeding for Field Crops in Post-genomics Era. DOI: 10.1007/978-981-19-8218-7_1</li> <li>Joukhadar, Reem and Daetwyler, Hans D. (2022). Data Integration, Imputation, and Meta-analysis for Genome-Wide Association Studies. Data Integration, Imputation, and Meta-analysis for Genome-Wide Association Studies. DOI: 10.1007/978-1-0716-2237-7_11</li> <li>Cooper, Mark and Messina, Carlos D. and Tang, Tom and Gho, Carla and Powell, Owen M. and Podlich, Dean W. and Technow, Frank and Hammer, Graeme L. (2022). Predicting Genotype\u2009\u00d7\u2009Environment\u2009\u00d7 Management (G\u2009\u00d7\u2009E\u2009\u00d7\u2009M) Interactions for the Design of Crop Improvement Strategies: Integrating Breeder, Agronomist, and Farmer Perspectives. Plant Breeding Reviews. DOI: 10.1002/9781119874157.ch8</li> <li>Bajgain, Prabin and Crain, Jared L. and Cattani, Douglas J. and Larson, Steven R. and Altendorf, Kayla R. and Anderson, James A. and Crews, Timothy E. and Hu, Ying and Poland, Jesse A. and Turner, M. Kathryn and Westerbergh, Anna and DeHaan, Lee R. (2022). Breeding Intermediate Wheatgrass for Grain Production. Plant Breeding Reviews. DOI: 10.1002/9781119874157.ch3</li> <li>Ruperao, Pradeep and Gandham, Prasad and Rathore, Abhishek (2022). Construction of Practical Haplotype Graph (PHG) with the Whole-Genome Sequence Data. NA. DOI: 10.1007/978-1-0716-2067-0_15</li> </ul> <ul> <li>Schreiber, Mona and Jayakodi, Murukarthick and Stein, Nils and Mascher, Martin (2024). Plant pangenomes for crop improvement, biodiversity and evolution. Nature Reviews Genetics. DOI: 10.1038/s41576-024-00691-4</li> <li>Ruperao, Pradeep and Rangan, Parimalan and Shah, Trushar and Thakur, Vivek and Kalia, Sanjay and Mayes, Sean and Rathore, Abhishek (2023). The Progression in Developing Genomic Resources for Crop Improvement. Life. DOI: 10.3390/life13081668</li> <li>Jha, Uday Chand and Nayyar, Harsh and von Wettberg, Eric J. B. and Naik, Yogesh Dashrath and Thudi, Mahendar and Siddique, Kadambot H. M. (2022). Legume Pangenome: Status and Scope for Crop Improvement. Plants. DOI: 10.3390/plants11223041</li> <li>Rai, Mayank and Tyagi, Wricha (2022). Haplotype breeding for unlocking and utilizing plant genomics data. Frontiers in Genetics. DOI: 10.3389/fgene.2022.1006288</li> <li>Tay Fernandez, Cassandria G. and Nestor, Benjamin J. and Danilevicz, Monica F. and Marsh, Jacob I. and Petereit, Jakob and Bayer, Philipp E. and Batley, Jacqueline and Edwards, David (2022). Expanding Gene-Editing Potential in Crop Improvement with Pangenomes. International Journal of Molecular Sciences. DOI: 10.3390/ijms23042276</li> <li>Tay Fernandez, Cassandria Geraldine and Nestor, Benjamin John and Danilevicz, Monica Furaste and Gill, Mitchell and Petereit, Jakob and Bayer, Philipp Emanuel and Finnegan, Patrick Michael and Batley, Jacqueline and Edwards, David (2022). Pangenomes as a Resource to Accelerate Breeding of Under-Utilised Crop Species. International Journal of Molecular Sciences. DOI: 10.3390/ijms23052671</li> <li>H\u00fcbner, Sariel (2022). Are we there yet? Driving the road to evolutionary graph-pangenomics. Current Opinion in Plant Biology. DOI: 10.1016/j.pbi.2022.102195</li> <li>Jayakodi, Murukarthick and Schreiber, Mona and Stein, Nils and Mascher, Martin (2021). Building pan-genome infrastructures for crop plants and their use in association genetics. DNA Research. DOI: 10.1093/dnares/dsaa030</li> <li>Della Coletta, Rafael and Qiu, Yinjie and Ou, Shujun and Hufford, Matthew B. and Hirsch, Candice N. (2021). How the pan-genome is changing crop genomics and improvement. Genome Biology. DOI: 10.1186/s13059-020-02224-8</li> <li>Bhat, Javaid Akhter and Yu, Deyue and Bohra, Abhishek and Ganie, Showkat Ahmad and Varshney, Rajeev K. (2021). Features and applications of haplotypes in crop breeding. Communications Biology. DOI: 10.1038/s42003-021-02782-y</li> <li>B\u00f6ndel, Katharina B. and Schmid, Karl J. (2021). Quinoa Diversity and Its Implications for Breeding. NA. DOI: 10.1007/978-3-030-65237-1_7</li> <li>Richards, Chris (2021). Genomics of Plant Gene Banks: Prospects for Managing and Delivering Diversity in the Digital Age. NA. DOI: 10.1007/13836_2021_95</li> </ul>"},{"location":"convenience_commands/","title":"Convenience Commands","text":"<p>Diploid hVCF to gVCF disclaimer</p> <p>There is a known bug in the <code>hvcf2gvcf</code> command that affects diploid hVCF files. This affects all PHGv2 versions after 2.3.7.144. The output gVCF will only contain variants from the first haplotype - variants from the second haplotype are omitted entirely. Haploid hVCF files are not affected. We are actively working on a fix and will update the documentation when it is resolved. If you have questions or need assistance, please [comment on the announcement discussion] (https://github.com/maize-genetics/phg_v2/discussions/347)</p> <p>In addition to the primary commands for the build, imputation, and resequencing pipelines, PHGv2 also provides a suite of \"convenience commands\" for miscellaneous \"quality of life (QoL)\" improvements. In  this document, we will discuss the currently available external commands for performing highly used tasks.</p> <ul> <li> <p> Conversion</p> <p>Commands for converting one file type to another.</p> <p> Go to section</p> </li> <li> <p> Merging</p> <p>Commands for merging various PHG file types.</p> <p> Go to section</p> </li> <li> <p> Statistics</p> <p>Commands for generating summary information in tabular format.</p> <p> Go to section</p> </li> <li> <p> Experimental</p> <p>Prototype commands that are under construction and may have significant changes in future updates.</p> <p> Go to section</p> </li> </ul>"},{"location":"convenience_commands/#conversion","title":"Conversion","text":""},{"location":"convenience_commands/#convert-gvcf-files-to-hvcf-files","title":"Convert gVCF files to hVCF files","text":"<p>Create hVCF files from existing gVCF files created by the PHG</p> <p>Command - <code>gvcf2hvcf</code></p> <p>Example</p> <pre><code>phg gvcf2hvcf \\\n    --bed my/bed/file.bed \\\n    --reference-file my/updated/ref/fasta.fa \\\n    --gvcf-dir gvcf/directory \\\n    --db-path my/phg/db\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--bed</code> BED file with entries that define the haplotype boundaries. <code>\"\"</code> <code>--gvcf-dir</code> Directory containing bgzipped and CSI indexed gVCF files. <code>\"\"</code> <code>--reference-file</code> Path to local Reference FASTA file. <code>\"\"</code> <code>--conda-env-prefix</code> Prefix for the Conda environment to use. If provided, this should be the full path to the Conda environment. Current active Conda environment <code>--db-path</code> Folder name where TileDB datasets and AGC record is stored. If not provided, the current working directory is used. Current working dir <p></p>"},{"location":"convenience_commands/#convert-hvcf-files-to-gvcf-files","title":"Convert hVCF files to gVCF files","text":"<p>Create gVCF files from existing hVCF files created by the PHG</p> <p>Command - <code>hvcf2gvcf</code></p> <p>Example</p> <pre><code>phg hvcf2gvcf \\\n    --reference-file my/updated/ref/fasta.fa \\\n    --hvcf-dir hvcf/directory \\\n    --db-path my/phg/db\n    --output-dir output/directory/for/gvcfs\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--hvcf-dir</code> Path to directory holding hVCF files. Data will be pulled directly from these files instead of querying TileDB. <code>\"\"</code> <code>--reference-file</code> Path to local Reference FASTA file. <code>\"\"</code> <code>--conda-env-prefix</code> Prefix for the Conda environment to use. If provided, this should be the full path to the Conda environment. Current active Conda environment <code>--db-path</code> Folder name where TileDB datasets and AGC record is stored. If not provided, the current working directory is used. Current working dir <code>--output-dir</code> Output directory for the gVCF files. If not provided, the current working directory is used. Current working dir <code>--batch-size</code> Number of sample vcf files to export in a single batch from tiledb <code>5</code> <p></p>"},{"location":"convenience_commands/#create-a-gff-file-from-an-imputed-hvcf-file","title":"Create a GFF file from an imputed hVCF file","text":"<p>Create a path-specific GFF file  from an imputed hVCF file and existing sample GFF files. Useful for creating a GFF file that contains only annotations for paths present in the imputed hVCF file.</p> <p>Command - <code>paths-to-gff</code></p> <p>Example</p> <pre><code>phg paths-to-gff \\\n    --hvcf-file my/hvcf/file.h.vcf \\\n    --key-file my/samples/keyfile.txt \\\n    --output-file output/path_specific.gff\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--hvcf-file</code> Path to hVCF file for which the GFF will be created <code>\"\"</code> <code>--key-file</code> Path to key file containing 2 tab-delimited columns: <ol><li>Sample ID</li><li> The full path to the GFF3 file for that sample</li></ol> <code>\"\"</code> <code>--output-file</code> Full path to the file where the new GFF3 file will be written <code>\"\"</code> <p>Advanced API use</p> <p>For advanced users who would like to leverage this GFF-based data structure in memory for downstream Kotlin pipelines in contrast to handling output files, you may use the following example code:</p> <pre><code>import net.maizegenetics.phgv2.utils.loadGFFsToGff3Feature\nimport net.maizegenetics.phgv2.utils.makeGffFromHvcf\n\n// Same as CLI parameter inputs\nval keyFile  = \"my/samples/keyfile.txt\"\nval hvcfFile = \"my/hvcf/file.h.vcf\"\n\n// Create GFF 'TreeMap' object\nval resTreeMap = loadGFFsToGff3Feature(keyFile)\n\n// Create HTSJDK 'Gff3Feature' set object\nval taxonPathGFF = makeGffFromHvcf(hvcfFile, resTreeMap)\n</code></pre> <p>In the above example, <code>taxonPathGFF</code> is an in-memory HTSJDK <code>Gff3Feature</code>  object that can be used for downstream purposes. See the <code>PathsToGff</code> class source code for further details.</p> <p></p>"},{"location":"convenience_commands/#create-a-ps4g-file-from-read-mapping-data","title":"Create a PS4G file from read mapping data","text":"<p>Convert read mapping data into a PS4G (positional support for gamete) file.</p> <p>Command - <code>convert-rm2ps4g</code></p> <p>Example</p> <pre><code>phg convert-rm2ps4g \\\n    --read-mapping-file /path/to/readmapping.txt \\\n    --output-dir /dir/for/ps4g/output/ \\\n    --hvcf-dir /path/to/hvcf/files/\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--read-mapping-file</code> Path to read mapping file. <code>\"\"</code> <code>--output-dir</code> Output directory for the generated PS4G file. <code>\"\"</code> <code>--hvcf-dir</code> Directory containing hVCF files. <code>\"\"</code> <p></p>"},{"location":"convenience_commands/#merging","title":"Merging","text":""},{"location":"convenience_commands/#merge-gvcf-files","title":"Merge gVCF files","text":"<p>Merge multiple gVCF files into a single gVCF file</p> <p>Command - <code>merge-gvcfs</code></p> <p>Example</p> <pre><code>phg merge-gvcfs \\\n    --input-dir my/gvcf/directory \\\n    --output-file output/merged_gvcfs.g.vcf \\\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--input-dir</code> Path to input gVCF file directory. <code>\"\"</code> <code>--output-dir</code> Path and/or filename for merged gVCF file. <code>\"\"</code> <p></p>"},{"location":"convenience_commands/#merge-hvcf-files","title":"Merge hVCF files","text":"<p>Merge multiple hVCF files into a single hVCF file</p> <p>Command - <code>merge-hvcfs</code></p> <p>Example</p> <pre><code>phg merge-hvcfs \\\n    --input-dir my/hvcf/directory \\\n    --output-file output/merged_hvcfs.h.vcf \\\n    --id-format CHECKSUM \\\n    --reference-file \\\n    --range-bedfile \n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--input-dir</code> Path to input hVCF file directory. <code>\"\"</code> <code>--output-dir</code> Path and/or filename for merged hVCF file. <code>\"\"</code> <code>--id-format</code> ID format for hVCF files. Options are: <code>CHECKSUM</code> or <code>RANGE_SAMPLE_GAMETE</code> (see notes for further details). <code>RANGE_SAMPLE_GAMETE</code> <code>--reference-file</code> Path to reference FASTA file. <code>\"\"</code> <code>--range-bedfile</code> Path to reference range BED file. <code>\"\"</code> <p>Note - <code>id-fomat</code></p> <p>If you select <code>CHECKSUM</code> for the <code>--id-format</code> parameter, the <code>ID</code> values will be MD5 checksums in the <code>##ALT</code> header:</p> <pre><code>##ALT=&lt;ID=06ae4e937668d301e325d43725a38c3f, ...&gt;\n</code></pre> <p>If you select <code>RANGE_SAMPLE_GAMETE</code>, the <code>ID</code> values will change to a <code>reference range/sample/gamete</code> ID format:</p> <pre><code>##ALT=&lt;ID=R000001_LineA_G01, ...&gt;\n</code></pre>"},{"location":"convenience_commands/#statistics","title":"Statistics","text":""},{"location":"convenience_commands/#list-sample-names-from-datasets","title":"List Sample names from datasets","text":"<p>List the sample names from the AGC compressed file, the TileDB gVCF  dataset, and/or the TileDB hVCF dataset</p> <p>Command - <code>list-samples</code></p> <p>Example</p> <pre><code>phg list-samples \\\n    --db-path my/phg/db \\\n    --output-file output/hvcf_samples.txt \\\n    --data-set hvcf \n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--db-path</code> Folder name where TileDB datasets and AGC record is stored. If not provided, the current working directory is used. <code>\"\"</code> <code>--output-file</code> Path and/or filename for the samples list file. <code>\"\"</code> <code>--data-set</code> Storage from which to pull sample names. Must be one of <code>all</code>, <code>agc</code>, <code>gvcf</code>, <code>hvcf</code> <code>hvcf</code> <code>--conda-env-prefix</code> Prefix for the Conda environment to use. If provided, this should be the full path to the Conda environment. Current active Conda environment <p>Example output</p> <pre><code>B73\nKi11\nMo18W\nKi3\n</code></pre> <p></p>"},{"location":"convenience_commands/#create-a-table-of-haplotype-ids-by-reference-range","title":"Create a table of haplotype IDs by reference range","text":"<p>Creates a tab-delimited table of haplotype IDs by reference range coordinates and sample IDs</p> <p>Command - <code>sample-hapid-by-range</code></p> <p>Example</p> <pre><code>phg sample-hapid-by-range \\\n    --input-dir my/hvcf/directory \\\n    --output-file output/hapid_table\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--input-dir</code> Path to directory holding hVCF files. <code>\"\"</code> <code>--output-dir</code> Path and/or filename for haplotype ID table. <code>\"\"</code> <p>Example output</p> <pre><code>#CHROM START   END   B73                                SEEDGWAS1                          SEEDGWAS10\nchr1   1       5000  &lt;c9ecfe3967a71282f3ad7c41d48e0bbf&gt; &lt;b19364bc9a4c07a80986b1ee181446c2&gt; &lt;5c8e72b2e9f11ecc652d5b8e8d0e5bf3&gt;\nchr1   5001    6000  &lt;f162e742c4d30f151ae6276fbebe762c&gt; &lt;fdfdaa361c39cf5b6f13fad195d0e519&gt; &lt;283a8261c193212fd5cf43d208673322&gt;\nchr1   6001    9000  &lt;471d4abbf0545dede647e65915345648&gt; &lt;d6dd5ecea7fb4e6f77f9e630f601b7a8&gt; &lt;13e0ac1a8d12e1aedd6a5302d1e221fd&gt;\n</code></pre>"},{"location":"convenience_commands/#create-a-table-of-haplotype-ids-to-sample","title":"Create a table of haplotype IDs to sample","text":"<p>Creates a tab-delimited table of haplotype IDs to sample gamete. Can be one or multiple samples mapping to each haplotype ID.</p> <p>Command - <code>hapid-sample-table</code></p> <p>Example</p> <pre><code>phg hapid-sample-table \\\n    --hvcf-dir my/hvcf/directory \\\n    --output-file output/hapid_table\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--hvcf-dir</code> Path to directory holding hVCF files. <code>\"\"</code> <code>--output-dir</code> Path and/or filename for haplotype ID table. <code>\"\"</code> <p>Note - <code>--hvcf-dir</code></p> <p>This is intended for use with hVCF files created from aligning assemblies. While this will work with hVCF files from imputation, all the sample names will be the imputed file name and not the sample names associated with the hapids when they were created.</p> <p>Example output</p> <pre><code>\"a81a7df7340ae0f14a6dccce0d9632db\"  Ki3\n\"c45452b07db68928da6f4e14d50ba1e3\"  Mo18W\n\"1b6d29dbb7b380e67b15c5a0f0142cf0\"  Ms71,R2D2\n\"a935ee46a1a1118df309fc34bdb9e5a5\"  B73,Ky21,Ki11\n\"b878dec3587e24c4714fec5131d4dbbb\"  C3PO\n</code></pre>"},{"location":"convenience_commands/#experimental","title":"Experimental","text":""},{"location":"convenience_commands/#initialize-custom-tiledb-instance-for-hvcfs","title":"Initialize custom TileDB instance for hVCFs","text":"<p>Creates a TileDB array instance to house hVCF header data</p> <p>Command - <code>init-hvcf-array</code></p> <p>Example</p> <pre><code>phg init-hvcf-array \\\n    --db-path /path/for/tiledb/instance/\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--db-path</code> Directory name under which TileDB datasets will be created. If this folder does not exist, it will be created. Current working directory <p></p>"},{"location":"convenience_commands/#load-hvcf-data-into-custom-tiledb-instance","title":"Load hVCF data into custom TileDB instance","text":"<p>Loads hVCF data into TileDB array instance that was created with the command <code>init-hvcf-array</code></p> <p>Command - <code>load-hvcf</code></p> <p>Example</p> <pre><code>phg load-hvcf \\\n    --db-path /path/for/tiledb/instance/ \\\n    --hvcf-dir /path/to/hvcf/files/\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--db-path</code> Directory name under which TileDB datasets will be created. If this folder does not exist, it will be created. Current working directory <code>--hvcf-dir</code> Full path to an hVCF file directory \"\" <p></p>"},{"location":"convenience_commands/#query-hvcf-arrays","title":"Query hVCF arrays","text":"<p>Query TileDB arrays created from <code>load-hvcf</code></p> <p>Command - <code>query-hvcf-arrays</code></p> <p>Example <pre><code>phg query-hvcf-arrays \\\n    --db-path /path/for/tiledb/instance/ \\\n    --query-type distinctSamples \\\n    --array-type variants \\\n    --ref-range-file /path/to/refrange/bed/file.bed \\\n    --output-file query_results.txt\n</code></pre></p> <p>Parameters</p> Parameter name Description Default value Required? <code>--db-path</code> Directory name under which TileDB datasets will be created. If this folder does not exist, it will be created. Current working directory <code>--query-type</code> Type of query to perform. Options to choose from are: <ul><li><code>distinctSamples</code></li><li><code>distinctRanges</code></li></ul> \"\" <code>--array-type</code> Type of array to query. Options to choose from are: <ul><li><code>altHeader</code></li><li><code>variants</code></li></ul> <code>variants</code> <code>--ref-range-file</code> Full path to a BED file-formatted list of reference ranges to query. If no file is provided, all reference ranges are queried. All reference ranges <code>--output-file</code> Name of file for query results. \"\""},{"location":"export_data/","title":"Exporting Data","text":"<p>In this document, we will discuss general strategies for exporting data from a PHG database.</p> <p>Note</p> <p>This section will assume you have a pre-existing database loaded with haplotype data. See the \"Building and Loading\" documentation for further information.</p>"},{"location":"export_data/#quickstart","title":"Quickstart","text":"<ul> <li>Export hVCF files from database <pre><code>phg export-vcf \\\n    --db-path /path/to/dbs \\\n    --dataset-type hvcf \\ # can also be 'gvcf'\n    --sample-names LineA,LineB \\ # comma separated list of sample names\n    -o /path/to/output/directory\n</code></pre></li> </ul> <p>Note</p> <p><code>--sample-names</code> can be replaced with <code>--sample-file &lt;file_name.txt&gt;</code>  where <code>file_name.txt</code> is a file containing the sample names, one  per line.</p> <ul> <li> <p>Create FASTA files from hVCF data or database <pre><code>phg create-fasta-from-hvcf \\\n  --hvcf-dir my/hvcf_dir \\ # can also be an individual file ('--hvcf-file')\n  --fasta-type composite \\ # can also be 'haplotype'\n  -o /path/to/output_folder\n</code></pre></p> </li> <li> <p>Data retrieval using BrAPI endpoints and rPHG2 <pre><code>phg start-server \\\n    --db-that /path/to/dbs \\\n    --port 8080\n</code></pre> <pre><code>library(rPHG2)\n\nPHGServerCon(\"localhost\", 8080) |&gt;\n    readHaplotypeData()\n</code></pre></p> </li> </ul>"},{"location":"export_data/#detailed-walkthrough","title":"Detailed walkthrough","text":""},{"location":"export_data/#export-vcf-data","title":"Export VCF data","text":"<p>In PHGv2, we leverage TileDB and TileDB-VCF for efficient storage and querying of VCF data. For this example, let's assume I have a pre-existing PHG database (located in my <code>vcf_dbs</code> directory) that contains several  samples:</p> <ul> <li>LineA</li> <li>LineB</li> <li>LineC</li> </ul> <p>If I want to export hVCF files for a given set of samples  (<code>LineA</code> and <code>LineC</code>) to an output directory (in my case  <code>output/hvcf_files</code>), I can use the <code>export-vcf</code> command:</p> <pre><code>phg export-vcf \\\n    --db-path vcf_dbs \\\n    --dataset-type hvcf \\\n    --sample-names LineA,LineC \\\n    -o output/hvcf_files\n</code></pre> <p>This command uses several parameters:</p> <ul> <li><code>--db-path</code> - path to directory storing the TileDB instances.</li> <li><code>--dataset-type</code> - what type of data do you want to extract? This   can either be:<ul> <li>hVCF (<code>hvcf</code>) data (default parameter)</li> <li>gVCF (<code>gvcf</code>) data</li> </ul> </li> <li><code>--sample-names</code> - a comma (<code>,</code>) separated list of sample IDs.</li> <li><code>-o</code> - output directory of VCF data.</li> <li><code>--regions-file</code> - a file of positions to be exported. Can be a     BED file or a VCF file.</li> </ul> <p>Note</p> <p>Make sure there is no whitespace between sample IDs. For example:</p> <ul> <li><code>LineA,LineB</code> \u2705</li> <li><code>LineA , LineB</code> \u274c</li> </ul> <p>Users may instead use the <code>--sample-file</code> parameter to specify a file  that contains the sample names, one per line. For example, if I have a text file called <code>sample_names.txt</code>, the contents of the file would look like the following:</p> <pre><code>LineA\nLineB\n</code></pre> <p>...and would be passed to the <code>export-vcf</code> command:</p> <p><pre><code>phg export-vcf \\\n    --db-path vcf_dbs \\\n    --dataset-type hvcf \\\n    --sample-file sample_names.txt \\\n    -o output/hvcf_files\n</code></pre> If input is specified for the <code>--regions-file</code> parameter, only  variants overlapping those positions will be exported.  The regions-file must be either a  BED file or a  VCF file, and  must have either a <code>.bed</code> or a <code>.vcf</code> extension.</p> <p>For example, if I want the regions from <code>1</code> to <code>5000</code> base pairs (bp) on chromosome 3 (in my case the ID would be <code>chr03</code>), I could make a BED  file:</p> <pre><code>chr03 0 5000\n</code></pre> <p>Note</p> <p>BED files are 0-based, so plan accordingly!</p> <p>...or this could be a VCF file that contains a data line for <code>chr03</code> region information for the <code>CHROM</code>, <code>POS</code>, and <code>INFO</code> columns with the <code>INFO</code> column containing a <code>END</code> field. For example:</p> <pre><code>#CHROM  POS ID  REF ALT QUAL  FILTER  INFO  ...\nchr03   1   r1  A   T   50    PASS    END=5000\n</code></pre>"},{"location":"export_data/#create-fasta-data","title":"Create FASTA data","text":"<p>While haplotype sequences are abstracted to MD5 hashes in hVCF files, sequence information can be recapitulated from these hash  values using the <code>create-fasta-from-hvcf</code> command:</p> <pre><code>phg create-fasta-from-hvcf \\\n  --hvcf-file my_sample.h.vcf \\\n  --fasta-type composite \\\n  -o /path/to/outputFolder\n</code></pre> <p>As the name of this command implies, we are creating FASTA files of nucleotide sequence data from a single hVCF file or a collection of hVCF files by specifying a directory. The output FASTA files will be written (one FASTA per hVCF file) to the specified output directory (<code>-o</code>). The format of the file names will be <code>sample_name_type.fa</code>  where <code>sample_name</code> is the name of the sample from the hVCF file name  and <code>type</code> is the type of fasta file created (<code>composite</code> or  <code>haplotype</code>). The following parameters may be used:</p> <ul> <li>input type (you can only select one):<ul> <li><code>--hvcf-file</code> - path to an hVCF file. Can be substituted with    <code>--hvcf-dir</code>.</li> <li><code>--hvcf-dir</code> - path to a directory containing hVCF files. Can   be substituted with <code>--hvcf-file</code>.</li> </ul> </li> <li><code>--fasta-type</code> - what type of FASTA format do you want to use?<ul> <li><code>composite</code> - generate a FASTA file that contains all haplotypes    concatenated together by consecutive reference ranges. This    composite or \"pseudo\" genome can be used for the resequencing    pipeline.</li> <li><code>haplotype</code> - generate a FASTA file where each haplotype is a   separate FASTA entry. Useful for read mapping, imputation   or simple haplotype sequence retrieval.</li> <li>'pangenomeHaplotype' - generate a FASTA file where we output   all the haplotypes from all the hvcf files in the directory</li> <li>'rangeFasta' - outputs one file per reference range specified   by the bedfile.  Each file contains the haplotype sequences   for each sample for the specified range.</li> </ul> </li> <li><code>-o</code> - output path to directory for the created fasta files.</li> </ul>"},{"location":"export_data/#data-retrieval-using-brapi-endpoints-and-rphg","title":"Data retrieval using BrAPI endpoints and rPHG","text":"<p>While the above commands allow for individual-oriented access to PHG data, another option is to start a \"RESTful\" web service. This service can provide access to a centralized PHG  database, allowing multiple individuals in a team to simultaneously retrieve PHG-relevant information. The following web service  leverages the Breeding API (BrAPI which provides a standard, community-driven collection of web retrieval calls relevant to plant breeding.</p> <p>To create a web service for serving PHG data, we can use the <code>start-server</code> command:</p> <pre><code>phg start-server \\\n    --db-path vcf_dbs \\\n    --port 8080\n</code></pre> <p>This command takes only two arguments:</p> <ul> <li><code>--db-path</code> - path to directory storing the TileDB instances.</li> <li><code>--port</code> - web server port    for the network connection. Defaults to <code>8080</code>.</li> </ul> <p>Once this command is run, a web service to <code>localhost</code> will start and data can be retrieved:</p> <ul> <li> <p>manually using  BrAPI endpoints and  cURL:   <pre><code># An example pointing to the 'samples' BrAPI endpoint\n$ curl http://localhost:8080/brapi/v2/samples\n</code></pre> <pre><code># An example pointing to a composite hVCF file\n$ curl http://localhost:8080/brapi/v2/variantsets\n</code></pre></p> </li> <li> <p>using the R package, <code>rPHG2</code>. Since this is a separate library,   more information about the library and retrieval methods can be   found here.</p> </li> </ul>"},{"location":"hvcf_region_handling/","title":"Haplotype Region Handling","text":"<p>In this document, we will go into further detail about how regions are reported in the alternative allele field of an hVCF file  and possible edge cases that may arise in hVCF creation.</p>"},{"location":"hvcf_region_handling/#alternative-allele-fields-an-overview","title":"Alternative allele fields - an overview","text":"<p>Note</p> <p>For more information about this hVCF field and other general information about hVCF specifications, please review the haplotype VCF specification article.</p> <p>The alternative allele (<code>##ALT</code>) field contains key-value information that represents the haplotype sequence and ID for a given reference-range in the PHG. One key-value pair is the <code>Regions</code> key. This represents the physical location of the sequence in the genome assembly FASTA file.</p> <p>A common example of this would be the following example:</p> <pre><code>##ALT=&lt;ID=57705b1e2541c7634ea59a48fc52026f,Description=\"haplotype data for line: LineA\",Source=\"data/test/smallseq/LineA.fa\",Regions=1:14-19,Checksum=Md5,RefRange=06ae4e937668d301e325d43725a38c3f&gt;\n</code></pre> <p>...we have the following regions key:</p> <pre><code>Regions=1:14-19\n</code></pre> <p>...for the following assembly/sample:</p> <pre><code>Description=\"haplotype data for line: LineA\"\n</code></pre> <p>This indicates that for <code>LineA</code>, we have a haplotype sequence that aligned against the given reference range (denoted with the MD5 hash, <code>06ae4e937668d301e325d43725a38c3f</code>) at chromosome 1 (<code>1:</code>) between the values of 14 and 19 (<code>14-19</code>) base-pairs. We can represent this graphically with a dot plot:</p> <p></p> <p>In the above example, <code>LineA</code> (y-axis) is aligned against our reference genome, in this case, identified as <code>Reference</code> (x-axis). The reference range boundary is located on chromosome 1 between the base pairs of 3 and 8 (denoted by the dashed lines). Given our  hypothetical alignment scenario, our sample, <code>LineA</code>, aligned against  the reference genome also on chromosome 1, but due to structural variations between the two genomes, matches at different base-pair positions (in this case between <code>14</code> and <code>19</code> base-pairs). Using this region value, we can navigate back to <code>LineA</code>'s FASTA file and identify the sequence region:</p> <p></p>"},{"location":"hvcf_region_handling/#possible-edge-cases-inversions","title":"Possible edge cases - inversions","text":"<p>Given the possibility of structural diversity events between the reference and assembly genomes. There are edge cases where a reference range boundary can land on  inversion regions. For example, let's illustrate this scenario with another dot plot:</p> <p></p> <p>Here, when we align <code>LineA</code> against the reference, there is an  inversion at the end of this boundary. How do we represent this using our <code>Region</code> key-value pairing scheme in the alternate allele field? The <code>Region</code> key can house multiple region elements using comma (<code>,</code>) separated values. Using the above example, this can be displayed as follows:</p> <pre><code>Regions=\"1:13-16,1:21-19\"\n</code></pre> <p>In the above example, we provide multiple regions (separated by  commas) to indicate the normal and inverted portions of the  alignment:</p> Region Chromosome Range (bp) Type <code>1:13-16</code> 1 13 to 16 normal <code>1:21-19</code> 1 21 to 19 inverted <p>Since the last element of the region is inverted, the larger value  will be the first value left of the <code>-</code> character and the smaller value will be to the right of the <code>-</code> character. This will indicate to PHGv2 that reverse transcription of the sequence will need to occur before the MD5 hashing step.</p>"},{"location":"hvcf_specifications/","title":"hVCF - haplotype Variant Call Format Specification","text":"<ul> <li>Specification version: <code>v2.4</code></li> <li>Date: 2024-11-19</li> </ul>"},{"location":"hvcf_specifications/#overview","title":"Overview","text":"<p>hVCF stands for haplotype Variant Call Format. This  format is used to store and encode haplotype information across  samples from the PHG (Practical Haplotype Graph). An hVCF file is based on the standards of a VCF file, specifically,  VCF v4.2. This  format leverages VCF's symbolic allele information from the <code>ALT</code>  field.</p>"},{"location":"hvcf_specifications/#the-hvcf-specification","title":"The hVCF specification","text":"<p>hVCF files can be broken into 3 main components: * Meta-information lines * Header line * Data lines containing information for each reference range   * Fixed fields   * Haplotype fields</p>"},{"location":"hvcf_specifications/#an-example","title":"An example","text":"<p>The following code block illustrates a formatted and merged example hVCF file:</p> <pre><code>##fileformat=VCFv4.2\n##FILTER=&lt;ID=PASS,Description=\"All filters passed\"&gt;\n##ALT=&lt;ID=06ae4e937668d301e325d43725a38c3f,Description=\"haplotype data for line: Ref\",Source=\"data/test/smallseq/Ref.fa\",SampleName=Ref,Regions=1:45001-49500,Checksum=06ae4e937668d301e325d43725a38c3f,RefChecksum=06ae4e937668d301e325d43725a38c3,RefRange=1:45001-49500&gt;\n##ALT=&lt;ID=073286a82fe47d6a370e8a7a3803f1d3,Description=\"haplotype data for line: Ref\",Source=\"data/test/smallseq/Ref.fa\",SampleName=Ref,Regions=1:39501-44000,Checksum=073286a82fe47d6a370e8a7a3803f1d3,RefChecksum=073286a82fe47d6a370e8a7a3803f1d,RefRange=1:39501-44000&gt;\n##ALT=&lt;ID=105c85412229b45439db1f03c3f064f4,Description=\"haplotype data for line: Ref\",Source=\"data/test/smallseq/Ref.fa\",SampleName=Ref,Regions=1:27501-28500,Checksum=105c85412229b45439db1f03c3f064f4,RefChecksum=105c85412229b45439db1f03c3f064f,RefRange=1:27501-28500&gt;\n##ALT=&lt;ID=105e63346a01d88e8339eddf9131c435,Description=\"haplotype data for line: Ref\",Source=\"data/test/smallseq/Ref.fa\",SampleName=Ref,Regions=2:50501-55000,Checksum=105e63346a01d88e8339eddf9131c435,RefChecksum=105e63346a01d88e8339eddf9131c43,RefRange=2:50501-55000&gt;\n##ALT=&lt;ID=2c4b8564bbbdf70c6560fdefdbe3ef6a,Description=\"haplotype data for line: Ref\",Source=\"data/test/smallseq/Ref.fa\",SampleName=Ref,Regions=2:34001-38500,Checksum=2c4b8564bbbdf70c6560fdefdbe3ef6a,RefChecksum=2c4b8564bbbdf70c6560fdefdbe3ef6,RefRange=2:34001-38500&gt;\n##ALT=&lt;ID=347f0478b1a553ef107243cb60a9ba7d,Description=\"haplotype data for line: Ref\",Source=\"data/test/smallseq/Ref.fa\",SampleName=Ref,Regions=2:11001-12000,Checksum=347f0478b1a553ef107243cb60a9ba7d,RefChecksum=347f0478b1a553ef107243cb60a9ba7,RefRange=2:11001-12000&gt;\n##ALT=&lt;ID=39f96726321b329964435865b3694fd2,Description=\"haplotype data for line: Ref\",Source=\"data/test/smallseq/Ref.fa\",SampleName=Ref,Regions=2:49501-50500,Checksum=39f96726321b329964435865b3694fd2,RefChecksum=39f96726321b329964435865b3694fd,RefRange=2:49501-50500&gt;\n##ALT=&lt;ID=43687e13112bbe841f811b0a9de82a94,Description=\"haplotype data for line: Ref\",Source=\"data/test/smallseq/Ref.fa\",SampleName=Ref,Regions=2:22001-23000,Checksum=43687e13112bbe841f811b0a9de82a94,RefChecksum=43687e13112bbe841f811b0a9de82a9,RefRange=2:22001-23000&gt;\n##ALT=&lt;ID=546d1839623a5b0ea98bbff9a8a320e2,Description=\"haplotype data for line: Ref\",Source=\"data/test/smallseq/Ref.fa\",SampleName=Ref,Regions=1:1-1000,Checksum=546d1839623a5b0ea98bbff9a8a320e2,RefChecksum=546d1839623a5b0ea98bbff9a8a320e2,RefRange=1:1-1000&gt;\n##ALT=&lt;ID=57705b1e2541c7634ea59a48fc52026f,Description=\"haplotype data for line: Ref\",Source=\"data/test/smallseq/Ref.fa\",SampleName=Ref,Regions=1:1001-5500,Checksum=57705b1e2541c7634ea59a48fc52026f,RefChecksum=57705b1e2541c7634ea59a48fc52026f,RefRange=1:1001-5500&gt;\n##ALT=&lt;ID=1bda8c63ae8e2f3678b85bac0ee7b8b9,Description=\"haplotype data for line: B97\",Source=\"data/test/smallseq/B97.fa\",SampleName=B97,Regions=1:1250-6750,Checksum=1bda8c63ae8e2f3678b85bac0ee7b8b9,RefChecksum=57705b1e2541c7634ea59a48fc52026f,RefRange=1:1001-5500&gt;\n##ALT=&lt;ID=5fedf293a1a5443cc896d59f12d1b92f,Description=\"haplotype data for line: CML231\"Source=\"data/test/smallseq/CML231.fa\",SampleName=CML231,Regions=2:22001-23000,Checksum=5fedf293a1a5443cc896d59f12d1b92f,RefChecksum=43687e13112bbe841f811b0a9de82a94,RefRange=2:22001-23000&gt;\n##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt;\n##INFO=&lt;ID=END,Number=1,Type=Integer,Description=\"Stop position of the interval\"&gt;\n##contig=&lt;ID=1,length=55000&gt;\n##contig=&lt;ID=2,length=55000&gt;\n##reference=https://s3.amazonaws.com/maizegenetics/phg/phgV2Test/Ref.fa\n#CHROM POS   ID REF ALT                                                                   QUAL FILTER INFO      FORMAT Ref B97 CML231\n1      1     .  G   &lt;546d1839623a5b0ea98bbff9a8a320e2&gt;                                    .    .      END=1000  GT     1   1   1\n1      1001  .  A   &lt;57705b1e2541c7634ea59a48fc52026f&gt;,&lt;1bda8c63ae8e2f3678b85bac0ee7b8b9&gt; .    .      END=5500  GT     1   2   1\n1      27501 .  G   &lt;105c85412229b45439db1f03c3f064f4&gt;                                    .    .      END=28500 GT     1   1   1\n1      39501 .  G   &lt;073286a82fe47d6a370e8a7a3803f1d3&gt;                                    .    .      END=44000 GT     1   1   1\n1      45001 .  G   &lt;06ae4e937668d301e325d43725a38c3f&gt;                                    .    .      END=49500 GT     1   1   1\n2      11001 .  A   &lt;347f0478b1a553ef107243cb60a9ba7d&gt;                                    .    .      END=12000 GT     1   1   1\n2      22001 .  A   &lt;43687e13112bbe841f811b0a9de82a94&gt;,&lt;5fedf293a1a5443cc896d59f12d1b92f&gt; .    .      END=23000 GT     1   1   2\n2      34001 .  A   &lt;2c4b8564bbbdf70c6560fdefdbe3ef6a&gt;                                    .    .      END=38500 GT     1   1   1\n2      49501 .  G   &lt;39f96726321b329964435865b3694fd2&gt;                                    .    .      END=50500 GT     1   1   1\n2      50501 .  G   &lt;105e63346a01d88e8339eddf9131c435&gt;                                    .    .      END=55000 GT     1   1   1\n</code></pre> <p>Note</p> <p>In the prior example, the hVCF output columns below the header line (e.g. below the line starting with <code>#CHROM</code>) are formatted for  visual clarity. In a real example, delimiters are tab (<code>\\t</code>) based.</p>"},{"location":"hvcf_specifications/#meta-information-lines","title":"Meta-information lines","text":"<p>The header portion of an hVCF file contain rows of \"meta-information\" , which are lines that start with <code>##</code> and must appear first in the  file. Like a VCF file, hVCF files can contain both unstructured  and structured meta-information.</p> <p>Unstructured meta-information is characterized by a  straightforward pairing of <code>key=value</code> logic. For instance, in the  previous illustration, the <code>##fileformat=VCF4.2</code> represents  unstructured meta-information, with <code>##=fileformat</code> serving as the  key and <code>VCF4.2</code> as the corresponding value.</p> <p>Structured meta-information also consists of a key-value pair, but in this case, the value is a collection of additional key-value pairs separated by a comma (<code>,</code>) and enclosed with angle brackets (<code>&lt;</code> and <code>&gt;</code>). In our prior example, sequence information fields (e.g. <code>##contig=&lt;ID=chr7,length=461&gt;</code>) represent structured  meta-information where <code>##contig</code> is the primary key and  <code>&lt;ID=chr7,length=461&gt;</code> is the value containing nested key-value pairs:</p> <ul> <li><code>ID=chr7</code></li> <li><code>length=461</code></li> </ul>"},{"location":"hvcf_specifications/#file-format-fileformat-field","title":"File format (<code>##fileformat</code>) field","text":"<p>Similar to VCF, a single line containing file format information (e.g. <code>fileformat</code>) must be the first line in the file. In the case of hVCF files, the version must be version 4.4 of the VCF specification (<code>VCFv4.4</code>).</p>"},{"location":"hvcf_specifications/#alternative-allele-alt-field","title":"Alternative allele (<code>##ALT</code>) field","text":"<p>The primary driver of the hVCF specification is information stored within the structured alternative allele field. At its core, the  alternative allele field contains two primary key-values pairs, the  <code>ID</code> and <code>Description</code> which describe symbolic alternate alleles in  the <code>ALT</code> column of VCF records. While this field is usually used  to describe possible structural variants and  IUPAC ambiguity codes, here it is used to represent a haplotype sequence and ID for a  given reference range. This is achieved by defining the <code>ID</code> value with an MD5 checksum of the given haplotype sequence and defining the <code>Description</code> value with information about the origin of the haplotype sequence. </p> <p>Since this haplotype sequence is (I) derived from a particular  sample, (II) related to reference range information,  and (III) has its own positional information, we can populate  the alternative allele field with additional key-value information.  Take the following example:</p> <pre><code>##ALT=&lt;ID=1bda8c63ae8e2f3678b85bac0ee7b8b9,Description=\"haplotype data for line: B97\",Source=\"data/test/smallseq/B97.fa\",SampleName=B97,Regions=1:1250-6750,Checksum=1bda8c63ae8e2f3678b85bac0ee7b8b9,RefChecksum=57705b1e2541c7634ea59a48fc52026f,RefRange=1:1001-5500&gt;\n</code></pre> <p>Here, we have the following information:</p> Key Value Description <code>ID</code> <code>1bda8c63ae8e2f3678b85bac0ee7b8b9</code> Identifier for the given haplotype sequence. Can be either the MD5 checksum or a 1-based genomic range identifier format (i.e., <code>contig:start-end</code>) <code>Description</code> <code>\"haplotype data for line: B97\"</code> Information about the origin of the haplotype sequence. <code>Source</code> <code>\"data/test/smallseq/B97.fa\"</code> Fasta file ID and path containing haplotype sequence <code>SampleName</code> <code>B97</code> The sample ID from which the haplotype originated. <code>Regions</code> <code>1:1250-6750</code> List of genomic regions which make up the haplotype. Regions are also represented in a 1-based genomic range identifier format (i.e., <code>contig:start-end</code>). If sub-regions are present, they will be separated by commas (e.g., <code>1:100-200,1:205-300</code>). <code>Checksum</code> <code>1bda8c63ae8e2f3678b85bac0ee7b8b9</code> The MD5 checksum for the haplotype sequence. <code>RefChecksum</code> <code>06ae4e937668d301e325d43725a38c3f</code> The MD5 checksum for the reference range sequence. If the sample in question is the reference assembly, this value will be the same as the value found in the <code>Checksum</code> key. <code>RefRange</code> <code>1:45001-49500</code> The genomic region the reference range sequence originate. Also represented in a 1-based genomic range identifier format (i.e., <code>contig:start-end</code>)."},{"location":"hvcf_specifications/#individual-format-format-field","title":"Individual format (<code>##FORMAT</code>) field","text":"<p>The meta-information contained in the individual format field closely  adheres to the VCF specification. This structured field provides a  description of the IDs found within the <code>FORMAT</code> column of the data  rows. The necessary keys are as follows:</p> Key Description <code>ID</code> Identifier for <code>FORMAT</code> entry <code>Number</code> Number (integer) of values representing <code>ID</code> <code>Type</code> Data type for <code>ID</code> <code>Description</code> Descriptive information about <code>ID</code>"},{"location":"hvcf_specifications/#information-info-field","title":"Information (<code>##INFO</code>) field","text":"<p>Much like the <code>##FORMAT</code> field, the <code>##INFO</code> field is a structured  meta-information field that provides details pertaining to each  reference range and the corresponding haplotype data contained within  those reference ranges. Similar to the <code>##FORMAT</code> field, the  necessary keys are as follows:</p> Key Description <code>ID</code> Identifier for <code>FORMAT</code> entry. Defaults to <code>END</code> <code>Number</code> Number (integer) of values representing <code>ID</code> <code>Type</code> Data type for <code>ID</code> <code>Description</code> Descriptive information about <code>ID</code> <p>In order to properly represent the information regarding reference ranges, the following values are required:</p> Value Description <code>End</code> End position of reference range (bp) <p>By combining the values identified within the <code>POS</code> column and the <code>END</code> value, we can specify the total length of the reference range along with assembly information.</p>"},{"location":"hvcf_specifications/#sequence-information-contig-field","title":"Sequence information (<code>##contig</code>) field","text":"<p>The contig field is used to detail additional attributes for each sequence represented within the haplotype data. For now, this is a structured field requiring the identifier (<code>ID</code>) for the  sequence and the length of the mentioned sequence (<code>length</code>)</p>"},{"location":"hvcf_specifications/#header","title":"Header","text":"<p>Like the VCF specifications, the 8 mandatory tab-delimited (<code>\\t</code>)  column headers are required:</p> <ul> <li><code>#CHROM</code></li> <li><code>POS</code></li> <li><code>ID</code></li> <li><code>REF</code></li> <li><code>ALT</code></li> <li><code>QUAL</code></li> <li><code>FILTER</code></li> </ul> <p>Since genotype (i.e. haplotype information) data is also required, the <code>FORMAT</code> column is also required with the <code>GT</code> identifier. More information about each of these fields is discussed in the next section.</p> <p>Note</p> <p>The end of the line must have no tab characters (<code>\\t</code>).</p>"},{"location":"hvcf_specifications/#data-lines","title":"Data lines","text":""},{"location":"hvcf_specifications/#fixed-fields","title":"Fixed fields","text":"<p>There are 8 fixed fields for each reference range record:</p> Field Description <code>CHROM</code> sequence identifier <code>POS</code> start position of reference range <code>ID</code> an optional identifier for a reference range record <code>REF</code> The allele at the start position for reference haplotype sequence <code>ALT</code> MD5 hash sums for each possible haplotype sequence detected at a given reference range record <code>QUAL</code> quality score (needed to satisfy VCF specifications) <code>FILTER</code> filter status (needed to satisfy VCF specifications) <code>INFO</code> information field used to represent the end position (<code>END</code>) value for the reference range record"},{"location":"hvcf_specifications/#haplotype-fields","title":"Haplotype fields","text":"<p>Haplotype information must be specified by first creating a format field (<code>FORMAT</code>) field along with a genotype (<code>GT</code>) identifier.</p> <p>The following fields proceeding the <code>FORMAT</code> field are specified with  the sample (e.g. taxa) identifier for each given sample referenced in the hVCF file. For example, let's take a look at a given record in the prior example (with added header for additional clarity):</p> <pre><code>#CHROM POS   ID REF ALT                                                                   QUAL FILTER INFO      FORMAT Ref B97 CML231\n1      1001  .  A   &lt;57705b1e2541c7634ea59a48fc52026f&gt;,&lt;1bda8c63ae8e2f3678b85bac0ee7b8b9&gt; .    .      END=5500  GT     1   2   1\n</code></pre> <p>One thing you will notice is that there are no calls to the  \"reference\" allele field; only calls to the alternate field since these allele values represent the haplotype sequence in MD5 hash form. Allele values, if using haploid path finding, are represented using  singular values  (e.g. <code>1</code>, <code>2</code>) which represent the indexed order of haplotype  sequences in the <code>ALT</code> field. In other terms, if a sample has an  allele value of <code>1</code>, this would refer to the first symbolic allele  in the <code>ALT</code> field for the haploid value.</p> <p>Using this information with the prior example, we can infer the  following haplotype sequence information for the given reference  range record (<code>1:1001-5500</code>):</p> Sample ID Allele values MD5 symbolic allele <code>Ref</code> <code>1</code> <code>57705b1e2541c7634ea59a48fc52026f</code> <code>B97</code> <code>2</code> <code>1bda8c63ae8e2f3678b85bac0ee7b8b9</code> <code>CML231</code> <code>1</code> <code>57705b1e2541c7634ea59a48fc52026f</code> <p>Alternatively, allele values in hVCF files can be generated using  diploid path finding during the PHGv2 imputation process. Here is an example entry of this:</p> <pre><code>#CHROM POS   ID REF ALT                                                                   QUAL FILTER INFO      FORMAT Ref B97 CML231\n1      1001  .  A   &lt;57705b1e2541c7634ea59a48fc52026f&gt;,&lt;1bda8c63ae8e2f3678b85bac0ee7b8b9&gt; .    .      END=5500  GT     1|1 2|1 1|1\n</code></pre> <p>Allele values are separated with a \"phased\" indicator (<code>|</code>) and never with an \"unphased\" indicator (<code>/</code>). Similar to haploid path finding, allele values represent the indexed order of haplotype sequences in the <code>ALT</code> field. In other terms, if a sample has an  allele value of <code>2|1</code>, this would refer to the second symbolic  allele in the <code>ALT</code> field for the first gamete and the first  symbolic allele for the second gamete.</p> <p>Using this information with the prior example, we can infer the following haploid sequence information for the given reference range record (<code>1:1001-5500</code>) using diploid values:</p> Sample ID Allele values MD5 symbolic allele (gamete 1) MD5 symbolic allele (gamete 2) <code>Ref</code> <code>1|1</code> <code>57705b1e2541c7634ea59a48fc52026f</code> <code>57705b1e2541c7634ea59a48fc52026f</code> <code>B97</code> <code>2|1</code> <code>1bda8c63ae8e2f3678b85bac0ee7b8b9</code> <code>57705b1e2541c7634ea59a48fc52026f</code> <code>CML231</code> <code>1|1</code> <code>57705b1e2541c7634ea59a48fc52026f</code> <code>57705b1e2541c7634ea59a48fc52026f</code>"},{"location":"imputation/","title":"Imputation","text":"<p>Imputation disclaimer</p> <p>This Imputation method is deprecated. Please use the new \"RopeBWT3 Imputation\" method instead.</p> <p>With this imputation method, we are experiencing issues, where some PHGs have low mapping rates (~20% with WGS reads),  causing errors in the path-finding steps.</p> <p>PHGv2 is under active development and we expect to encounter bugs. We aim to report bugs as soon as they are discovered.</p> <p>In this document, we will discuss the steps needed to perform imputation using the PHG:</p> <ol> <li>Export hVCF data</li> <li>Index k-mers from exported hVCF data</li> <li>Map short reads</li> <li>Find paths</li> </ol> <p>Note</p> <p>The steps detailed in this document build on the materials from the \"Building and Loading\" documentation.  Please review this if you have not worked with the PHG before!</p>"},{"location":"imputation/#quick-start","title":"Quick start","text":"<ul> <li> <p>Get list of samples to export:   <pre><code>phg list-samples \\\n    --db-path /my/db/uri \\\n    --data-set hvcf \\\n    --output-file /my/sample_names.txt\n</code></pre></p> </li> <li> <p>Export hVCF data:   <pre><code>phg export-vcf \\\n    --db-path /my/db/uri \\\n    --dataset-type hvcf \\\n    --sample-file /my/sample_names.txt \\\n    --output-dir /my/hvcf/dir\n</code></pre></p> </li> <li> <p>Index k-mers:   <pre><code>phg build-kmer-index \\\n    --db-path /my/db/uri \\\n    --hvcf-dir /my/hvcf/dir\n</code></pre></p> </li> <li> <p>Map short reads   <pre><code>phg map-kmers \\\n    --hvcf-dir /my/hvcf/dir \\\n    --kmer-index /my/hvcf/dir/kmerIndex.txt \\\n    --key-file /my/path/keyfile \\\n    --output-dir /my/mapping/dir\n</code></pre></p> </li> <li> <p>Find paths   <pre><code>phg find-paths \\\n    --path-keyfile /my/path/keyfile \\\n    --hvcf-dir /my/hvcf/dir \\\n    --reference-genome /my/ref/genome \\\n    --path-type haploid \\\n    --output-dir /my/imputed/hvcfs\n</code></pre></p> </li> <li> <p>OPTIONAL: Get SNPs from imputed hVCFs   <pre><code>phg hvcf2gvcf \\\n    --hvcf-dir /my/imputed/hvcfs \\\n    --db-path /my/db/uri \\\n    --reference-file /my/ref/genome \\\n    --output-dir /my/gvcf/dir\n</code></pre></p> </li> </ul>"},{"location":"imputation/#detailed-walkthrough","title":"Detailed walkthrough","text":""},{"location":"imputation/#hvcf-export","title":"hVCF export","text":"<p>Note</p> <p>This step is currently needed, but will be removed in the future as  we can do direct connections to the hVCF TileDB instance.</p> <p>Where we last left off in the \"Build and Load\" steps, we had an example directory that looks similar to the following:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u2514\u2500\u2500 vcf_files/\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre> <p>Note</p> <p>The following steps in the rest of this document will assume we are  in the top level of our <code>phg_v2_example</code> working directory.</p> <p>For this example imputation pipeline, we will be using haplotypes  generated from two samples in our database:</p> <ul> <li><code>LineA</code></li> <li><code>LineB</code></li> </ul> <p>If you do not have hVCF files for samples  you wish to impute against already generated, we will first need to  export this haplotype data in the form of hVCF data from the TileDB  instance. This is done using the <code>export-vcf</code> command:</p> <p><pre><code>./phg export-vcf \\\n    --db-path vcf_dbs \\\n    --dataset-type hvcf \\\n    --sample-file /my/sample_names.txt \\\n    --output-dir output/vcf_files\n</code></pre> This command takes in 4 parameters:</p> <ul> <li><code>--db-path</code> - path to directory storing the TileDB instances. The   AGC compressed genomes will be placed here on completion.</li> <li><code>--dataset-type</code> - the type of dataset to export. In this case, we   are exporting the hVCF data.</li> <li><code>--sample-file</code> - text file with list of sample names to export, one per line.</li> <li><code>--output-dir</code> - the directory to place the exported hVCF files.</li> </ul> <p>In our above example, we use the <code>--sample-file</code> parameter, but if your sample list is small, you can use the <code>--sample-names</code> parameter. An example of a sample file is as follows:</p> <pre><code>$ cat sample_names.txt\n\nLineA\nLineB\n</code></pre> <p>Example command using --sample-names parameter:</p> <pre><code>./phg export-vcf \\\n    --db-path vcf_dbs \\\n    --dataset-type hvcf \\\n    --sample-names LineA,LineB \\\n    --output-dir output/vcf_files\n</code></pre> <p>After the export command, our directory structure will have new files added to the <code>vcf_files</code> directory:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u2514\u2500\u2500 vcf_files\n\u2502       \u251c\u2500\u2500 LineA.h.vcf *\n\u2502       \u2514\u2500\u2500 LineB.h.vcf *\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre>"},{"location":"imputation/#k-mer-indexing","title":"K-mer indexing","text":"<p>In order to run the later k-mer read mapping steps  (and run them quickly with a low memory footprint!), we will first need to build an index containing all 31 bp length  k-mers identified in the haplotypes exported from the prior step and store this as a flat file. This is performed using the <code>build-kmer-index</code> command:</p> <pre><code>./phg build-kmer-index \\\n    --db-path vcf_dbs \\\n    --hvcf-dir output/vcf_files\n</code></pre> <p>This command has 2 required parameters and 1 optional  parameter that I will specify for example purposes:</p> <ul> <li><code>--db-path</code> - path to directory storing the TileDB instances and   <code>assemblies.agc</code> file made in the    \"Compress FASTA files\"   section of the \"Build and Load\" documentation.</li> <li><code>--hvcf-dir</code> - the directory containing the hVCF files. This is the   output directory from the <code>export-vcf</code> command.  Right now this is    required, but will be optional in the future.</li> <li><code>--index-file</code> (optional) - The full path of the k-mer index   file. If not specified, the default path will be:</li> <li><code>&lt;--hvcf-dir input&gt;/kmerIndex.txt</code></li> <li>In my case, this would be <code>output/vcf_files/kmerIndex.txt</code></li> </ul> <p>Running build-kmer-index creates an index file and a diagnostic file, kmerIndexStatistics.txt,  that is written to the same directory as the index file. Since we have used defaults, a new index file and a statistics file will show in the following <code>output/vcf_files</code> directory of our example:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u2514\u2500\u2500 vcf_files\n\u2502       \u251c\u2500\u2500 kmerIndex.txt *\n\u2502       \u251c\u2500\u2500 kmerIndexStatistics.txt *\n\u2502       \u251c\u2500\u2500 LineA.h.vcf\n\u2502       \u2514\u2500\u2500 LineB.h.vcf\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre>"},{"location":"imputation/#optional-parameters","title":"Optional parameters","text":"<p>In addition to <code>--index-file</code>, this command can take other optional  parameters:</p> Parameter name Description Default value <code>--max-hap-proportion</code> Note: This parameter is currently unused.  Will be removed in a future update. Only k-mers mapping to less than or equal to maxHapProportion of haplotypes in a reference range will be retained. <code>0.75</code> <code>--max-arg-length</code> The maximum argument length for a call to the AGC program. <code>200000</code> <code>--no-diagnostics</code> or <code>-n</code> A flag that eliminates the diagnostic report Disabled (report is written) <code>--max-sample-proportion</code> This parameter will allow for kmers seen below --max-sample-proportion * numSamples to be retained.  Any kmers occurring more than that ratio will be filtered out. <code>0.5</code> <code>--use-big-discard-set</code> A flag to use a different data structure to hold the discard set at the expense of RAM and speed.  If the default option fails due to an error consider enabling this flag. Disabled (default data structure is used) <p>Tip</p> <p>If you get an error caused by a call to AGC being too long, try reducing the <code>--max-arg-length</code> value.</p> <p>The following optional parameters affect how k-mers are pre-filtered  to determine which are used for indexing. They would only need to be  adjusted if the number of k-mers in the index is too low or too high:</p> Parameter name Description Default value <code>--hash-mask</code> In conjunction with <code>--hash-filter</code>, used to mask k-mers for filtering. Default uses only the last k-mer nucleotide. Only change this value unless you know what you are doing. <code>3</code> <code>--hash-filter</code> Only hashes that pass the filter ((hashValue and hashMask) == hashFilter) will be considered. Only change this value unless you know what you are doing. <code>1</code>"},{"location":"imputation/#kmer-index-diagnostics","title":"Kmer index diagnostics","text":"<p>Looking at kmer counts by reference range can help determine if diagnostic kmers provide adequate coverage for mapping  or if there are reference ranges or genomic regions with inadequate coverage. One potential issue is that similar sequence could be assigned to adjacent reference ranges in different assemblies instead of to the same range. This could result in the kmers being discarded because they map to adjacent ranges. The column \"adjacentCount\" lists, for each reference range, the  number of kmers discarded because they were detected in the previous reference range.</p>"},{"location":"imputation/#read-mapping","title":"Read mapping","text":"<p>Now that we have a generated k-mer index file, we can efficiently  align short reads against the PHG using the <code>map-kmers</code> command. To demonstrate this, I will retrieve some example data from our PHGv2 GitHub test directory. The files I will be using from this directory for this walkthrough  will be the following:</p> <ul> <li><code>LineA_LineB_1.fq</code></li> <li><code>LineA_LineB_2.fq</code></li> </ul> <p>These files are simulated paired-end (e.g., <code>_1.fq</code> and <code>_2.fq</code>)  short reads with a length of 150 bps in  FASTQ format. I will place these files under the <code>data</code> directory in sub folder called <code>short_reads</code>:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 short_reads          *\n\u2502   \u2502   \u251c\u2500\u2500 LineA_LineB_1.fq *\n\u2502   \u2502   \u2514\u2500\u2500 LineA_LineB_2.fq * \n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u2514\u2500\u2500 vcf_files\n\u2502       \u251c\u2500\u2500 kmerIndex.txt\n\u2502       \u251c\u2500\u2500 LineA.h.vcf\n\u2502       \u2514\u2500\u2500 LineB.h.vcf\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre> <p>Now that we have both short read data and our k-mer index file, we can pass these to the <code>map-kmers</code> command:</p> <pre><code>./phg map-kmers \\\n    --hvcf-dir output/vcf_files \\\n    --key-file data/key_files/read_mapping_data.txt \\\n    --output-dir output/read_mappings\n</code></pre> <p>This command has the following parameters:</p> <ul> <li><code>--hvcf-dir</code> - the directory containing the hVCF files.</li> <li><code>--key-file</code> - a tab-delimited list of FASTQ files for a collection   of samples.<ul> <li>In the above example, I have made a keyfile and placed it in a subdirectory under the <code>data</code> folder called <code>key_files</code>.</li> <li>My example keyfile would look like the following:   <pre><code>sampleName  filename  filename2\nLineA_B data/short_reads/LineA_LineB_1.fq  data/short_reads/LineA_LineB_2.fq\n</code></pre></li> <li>If you have more than one sample, you would place additional lines at the bottom of the keyfile. For example:   <pre><code>sampleName  filename  filename2\nLineA_B data/short_reads/LineA_LineB_1.fq  data/short_reads/LineA_LineB_2.fq\nCrossC data/short_reads/cross_c_1.fq  data/short_reads/cross_c_2.fq\nCrossD data/short_reads/cross_d_1.fq  data/short_reads/cross_d_2.fq\n</code></pre></li> <li> <p>\u2139\ufe0f Note The keyfile for this parameter needs column names. If you are using single-reads, the column names would be: <code>sampleName</code> and <code>filename</code>. If you have paired-end reads (like  the example above), the column names would be <code>sampleName</code>,  <code>filename</code>, and <code>filename2</code>.</p> </li> <li> <p>\u2139\ufe0f Note File names must be of type \"FASTQ\". In other words, files must  end in the permitted extensions: <code>.fq</code>, <code>.fq.gz</code>, <code>.fastq</code>, and  <code>.fastq.gz</code>. </p> </li> </ul> </li> <li><code>--output-dir</code> - the directory to place the read-mapping files.</li> <li><code>--kmer-index</code> (optional) - the k-mer index file created by the   <code>build-kmer-index</code> command.<ul> <li> <p>\u2139\ufe0f Note   The <code>--kmer-index</code> parameter should only be used if you   specify a given name and path for your k-mer index file   generated in the last step. If you have   chosen the \"default\" option (i.e., leaving this blank), the    <code>map-kmer</code> command will automatically detect the    <code>kmerIndex.txt</code> file found in the directory specified by the    <code>--hvcf-dir</code> parameter.</p> </li> </ul> </li> </ul> <p>Tip</p> <p>If you do not want to create a keyfile and only have one sample, you can replace the <code>--key-file</code> parameter with the <code>--read-files</code> parameter. This parameter will take the paths to the FASTQ reads as a comma separated list.</p> <p>For example, if we were to modify the prior example, the command structure would look like the following:</p> <pre><code>./phg map-kmers \\\n   --hvcf-dir output/vcf_files \\\n   --kmer-index output/kmer_index.txt \\\n   --read-files data/short_reads/LineA_LineB_1.fq,data/short_reads/LineA_LineB_2.fq \\\n   --output-dir output/read_mappings \n</code></pre> <p>Note</p> <p>If no value is passed to the <code>--kmer-index</code> parameter, it will  default to <code>&lt;--hvcf-dir value&gt;/kmerIndex.txt</code>, the default value  used by the <code>build-kmer-index</code> command. If a  non-default value was used for the <code>--index-file</code> parameter in  <code>build-kmer-index</code>, that same value needs to be set here for the  <code>--kmer-index</code> parameter.</p>"},{"location":"imputation/#optional-parameters_1","title":"Optional parameters","text":"<p>The Map Kmers  command can take the following optional parameters:</p> Parameter name Description Default value <code>--threads</code> Number of threads used for mapping <code>5</code> <code>--min-proportion-of-max-count</code> Minimum proportion of the maximum kmer count for a read to be considered a match <code>1.0</code> <code>--min-proportion-same-reference-range</code> Minimum proportion of the read that must align to the same reference range <code>0.9</code> <p>Note</p> <p>We have removed <code>--limit-single-ref-range</code> as it introduces a very high error rate. <code>map-kmers</code> will only use kmers from a single reference range for each read.</p> <p>Now that we have performed k-mer mapping, we will have a new  read-mapping file in our example working directory that will be used for the path finding algorithm in the next step:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 short_reads\n\u2502   \u2502   \u251c\u2500\u2500 LineA_LineB_1.fq\n\u2502   \u2502   \u2514\u2500\u2500 LineA_LineB_2.fq\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u251c\u2500\u2500 vcf_files\n\u2502   \u2502   \u251c\u2500\u2500 LineA.h.vcf\n\u2502   \u2502   \u2514\u2500\u2500 LineB.h.vcf\n\u2502   \u2514\u2500\u2500 read_mappings                     *\n\u2502       \u2514\u2500\u2500 LineA_LineB_1_readMapping.txt *\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre> <p>Note</p> <p>The naming scheme for the read-mapping files follows the first sample in the keyfile or comma-delimited list and: 1. strips the path and FASTQ extension text 2. adds the <code>readMapping.txt</code> suffix to the stripped sample name</p> <p>Additionally, \\(n\\) amount of files will be generated for \\(n\\) amount  of short read samples found in the keyfile or comma-delimited list. For example, if you have 3 samples you are trying to impute, 3 read  mapping files will be generated in the given output. </p>"},{"location":"imputation/#find-paths","title":"Find Paths","text":"<p><code>find-paths</code> is the command used to impute paths through a  <code>HaplotypeGraph</code>  object based on a set of read-mappings. The method uses read-mapping  files generated by the <code>map-kmers</code> command. Running <code>find-paths</code> uses  the Viterbi algorithm  to solve a hidden Markov model (HMM)  to identify the set of haplotypes most likely to have generated the  observed read-mappings. Two path types are implemented: </p> <ul> <li><code>haploid</code> - A haploid path is a single path through the graph. </li> <li><code>diploid</code> - A diploid path is two paths through the graph that    jointly explain the read-mappings.</li> </ul>"},{"location":"imputation/#input-files","title":"Input files","text":"<p><code>find-paths</code> can take either read-mapping files generated by the command  <code>map-kmers</code> or FASTQ files of sequence reads. If FASTQ files are used,  <code>map-kmers</code> is run to generate read-mappings but the intermediate  read-mapping files are not saved. To save a copy of the read-mapping  files, <code>map-kmers</code> and <code>find-paths</code> must be run separately. For  more information, please refer to the prior \"Read Mapping\" section for additional input details.</p> <p>Note</p> <p>If you using read-mapping files, the files must end with the  <code>_readMapping.txt</code> suffix or errors will occur.</p>"},{"location":"imputation/#example-usage","title":"Example usage","text":"<p>Using our prior working example, we can set up the path finding commands in the following code block. For reference, I have made a new subdirectory in the <code>output</code> folder called <code>vcf_files_imputed</code> and will place imputed hVCF files there:</p> <pre><code>./phg find-paths \\\n    --path-keyfile data/key_files/path_finding_data.txt \\\n    --hvcf-dir output/vcf_files \\\n    --reference-genome output/updated_assemblies/Ref.fa \\\n    --path-type haploid \\\n    --output-dir output/vcf_files_imputed\n</code></pre> <p>This command has the following required parameters:</p> <ul> <li><code>--path-keyfile</code> or <code>--read-file</code> but not both: <ul> <li><code>--path-keyfile</code> - name and path of a tab-delimited keyfile that    contains a list of the read-mapping files or read files (FASTQ). <ul> <li>In the working example, I have made a keyfile and placed it in   the <code>key_files</code> folder under the <code>data</code> directory. The keyfile   looks something like this:   <pre><code>sampleName  filename\nLineA_B output/read_mappings/LineA_LineB_1_readMapping.txt\n</code></pre></li> <li>If you wish to skip the prior manual <code>map-kmers</code> step, the data   in the keyfile can also be paths to FASTQ data (see the    \"Read Mapping\" section for further details) </li> <li> <p>\u2139\ufe0f Note   The keyfile must have two columns labeled <code>sampleName</code> and    <code>filename</code> if you are specifying paths to read-mapping files.   If you are using paths to FASTQ data, you may add another   column to your keyfile, labelled <code>filename2</code>, only if your    FASTQ data is paired-end.</p> </li> <li> <p>\u2139\ufe0f Note   The samples in the <code>sampleName</code> must be unique and must match   prior keyfile data from the \"Read Mapping\"    steps.</p> </li> <li> <p>\u2139\ufe0f Note   If an output hVCF for a sample name already exists in the    output directory, the sample will be skipped.</p> </li> </ul> </li> <li><code>--read-file</code> - FASTQ samples or read-mapping files as a    comma-separated list.<ul> <li>If bypassing the manual <code>map-kmers</code> step, paths to FASTQ   files can be used. Either 1 (for single-end) or 2 (for    paired-end) comma-separated files can be input this way.</li> <li>If specifying read-mapping files, comma-separated paths to   files must have the <code>_readMapping.txt</code> suffix to work.</li> </ul> </li> </ul> </li> <li><code>--hvcf-dir</code> - The directory containing the hvcf used to build the    haplotype graph used for imputation.</li> <li><code>--reference-genome</code> -  The name and path to the reference genome    fasta or fastq file.</li> <li><code>--output-dir</code> - The directory where the output hVCFs will be    written. One file will be written for each sample, and the output    file name will be <code>&lt;sample name&gt;.h.vcf</code>.</li> <li><code>--path-type</code>: The type of path to be imputed. The value must be    either <code>haploid</code> or <code>diploid</code></li> </ul>"},{"location":"imputation/#optional-parameters_2","title":"Optional parameters","text":"<p>Additional optional parameters may also be set to optimize your imputation pipeline. Please see the proceeding section  (\"Likely Ancestors\") for more information on how to leverage a set of the following parameters.</p> Parameter name Description Default value <code>--out-parents-dir</code> The directory where the imputed parents (ancestors) will be written for each sample. File names will be _imputed_parents.txt. If no directory name is supplied, the files will not be written. <code>\"\"</code> <code>--prob-correct</code> The probability that a mapped read was mapped correctly. <code>0.99</code> <code>--prob-same-gamete</code> The probability of transitioning to the same gamete (sample) in the next reference range. This should be equal to 1 - (probability of a recombination). Probability of a recombination can be estimated as the total number of expected recombinations in a sample divided by the number of reference ranges <code>0.99</code> <code>--min-gametes</code> The minimum number of gametes with a haplotype in a reference range. Reference ranges with fewer gametes will not be imputed. <code>1</code> <code>--min-reads</code> The minimum number of reads per reference range. Reference ranges with fewer reads will not be imputed. If <code>--min-reads = 0</code>, all reference ranges will be imputed <code>0</code> <code>--inbreeding-coefficient</code> The estimated coefficient of inbreeding for the samples being evaluated. Only used for diploid path type. The value must be between <code>0.0</code> and <code>1.0</code> <code>0.0</code> <code>--max-reads-per-kb</code> ReferenceRanges with more than max-reads-per-kb will not be imputed. <code>1000</code> <code>--use-likely-ancestors</code> The value must be <code>true</code> or <code>false</code>. This indicates whether the most likely ancestors of each sample will be used for path finding. <code>false</code> <code>--max-ancestors</code> If <code>--use-likely-ancestors = true</code>, use at most max-ancestors. <code>Integer.MAX_VALUE</code> <code>--min-coverage</code> If <code>--use-likely-ancestors = true</code>, use the fewest number of ancestors that together have this proportion of mappable reads. The values must be between <code>0.5</code> and <code>1.0</code> <code>1.0</code> <code>--likely-ancestor-file</code> If <code>--use-likely-ancestors = true</code>, a record of the ancestors used for each sample will be written to this file, if a file name is provided. <code>\"\"</code> <code>--threads</code> The number of threads used to find paths. <code>3</code>"},{"location":"imputation/#likely-ancestors","title":"Likely ancestors","text":"<p>The term \"ancestors\" refers to the taxa or samples (i.e., assemblies) used to construct the haplotype graph used for imputation. The term  \"ancestors\" is used because the model considers the haplotypes in the  graph to represent the potential ancestral haplotypes from which the  samples to be imputed were derived.</p> <p>In some scenarios, limiting the number of ancestors used to impute a sample will be useful. Some examples include:</p> <ul> <li>if some samples were derived from an F1 cross and the individuals    used to build the graph include those parents, using only those    parents for imputation can improve accuracy.</li> <li>Also, because imputing diploid   paths is more computationally intensive than haploid paths, limiting   the number of ancestors used per sample may be necessary to control   the amount of time required to impute each sample.</li> </ul> <p>To restrict the number of ancestors used, set the <code>--use-likely-ancestors</code> parameter to <code>true</code>, and provide a value for either <code>--max-ancestors</code> or <code>--min-coverage</code>. For the case where the samples have been derived from a limited number of ancestors setting  <code>--min-coverage</code> to <code>0.95</code> or <code>--max-ancestors</code> to the expected  number of ancestors is a useful strategy. In either case, providing a  name for the output file saves a record of the ancestors used for  each sample and should be checked to make sure the samples behaved as  expected.</p> <p>When using this method, ancestors are chosen by first counting the number of reads for a sample that map to each ancestor in the haplotype graph. The ancestor with the most mapped reads is chosen. Next the reads not mapping to first chosen ancestor are used to count reads mapping to the remaining ancestors and the ancestor with the most reads is selected. This is repeated until either the proportion of reads mapping to the selected ancestors &gt;= <code>--min-coverage</code> or the number ancestors selected equals <code>--max-ancestors</code>. If a name is  provided for the <code>--likely-ancestor-file</code>, the chosen ancestors are  written to that file in the order they were chosen along with the  cumulative coverage, which is the proportion of reads mapping to each  ancestor and the previously chosen ones.</p>"},{"location":"imputation/#general-output-and-next-steps","title":"General output and next steps","text":"<p>Now that we have newly imputed hVCF data, our example working directory looks like the following:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 short_reads\n\u2502   \u2502   \u251c\u2500\u2500 LineA_LineB_1.fq\n\u2502   \u2502   \u2514\u2500\u2500 LineA_LineB_2.fq\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u251c\u2500\u2500 vcf_files\n\u2502   \u2502   \u251c\u2500\u2500 LineA.h.vcf\n\u2502   \u2502   \u2514\u2500\u2500 LineB.h.vcf\n\u2502   \u2502   vcf_files_imputed *\n\u2502   \u2502   \u2514\u2500\u2500 LineA_B.h.vcf * \n\u2502   \u2514\u2500\u2500 read_mappings\n\u2502       \u2514\u2500\u2500 LineA_LineB_1_readMapping.txt\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre> <p>Similar to the \"Build and Load\" steps, we can now load this data into our TileDB instance using the <code>load-vcf</code> command:</p> <pre><code>./phg load-vcf \\\n    --vcf-dir output/vcf_files_imputed \\\n    --db-path vcf_dbs \\\n    --threads 10\n</code></pre> <p>This command takes three parameters:</p> <ul> <li><code>--vcf-dir</code> - Directory containing hVCF data. Since I have imputed    data in a specific subdirectory, I will use the path   <code>output/vcf_files_imputed</code></li> <li><code>--db-path</code> - Path to the directory containing the TileDB instances.</li> <li><code>--threads</code> - Number of threads for use by the TileDB loading   procedure.</li> </ul>"},{"location":"imputation/#create-gvcf-files-optional","title":"Create g.vcf files (OPTIONAL)","text":"<p>Our imputed hVCF files provide data on a haplotype level. If desired we can take  the hVCF files and create gVCF files. This provides SNP level data and is done using  the <code>hvcf2gvcf</code> command:</p> <pre><code>./phg hvcf2gvcf \\\n    --hvcf-dir output/vcf_files_imputed \\\n    --db-path vcf_dbs \\\n    --reference-file output/updated_assemblies/Ref.fa \\\n    --output-dir output/gvcf_files\n</code></pre> <p>This command takes 4 parameters:</p> <ul> <li><code>--hvcf-dir</code> - Directory containing hVCF data. Since I have imputed    data in a specific subdirectory, I will use the path   <code>output/vcf_files_imputed</code></li> <li><code>--db-path</code> - Path to the directory containing the TileDB instances.</li> <li><code>--reference-file</code> - The reference genome fasta file.</li> <li><code>--output-dir</code> - The directory to place the gVCF files.</li> </ul>"},{"location":"imputation_ml/","title":"Imputation using Machine Learning","text":"<p>Disclaimer</p> <p>This Imputation using Machine Learning (ML) section is a work in progress.  It is not yet complete and may contain inaccuracies or incomplete  information. </p> <p>In this document, we will discuss the steps needed to build PS4G files  using the full assemblies and then to perform Machine Learning Based  imputation using the PHG:</p> <ol> <li>Run ropebwt3 indexing by full length chromosomes</li> <li>Build Splines for the assemblies</li> <li>Align Reads against the Index</li> <li>Create a PS4G file from ropebwt3 BED data</li> <li>Run ML Model - Future Work</li> </ol> <p>Note</p> <p>The steps detailed in this document build on the materials from the \"Building and Loading\" documentation. Please review this if you have not worked with the PHG before!</p>"},{"location":"imputation_ml/#quick-start","title":"Quick start","text":"<ul> <li> <p>Run ropebwt3 indexing by full length chromosomes:   <pre><code>phg rope-bwt-chr-index \\\n    --keyfile /my/keyfile.txt \\\n    --output-dir /my/phgIndex/ \\\n    --index-file-prefix phg_index \\\n    --threads 20\n</code></pre></p> </li> <li> <p>Build Splines for the assemblies:   <pre><code>phg build-spline-knots \\\n    --vcf-dir /my/vcfs \\\n    --vcf-type gvcf \\\n    --output-dir /my/spline_output_dir \\\n    --min-indel-length 10 \\\n    --num-bps-per-knot 50000 \\\n    --contig-list chr1,chr2,chr3\n</code></pre></p> </li> </ul> <p>!!! note \"Getting gVCF files without reference ranges\"       When not using reference ranges (skipping <code>phg create-ranges</code>), you cannot       run the standard <code>phg create-ref-vcf</code> and <code>phg create-maf-vcf</code> commands.       Instead, you can convert MAF alignment files directly to gVCF format using       the <code>maf-to-gvcf-converter</code> command from biokotlin-tools.</p> <ul> <li> <p>Find maximal exact matches between ropebwt3 index and reads:   <pre><code>phg align-reads \\\n    --index ropebwt.fmd \\\n    --query query.fastq \\\n    --min-smem-len 31 \\\n    --threads 4 \\\n    --output example.bed\n</code></pre></p> </li> <li> <p>Create a PS4G file from ropebwt3 BED data:   <pre><code>phg convert-bed-to-ps4g \\\n    --db-path /my/db/uri \\\n    --data-set hvcf \\\n    --sample-names /my/sample_names.txt \\\n    --output-dir /my/ps4g_output_dir\n</code></pre></p> </li> </ul>"},{"location":"imputation_ml/#detailed-walkthrough","title":"Detailed walkthrough","text":""},{"location":"imputation_ml/#run-ropebwt3-indexing-by-full-length-chromosomes","title":"Run ropebwt3 indexing by full length chromosomes","text":"<p>Creates a ropeBWT3 index for a set of assemblies using the full length indexing method. Each FASTA is taken one at a time and is processed and indexed into the ropeBWT3 index. Once the initial index is finished, the <code>.fmr</code> index is converted to <code>.fmd</code> and the suffix array is built.</p> <p>Command - <code>rope-bwt-chr-index</code></p> <p>Example</p> <pre><code>phg rope-bwt-chr-index \\\n    --keyfile keyfile.txt \\\n    --output-dir /path/to/output/bed/files/ \\\n    --index-file-prefix phgIndex \\\n    --threads 20 \n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--keyfile</code> Tab-delimited file containing 2 columns: <code>Fasta</code> and <code>SampleName</code>. <code>Fasta</code> is the full path to the FASTA file, and <code>SampleName</code> is the name for assembly. None <code>--output-dir</code> Output directory where the index files will be written. None <code>--index-file-prefix</code> Prefix for the ropebwt3 index file. This prefix will be added to the output directory and used for generated index files. None <code>--threads</code> Number of threads to use for index creation. <code>3</code> <code>--delete-fmr-index</code> Delete the <code>.fmr</code> index file after converting to <code>.fmd</code>. <code>true</code> <code>--conda-env-prefix</code> Prefix for the Conda environment to use. If provided, this should be the full path to the Conda environment. <code>\"\"</code> <p>Note</p> <ul> <li><code>--keyfile</code> is a tab-delimited file with 2 columns with names <code>Fasta</code> and <code>SampleName</code>. <code>Fasta</code> needs the full path for each assembly FASTA file and <code>SampleName</code> needs to be the name you want included in the contig name. The first step of the indexer is to open up each FASTA file and rename the contigs to include the provided sample name separated by an '_' (e.g., <code>lineA_chr1</code>).</li> <li><code>--index-file-prefix</code> is the prefix for all the output index files. This tool will make a number of files (some temporary) while it is running each with this prefix. There should not be an extension here as this will be added as need be.</li> </ul> <p>Note</p> <p>Be sure to include your reference fasta file in the <code>--keyfile</code> as well.</p> <p>Note</p> <p>The SampleName should not have any underscores in it. We rename the  contigs in the assembly fasta file temporarily so that RopeBWT3 can  differentiate between contigs of the same name but coming from  different assemblies (e.g., <code>chr1</code>, <code>chr2</code>, ... etc.).</p> <p></p>"},{"location":"imputation_ml/#build-spline-knots","title":"Build spline knots","text":"<p>Build spline knot points from gVCF or hVCF data</p> <p>Command - <code>build-spline-knots</code></p> <p>Example</p> <pre><code>build-spline-knots \\\n  --vcf-dir /data/hvcf_files \\\n  --output-dir /results/spline_knots \\\n  --vcf-type hvcf \\\n  --num-bps-per-knot 75000 \\\n  --random-seed 42\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--vcf-dir</code> Directory containing the hVCF or gVCF files. <code>\"\"</code> <code>--vcf-type</code> Type of VCFs to build the splines from. Accepts <code>\"hvcf\"</code> or <code>\"gvcf\"</code>. <code>\"hvcf\"</code> <code>--output-dir</code> Output directory to write the spline knots to. <code>\"\"</code> <code>--min-indel-length</code> Minimum length of an indel to break up the running block for spline creation of gVCFs. Ignored if <code>--vcf-type</code> is <code>hvcf</code>. <code>10</code> <code>--num-bps-per-knot</code> Maximum number of base pairs per knot for each contig\u2019s spline. The actual number may be lower if a contig has fewer bases. <code>50000</code> <code>--contig-list</code> Comma-separated list of chromosomes to include in spline generation. If not provided, all chromosomes will be included. <code>\"\"</code> <code>--random-seed</code> Random seed used for downsampling the number of points per chromosome. Ensures reproducibility. <code>12345</code> <p></p>"},{"location":"imputation_ml/#align-reads-to-ropebwt3-index","title":"Align reads to ropebwt3 index","text":"<p>This command serves as a high-level wrapper around the <code>ropebwt3 mem</code>  algorithm, providing a streamlined interface for aligning short  sequencing reads to a pre-built FM-index reference.</p> <p>This command identifies Super-Maximal Exact Matches (SMEMs) between  query reads and the indexed reference, efficiently mapping reads to genomic  coordinates. Results are exported in BED format for easy visualization  and downstream PS4G construction (see next section).</p> <p>Command - <code>align-reads</code></p> <p>Example</p> <pre><code>phg align-reads \\\n    --index ropebwt.fmd \\\n    --query query.fastq \\\n    --min-smem-len 31 \\\n    --threads 4 \\\n    --output example.bed\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--index</code> Path to the ropebwt3 FM-index (<code>.fmd</code>) file used as the reference for read alignment. <code>\"\"</code> <code>--query</code> Input FASTQ file containing reads to align. Supports both uncompressed and <code>.gz</code>-compressed files. <code>\"\"</code> <code>--min-smem-len</code> Minimum SMEM (Super-Maximal Exact Match) length used as a seed for alignment. Larger values produce fewer but more specific matches. <code>31</code> <code>--threads</code> Number of threads to use for parallel processing during alignment. Improves performance on multicore systems. <code>1</code> <code>--output</code> Output file in BED format containing aligned read intervals, mapping quality, and strand information. <code>\"\"</code> <p></p>"},{"location":"imputation_ml/#create-a-ps4g-file-from-ropebwt3-bed-data","title":"Create a PS4G file from ropebwt3 BED data","text":"<p>Convert a ropebwt3 BED file into a PS4G (positional support for gamete) file.</p> <p>Note</p> <p>This command will only work with ropebwt3 files where the reads are aligned to the whole assembly chromosome using the <code>mem</code> command. MEMs (Maximal Exact Matches) are used to determine what the optimal mappping is. One downside to this approach is that if a SNP is in the middle of the read, the mappings will be ignored. We may integrate running this in conjunction with ropebwt3's Burrows-Wheeler Aligner's Smith-Waterman Alignment (BWA-SW) approach (i.e., the <code>sw</code> command) in a future update.</p> <p>Command - <code>convert-ropebwt2-ps4g-file</code></p> <p>Example</p> <pre><code>phg convert-ropebwt2-ps4g-file \\\n    --ropebwt-bed /data/alignments/sample.mem.bed \\\n    --output-dir /results/ps4g \\\n    --spline-knot-dir /refs/spline_knots \\\n    --min-mem-length 148 \\\n    --max-num-hits 50 \\\n    --sort-positions\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--ropebwt-bed</code> Path to the RopeBWT3 BED file (MEM hits per read) to convert into PS4G. <code>\"\"</code> <code>--output-dir</code> Output directory where the generated PS4G file will be written. <code>\"\"</code> <code>--spline-knot-dir</code> Directory containing Spline Knot lookup files (per contig / sample) used to transform MEM positions into consensus PS4G positions. <code>\"\"</code> <code>--min-mem-length</code> Minimum length of a match (MEM) to be considered for consensus. Shorter MEMs are ignored. <code>148</code> <code>--max-num-hits</code> Maximum total number of hits allowed (sum across best MEMs). If a read hits more haplotypes than this, it is ignored. <code>50</code> <code>--sort-positions</code> Sort positions in the resulting PS4G file. Use <code>--no-sort-positions</code> to disable. <code>true</code> <p>Note</p> <p>ropebwt3 can hit more than the value provided in the <code>--max-num-hits</code> parameter but any alignment hitting more haplotypes than this will be ignored.</p>"},{"location":"imputation_ropebwt/","title":"Imputation using RopeBWT3","text":"<p>Note</p> <p>This is the recommended imputation method. Please  let us know if  you have any questions or issues.</p> <p>Diploid hVCF to gVCF disclaimer</p> <p>There is a known bug in the <code>hvcf2gvcf</code> command that affects diploid hVCF files. This affects all PHGv2 versions after 2.3.7.144. The output gVCF will only contain variants from the first haplotype - variants from the second haplotype are omitted entirely. Haploid hVCF files are not affected. We are actively working on a fix and will update the documentation when it is resolved. If you have questions or need assistance, please [comment on the announcement discussion] (https://github.com/maize-genetics/phg_v2/discussions/347)</p> <p>In this document, we will discuss the steps needed to perform imputation using the ropebwt3 tool within the PHG. The steps:</p> <ol> <li>Export hVCF data</li> <li>Index the pangenome from exported hVCF data</li> <li>Map short reads</li> <li>Find paths</li> </ol> <p>Note</p> <p>The steps detailed in this document build on the materials from the \"Building and Loading\" documentation. Please review this if you have not worked with the PHG before!</p>"},{"location":"imputation_ropebwt/#quick-start","title":"Quick start","text":"<ul> <li> <p>Get list of samples to export:   <pre><code>phg list-samples \\\n    --db-path /my/db/uri \\\n    --data-set hvcf \\\n    --output-file /my/sample_names.txt\n</code></pre></p> </li> <li> <p>Export hVCF data:   <pre><code>phg export-vcf \\\n    --db-path /my/db/uri \\\n    --dataset-type hvcf \\\n    --sample-file /my/sample_names.txt \\\n    --output-dir /my/hvcf/dir\n</code></pre></p> </li> <li> <p>Index pangenome:   <pre><code>phg rope-bwt-index \\\n    --db-path /my/db/uri \\\n    --hvcf-dir /my/hvcf/dir \\\n    --output-dir /my/index/dir \\\n    --index-file-prefix myindex\n</code></pre></p> </li> <li> <p>OPTIONAL: Align reads to index for BED generation:   <pre><code>phg align-reads \\\n    --index /my/index/dir/myindex.fmd \\\n    --query /my/reads.fq \\\n    --min-smem-len 31 \\\n    --threads 4 \\\n    --output /my/mapping/dir/output.bed\n</code></pre></p> </li> <li> <p>Map short reads   <pre><code>phg map-reads \\\n    --hvcf-dir /my/hvcf/dir \\\n    --index /my/index/dir/myindex.fmd \\\n    --key-file /my/path/keyfile \\\n    --output-dir /my/mapping/dir\n</code></pre></p> </li> <li> <p>Find paths   <pre><code>phg find-paths \\\n    --path-keyfile /my/path/keyfile \\\n    --hvcf-dir /my/hvcf/dir \\\n    --reference-genome /my/ref/genome \\\n    --path-type haploid \\\n    --output-dir /my/imputed/hvcfs\n</code></pre></p> </li> <li> <p>OPTIONAL: Get SNPs from imputed hVCFs   <pre><code>phg hvcf2gvcf \\\n    --hvcf-dir /my/imputed/hvcfs \\\n    --db-path /my/db/uri \\\n    --reference-file /my/ref/genome \\\n    --output-dir /my/gvcf/dir\n</code></pre></p> </li> </ul>"},{"location":"imputation_ropebwt/#detailed-walkthrough","title":"Detailed walkthrough","text":""},{"location":"imputation_ropebwt/#hvcf-export","title":"hVCF export","text":"<p>Note</p> <p>This step is currently needed, but will be removed in the future as we can do direct connections to the hVCF TileDB instance.</p> <p>Where we last left off in the \"Build and Load\" steps, we had an example directory that looks similar to the following:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u2514\u2500\u2500 vcf_files/\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre> <p>Note</p> <p>The following steps in the rest of this document will assume we are in the top level of our <code>phg_v2_example</code> working directory.</p> <p>For this example imputation pipeline, we will be using haplotypes generated from two samples in our database:</p> <ul> <li><code>LineA</code></li> <li><code>LineB</code></li> </ul> <p>If you do not have hVCF files for samples you wish to impute against already generated, we will first need to export this haplotype data in the form of hVCF data from the TileDB instance. This is done using the <code>export-vcf</code> command:</p> <p><pre><code>./phg export-vcf \\\n    --db-path vcf_dbs \\\n    --dataset-type hvcf \\\n    --sample-file /my/sample_names.txt \\\n    --output-dir output/vcf_files\n</code></pre> This command takes in 4 parameters:</p> <ul> <li><code>--db-path</code> - path to directory storing the TileDB instances. The   AGC compressed genomes will be placed here on completion.</li> <li><code>--dataset-type</code> - the type of dataset to export. In this case, we   are exporting the hVCF data.</li> <li><code>--sample-file</code> - text file with list of sample names to export,    one per line.</li> <li><code>--output-dir</code> - the directory to place the exported hVCF files.</li> </ul> <p>In our above example, we use the <code>--sample-file</code> parameter, but if your sample list is small, you can use the <code>--sample-names</code> parameter. An example of a sample file is as follows:</p> <pre><code>$ cat sample_names.txt\n\nLineA\nLineB\n</code></pre> <p>Example command using --sample-names parameter:</p> <pre><code>./phg export-vcf \\\n    --db-path vcf_dbs \\\n    --dataset-type hvcf \\\n    --sample-names LineA,LineB \\\n    --output-dir output/vcf_files\n</code></pre> <p>After the export command, our directory structure will have new files added to the <code>vcf_files</code> directory:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u2514\u2500\u2500 vcf_files\n\u2502       \u251c\u2500\u2500 LineA.h.vcf *\n\u2502       \u2514\u2500\u2500 LineB.h.vcf *\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre>"},{"location":"imputation_ropebwt/#pangenome-indexing","title":"Pangenome indexing","text":"<p>Upcoming v2.5 Release Notice</p> <p>We will soon update how indexing is performed for the imputation  pipeline by replacing the <code>rope-bwt-index</code> command with  <code>rope-bwt-chr-index</code>. If you wish to try it out now, the indexer  is currently usable as a standalone command. More information  about how to use <code>rope-bwt-chr-index</code> can be found here.</p> <p>In order to run the later ropebwt3 read mapping steps, we will first need to build a ropebwt3 index.  This is performed using the <code>rope-bwt-index</code> command:</p> <pre><code>phg rope-bwt-index \\\n    --db-path vcf_dbs \\\n    --hvcf-dir output/vcf_files \\\n    --output-dir output/index_files \\\n    --index-file-prefix myindex\n</code></pre> <p>This command has 4 required parameters:</p> <ul> <li><code>--db-path</code> - path to directory storing the TileDB instances and   <code>assemblies.agc</code> file made in the   \"Compress FASTA files\"   section of the \"Build and Load\" documentation.</li> <li><code>--hvcf-dir</code> - the directory containing the hVCF files. This is the   output directory from the <code>export-vcf</code> command.  Right now this is   required, but will be optional in the future.</li> <li><code>--output-dir</code> - the directory to place the index files.</li> <li><code>--index-file-prefix</code>  - the prefix for the index file   name. A number of files will be created all of which start with    <code>--index-file-prefix</code>.</li> </ul> <p>Running rope-bwt-index creates an index file with the extension <code>.fmd</code>  and some additional files for a sequence suffix array(<code>.ssa</code>) and a  contig length file (<code>.len.gz</code>). Both of these are needed to map reads  to this index. Since we have used defaults, the index files will show  up in the following <code>output/index_files</code> directory of our example:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u2514\u2500\u2500 vcf_files\n\u2502   \u2502   \u251c\u2500\u2500 LineA.h.vcf\n\u2502   \u2502   \u2514\u2500\u2500 LineB.h.vcf\n\u2502   \u2514\u2500\u2500 index_files\n\u2502       \u251c\u2500\u2500 myindex.fmd *\n\u2502       \u251c\u2500\u2500 myindex.fmd.len.gz *\n\u2502       \u2514\u2500\u2500 myindex.fmd.ssa *\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre>"},{"location":"imputation_ropebwt/#optional-parameters","title":"Optional parameters","text":"<p>In addition to <code>--index-file</code>, this command can take other optional parameters:</p> Parameter name Description Default value <code>--threads</code> Number of threads to use during indexing. <code>3</code> <code>--delete-fmr-index</code> RopeBWT3 will originally output a .fmr index file.  This is converted to fmd for efficiency.  If this flag is set to false it will keep that original file. <code>true</code> <code>--no-diagnostics</code> or <code>-n</code> A flag that eliminates the diagnostic report Disabled (report is written)"},{"location":"imputation_ropebwt/#align-reads-to-ropebwt3-index-optional","title":"Align reads to ropebwt3 index (optional)","text":"<p>This command serves as a high-level wrapper around the <code>ropebwt3 mem</code>  algorithm, providing a streamlined interface for aligning short  sequencing reads to a pre-built FM-index reference.</p> <p>This command identifies Super-Maximal Exact Matches (SMEMs) between  query reads and the indexed reference, efficiently mapping reads to genomic  coordinates. Results are exported in BED format for easy visualization  and downstream analysis.</p> <p>Tip</p> <p>This step is optional if you plan to use the <code>map-reads</code> command directly, which performs alignment internally. Use <code>align-reads</code> when you want the intermediate BED file for inspection, debugging, or use with <code>map-reads-from-bed</code>.</p> <p>Command - <code>align-reads</code></p> <p>Example</p> <pre><code>phg align-reads \\\n    --index output/index_files/myindex.fmd \\\n    --query data/short_reads/LineA_LineB_1.fq \\\n    --min-smem-len 31 \\\n    --threads 4 \\\n    --output output/read_mappings/LineA_LineB_1.bed\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--index</code> Path to the ropebwt3 FM-index (<code>.fmd</code>) file used as the reference for read alignment. <code>\"\"</code> <code>--query</code> Input FASTQ file containing reads to align. Supports both uncompressed and <code>.gz</code>-compressed files. <code>\"\"</code> <code>--min-smem-len</code> Minimum SMEM (Super-Maximal Exact Match) length used as a seed for alignment. Larger values produce fewer but more specific matches. <code>31</code> <code>--threads</code> Number of threads to use for parallel processing during alignment. Improves performance on multicore systems. <code>1</code> <code>--output</code> Output file in BED format containing aligned read intervals, mapping quality, and strand information. <code>\"\"</code> <p></p>"},{"location":"imputation_ropebwt/#read-mapping","title":"Read mapping","text":"<p>Now that we have a generated ropeBWT3 index file, we can efficiently align short reads against the PHG using the <code>map-reads</code> command. To demonstrate this, I will retrieve some example data from our PHGv2 GitHub test directory. The files I will be using from this directory for this walkthrough will be the following:</p> <ul> <li><code>LineA_LineB_1.fq</code></li> <li><code>LineA_LineB_2.fq</code></li> </ul> <p>These files are simulated paired-end (e.g., <code>_1.fq</code> and <code>_2.fq</code>) short reads with a length of 150 bps in FASTQ format. I will place these files under the <code>data</code> directory in sub folder called <code>short_reads</code>:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 short_reads          *\n\u2502   \u2502   \u251c\u2500\u2500 LineA_LineB_1.fq *\n\u2502   \u2502   \u2514\u2500\u2500 LineA_LineB_2.fq * \n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u2514\u2500\u2500 vcf_files\n\u2502   \u2502   \u251c\u2500\u2500 LineA.h.vcf\n\u2502   \u2502   \u2514\u2500\u2500 LineB.h.vcf\n\u2502   \u2514\u2500\u2500 index_files\n\u2502       \u251c\u2500\u2500 myindex.fmd\n\u2502       \u251c\u2500\u2500 myindex.fmd.len.gz\n\u2502       \u2514\u2500\u2500 myindex.fmd.ssa\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre> <p>Now that we have both short read data and our k-mer index file, we can pass these to the <code>map-reads</code> command:</p> <pre><code>./phg map-reads \\\n    --hvcf-dir output/vcf_files \\\n    --index output/index_files/myindex.fmd \\\n    --key-file data/key_files/read_mapping_data.txt \\\n    --output-dir output/read_mappings\n</code></pre> <p>This command has the following parameters:</p> <ul> <li><code>--hvcf-dir</code> - the directory containing the hVCF files.</li> <li><code>--key-file</code> - a tab-delimited list of FASTQ files for a collection   of samples.<ul> <li>In the above example, I have made a keyfile and placed it in a   subdirectory under the <code>data</code> folder called <code>key_files</code>.</li> <li>My example keyfile would look like the following:   <pre><code>sampleName  filename  filename2\nLineA_B data/short_reads/LineA_LineB_1.fq  data/short_reads/LineA_LineB_2.fq\n</code></pre></li> <li>If you have more than one sample, you would place additional   lines at the bottom of the keyfile. For example:   <pre><code>sampleName  filename  filename2\nLineA_B data/short_reads/LineA_LineB_1.fq  data/short_reads/LineA_LineB_2.fq\nCrossC data/short_reads/cross_c_1.fq  data/short_reads/cross_c_2.fq\nCrossD data/short_reads/cross_d_1.fq  data/short_reads/cross_d_2.fq\n</code></pre></li> <li> <p>\u2139\ufe0f Note   The keyfile for this parameter needs column names. If you   are using single-reads, the column names would be:   <code>sampleName</code> and <code>filename</code>. If you have paired-end reads (like   the example above), the column names would be <code>sampleName</code>,   <code>filename</code>, and <code>filename2</code>.</p> </li> <li> <p>\u2139\ufe0f Note   File names must be of type \"FASTQ\". In other words, files must   end in the permitted extensions: <code>.fq</code>, <code>.fq.gz</code>, <code>.fastq</code>, and   <code>.fastq.gz</code>.</p> </li> </ul> </li> <li><code>--output-dir</code> - the directory to place the read-mapping files.</li> <li><code>--index</code> - the ropeBWT3 index file created by the   <code>ropebwt-index</code> command.</li> </ul> <p>Tip</p> <p>If you do not want to create a keyfile and only have one sample, you can replace the <code>--key-file</code> parameter with the <code>--read-files</code> parameter. This parameter will take the paths to the FASTQ reads as a comma separated list.</p> <pre><code>For example, if we were to modify the prior example, the command\nstructure would look like the following:\n\n```shell\n./phg map-reads \\\n   --hvcf-dir output/vcf_files \\\n   --index output/index_files/myindex.fmd \\\n   --read-files data/short_reads/LineA_LineB_1.fq,data/short_reads/LineA_LineB_2.fq \\\n   --output-dir output/read_mappings \n```\n</code></pre>"},{"location":"imputation_ropebwt/#optional-parameters_1","title":"Optional parameters","text":"<p>The Map Reads  command can take the following optional parameters:</p> Parameter name Description Default value <code>--threads</code> Number of threads used for mapping <code>5</code> <code>--min-mem-length</code> Minimum length of the Maximal Exact Match(MEM) to retain.  Any alignments below this value are ignored <code>148</code> <code>--max-num-hits</code> Maximum number of alignments to consider for a given read.  If there are more than <code>--max-num-hits</code> alignments, the read will be ignored. <code>50</code> <p>Now that we have performed read mapping, we will have a new read-mapping file in our example working directory that will be used for the path finding algorithm in the next step:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 short_reads\n\u2502   \u2502   \u251c\u2500\u2500 LineA_LineB_1.fq\n\u2502   \u2502   \u2514\u2500\u2500 LineA_LineB_2.fq\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u251c\u2500\u2500 vcf_files\n\u2502   \u2502   \u251c\u2500\u2500 LineA.h.vcf\n\u2502   \u2502   \u2514\u2500\u2500 LineB.h.vcf\n\u2502   \u2514\u2500\u2500 read_mappings                     *\n\u2502       \u2514\u2500\u2500 LineA_LineB_1_readMapping.txt *\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre> <p>Note</p> <p>The naming scheme for the read-mapping files follows the first sample in the keyfile or comma-delimited list and: 1. strips the path and FASTQ extension text 2. adds the <code>readMapping.txt</code> suffix to the stripped sample name</p> <p>Additionally, \\(n\\) amount of files will be generated for \\(n\\) amount  of short read samples found in the keyfile or comma-delimited list. For example, if you have 3 samples you are trying to impute, 3 read  mapping files will be generated in the given output. </p>"},{"location":"imputation_ropebwt/#find-paths","title":"Find Paths","text":"<p><code>find-paths</code> is the command used to impute paths through a <code>HaplotypeGraph</code> object based on a set of read-mappings. The method uses read-mapping files generated by the <code>map-reads</code> command. Running <code>find-paths</code> uses the Viterbi algorithm to solve a hidden Markov model (HMM) to identify the set of haplotypes most likely to have generated the observed read-mappings. Two path types are implemented:</p> <ul> <li><code>haploid</code> - A haploid path is a single path through the graph.</li> <li><code>diploid</code> - A diploid path is two paths through the graph that   jointly explain the read-mappings.</li> </ul> <p>Note</p> <p><code>find-paths</code> can run the old kmer read mapping pipeline. Eventually this will be transitioned over to using the ropebwt3 index  and alignment process.  For now run the index and mapping step manually.</p>"},{"location":"imputation_ropebwt/#input-files","title":"Input files","text":"<p><code>find-paths</code> can take either read-mapping files generated by the command <code>map-kmers</code> or FASTQ files of sequence reads. If FASTQ files are used, <code>map-kmers</code> is run to generate read-mappings but the intermediate read-mapping files are not saved. To save a copy of the read-mapping files, <code>map-kmers</code> and <code>find-paths</code> must be run separately. For more information, please refer to the prior \"Read Mapping\" section for additional input details.</p> <p>Note</p> <p>If you using read-mapping files, the files must end with the <code>_readMapping.txt</code> suffix or errors will occur.</p>"},{"location":"imputation_ropebwt/#example-usage","title":"Example usage","text":"<p>Using our prior working example, we can set up the path finding commands in the following code block. For reference, I have made a new subdirectory in the <code>output</code> folder called <code>vcf_files_imputed</code> and will place imputed hVCF files there:</p> <pre><code>./phg find-paths \\\n    --path-keyfile data/key_files/path_finding_data.txt \\\n    --hvcf-dir output/vcf_files \\\n    --reference-genome output/updated_assemblies/Ref.fa \\\n    --path-type haploid \\\n    --output-dir output/vcf_files_imputed\n</code></pre> <p>This command has the following required parameters:</p> <ul> <li><code>--path-keyfile</code> or <code>--read-file</code> but not both:<ul> <li><code>--path-keyfile</code> - name and path of a tab-delimited keyfile that   contains a list of the read-mapping files or read files (FASTQ).<ul> <li>In the working example, I have made a keyfile and placed it in   the <code>key_files</code> folder under the <code>data</code> directory. The keyfile   looks something like this:   <pre><code>sampleName  filename\nLineA_B output/read_mappings/LineA_LineB_1_readMapping.txt\n</code></pre></li> <li>If you wish to skip the prior manual <code>map-reads</code> step, the data   in the keyfile can also be paths to FASTQ data (see the   \"Read Mapping\" section for further details)</li> <li> <p>\u2139\ufe0f Note   The keyfile must have two columns labeled <code>sampleName</code> and   <code>filename</code> if you are specifying paths to read-mapping files.   If you are using paths to FASTQ data, you may add another   column to your keyfile, labelled <code>filename2</code>, only if your   FASTQ data is paired-end.</p> </li> <li> <p>\u2139\ufe0f Note   The samples in the <code>sampleName</code> must be unique and must match   prior keyfile data from the \"Read Mapping\"   steps.</p> </li> <li> <p>\u2139\ufe0f Note   If an output hVCF for a sample name already exists in the   output directory, the sample will be skipped.</p> </li> </ul> </li> <li><code>--read-file</code> - FASTQ samples or read-mapping files as a   comma-separated list.<ul> <li>If bypassing the manual <code>map-reads</code> step, paths to FASTQ   files can be used. Either 1 (for single-end) or 2 (for   paired-end) comma-separated files can be input this way.</li> <li>If specifying read-mapping files, comma-separated paths to   files must have the <code>_readMapping.txt</code> suffix to work.</li> </ul> </li> </ul> </li> <li><code>--hvcf-dir</code> - The directory containing the hvcf used to build the   haplotype graph used for imputation.</li> <li><code>--reference-genome</code> -  The name and path to the reference genome   fasta or fastq file.</li> <li><code>--output-dir</code> - The directory where the output hVCFs will be   written. One file will be written for each sample, and the output   file name will be <code>&lt;sample name&gt;.h.vcf</code>.</li> <li><code>--path-type</code>: The type of path to be imputed. The value must be   either <code>haploid</code> or <code>diploid</code></li> </ul>"},{"location":"imputation_ropebwt/#optional-parameters_2","title":"Optional parameters","text":"<p>Additional optional parameters may also be set to optimize your imputation pipeline. Please see the proceeding section (\"Likely Ancestors\") for more information on how to leverage a set of the following parameters.</p> Parameter name Description Default value <code>--out-parents-dir</code> The directory where the imputed parents (ancestors) will be written for each sample. File names will be _imputed_parents.txt. If no directory name is supplied, the files will not be written. <code>\"\"</code> <code>--prob-correct</code> The probability that a mapped read was mapped correctly. <code>0.99</code> <code>--prob-same-gamete</code> The probability of transitioning to the same gamete (sample) in the next reference range. This should be equal to 1 - (probability of a recombination). Probability of a recombination can be estimated as the total number of expected recombinations in a sample divided by the number of reference ranges <code>0.99</code> <code>--min-gametes</code> The minimum number of gametes with a haplotype in a reference range. Reference ranges with fewer gametes will not be imputed. <code>1</code> <code>--min-reads</code> The minimum number of reads per reference range. Reference ranges with fewer reads will not be imputed. If <code>--min-reads = 0</code>, all reference ranges will be imputed <code>0</code> <code>--inbreeding-coefficient</code> The estimated coefficient of inbreeding for the samples being evaluated. Only used for diploid path type. The value must be between <code>0.0</code> and <code>1.0</code> <code>0.0</code> <code>--max-reads-per-kb</code> ReferenceRanges with more than max-reads-per-kb will not be imputed. <code>1000</code> <code>--use-likely-ancestors</code> The value must be <code>true</code> or <code>false</code>. This indicates whether the most likely ancestors of each sample will be used for path finding. <code>false</code> <code>--max-ancestors</code> If <code>--use-likely-ancestors = true</code>, use at most max-ancestors. <code>Integer.MAX_VALUE</code> <code>--min-coverage</code> If <code>--use-likely-ancestors = true</code>, use the fewest number of ancestors that together have this proportion of mappable reads. The values must be between <code>0.5</code> and <code>1.0</code> <code>1.0</code> <code>--likely-ancestor-file</code> If <code>--use-likely-ancestors = true</code>, a record of the ancestors used for each sample will be written to this file, if a file name is provided. <code>\"\"</code> <code>--threads</code> The number of threads used to find paths. <code>3</code>"},{"location":"imputation_ropebwt/#likely-ancestors","title":"Likely ancestors","text":"<p>The term \"ancestors\" refers to the taxa or samples (i.e., assemblies) used to construct the haplotype graph used for imputation. The term \"ancestors\" is used because the model considers the haplotypes in the graph to represent the potential ancestral haplotypes from which the samples to be imputed were derived.</p> <p>In some scenarios, limiting the number of ancestors used to impute a sample will be useful. Some examples include:</p> <ul> <li>if some samples were derived from an F1 cross and the individuals   used to build the graph include those parents, using only those   parents for imputation can improve accuracy.</li> <li>Also, because imputing diploid   paths is more computationally intensive than haploid paths, limiting   the number of ancestors used per sample may be necessary to control   the amount of time required to impute each sample.</li> </ul> <p>To restrict the number of ancestors used, set the <code>--use-likely-ancestors</code> parameter to <code>true</code>, and provide a value for either <code>--max-ancestors</code> or <code>--min-coverage</code>. For the case where the samples have been derived from a limited number of ancestors setting <code>--min-coverage</code> to <code>0.95</code> or <code>--max-ancestors</code> to the expected number of ancestors is a useful strategy. In either case, providing a name for the output file saves a record of the ancestors used for each sample and should be checked to make sure the samples behaved as expected.</p> <p>When using this method, ancestors are chosen by first counting the number of reads for a sample that map to each ancestor in the haplotype graph. The ancestor with the most mapped reads is chosen. Next the reads not mapping to first chosen ancestor are used to count reads mapping to the remaining ancestors and the ancestor with the most reads is selected. This is repeated until either the proportion of reads mapping to the selected ancestors &gt;= <code>--min-coverage</code> or the number ancestors selected equals <code>--max-ancestors</code>. If a name is provided for the <code>--likely-ancestor-file</code>, the chosen ancestors are written to that file in the order they were chosen along with the cumulative coverage, which is the proportion of reads mapping to each ancestor and the previously chosen ones.</p>"},{"location":"imputation_ropebwt/#general-output-and-next-steps","title":"General output and next steps","text":"<p>Now that we have newly imputed hVCF data, our example working directory looks like the following:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 short_reads\n\u2502   \u2502   \u251c\u2500\u2500 LineA_LineB_1.fq\n\u2502   \u2502   \u2514\u2500\u2500 LineA_LineB_2.fq\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies\n\u2502   \u2502   \u251c\u2500\u2500 Ref.fa\n\u2502   \u2502   \u251c\u2500\u2500 LineA.fa\n\u2502   \u2502   \u2514\u2500\u2500 LineB.fa\n\u2502   \u251c\u2500\u2500 vcf_files\n\u2502   \u2502   \u251c\u2500\u2500 LineA.h.vcf\n\u2502   \u2502   \u2514\u2500\u2500 LineB.h.vcf\n\u2502   \u2502   vcf_files_imputed *\n\u2502   \u2502   \u2514\u2500\u2500 LineA_B.h.vcf * \n\u2502   \u2514\u2500\u2500 read_mappings\n\u2502       \u2514\u2500\u2500 LineA_LineB_1_readMapping.txt\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre> <p>Similar to the \"Build and Load\" steps, we can now load this data into our TileDB instance using the <code>load-vcf</code> command:</p> <pre><code>./phg load-vcf \\\n    --vcf-dir output/vcf_files_imputed \\\n    --db-path vcf_dbs \\\n    --threads 10\n</code></pre> <p>This command takes three parameters:</p> <ul> <li><code>--vcf-dir</code> - Directory containing hVCF data. Since I have imputed   data in a specific subdirectory, I will use the path   <code>output/vcf_files_imputed</code></li> <li><code>--db-path</code> - Path to the directory containing the TileDB instances.</li> <li><code>--threads</code> - Number of threads for use by the TileDB loading   procedure.</li> </ul>"},{"location":"imputation_ropebwt/#create-gvcf-files-optional","title":"Create g.vcf files (OPTIONAL)","text":"<p>Our imputed hVCF files provide data on a haplotype level. If desired we can take the hVCF files and create gVCF files. This provides SNP level data and is done using the <code>hvcf2gvcf</code> command:</p> <pre><code>./phg hvcf2gvcf \\\n    --hvcf-dir output/vcf_files_imputed \\\n    --db-path vcf_dbs \\\n    --reference-file output/updated_assemblies/Ref.fa \\\n    --output-dir output/gvcf_files\n</code></pre> <p>This command takes 4 parameters:</p> <ul> <li><code>--hvcf-dir</code> - Directory containing hVCF data. Since I have imputed   data in a specific subdirectory, I will use the path   <code>output/vcf_files_imputed</code></li> <li><code>--db-path</code> - Path to the directory containing the TileDB instances.</li> <li><code>--reference-file</code> - The reference genome fasta file.</li> <li><code>--output-dir</code> - The directory to place the gVCF files.</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>In this document, we will discuss the overall steps on how to download and set up the PHGv2 package.</p>"},{"location":"installation/#quick-start","title":"Quick start","text":"<ul> <li>Run on a Unix-based operating system (Windows not currently tested)</li> <li>Make sure you have \\(\\geq\\) Java 17</li> <li>Make sure you have Miniconda installed</li> <li>Make sure you have the libmamba solver installed:   <pre><code>conda update -n base conda\nconda install -n base conda-libmamba-solver\nconda config --set solver libmamba\n</code></pre></li> <li>Download the latest release of the PHGv2 package:   <pre><code>curl -s https://api.github.com/repos/maize-genetics/phg_v2/releases/latest \\\n| awk -F': ' '/browser_download_url/ &amp;&amp; /\\.tar/ {gsub(/\"/, \"\", $(NF)); system(\"curl -LO \" $(NF))}'\n</code></pre></li> <li> <p>Untar the package:   <pre><code>tar -xvf &lt;PHGv2_Release&gt;.tar\n</code></pre></p> </li> <li> <p>Navigate into uncompressed PHGv2 package:   <pre><code>cd phg/bin\n</code></pre></p> </li> <li>Invoke PHGv2 through the <code>phg</code> wrapper script:   <pre><code>./phg --version\n</code></pre></li> <li>Basic syntax is:   <pre><code>phg [&lt;options&gt;] &lt;command&gt; [&lt;args&gt;]...\n</code></pre></li> </ul>"},{"location":"installation/#requirements","title":"Requirements","text":"<p>PHGv2 requires basic software components: a Unix-based operating  system and Java version 17 or higher. PHGv2  also relies on external programs for alignment and storage, including  AnchorWave and  TileDB-VCF.  To facilitate this, we strongly recommend using the  Conda  environment management system, with a focus on the lightweight Conda  package manager, Miniconda.</p> <p>Note</p> <p>This has currently been tested on Fedora- and  Debian-derived Unix systems</p> <p>Note</p> <p>AnchorWave is currently not supported on Windows.  See AnchorWave documentation for further details.</p>"},{"location":"installation/#get-phgv2","title":"Get PHGv2","text":"<p>You can download the latest version of PHGv2  here.  Assuming you have downloaded PHGv2 locally, these instructions  presume you will run the program directly. Obtain the <code>.tar</code> file  manually from the provided link or use the following <code>curl</code> and <code>awk</code>  commands to retrieve the latest release:</p> <pre><code>curl -s https://api.github.com/repos/maize-genetics/phg_v2/releases/latest \\\n| awk -F': ' '/browser_download_url/ &amp;&amp; /\\.tar/ {gsub(/\"/, \"\", $(NF)); system(\"curl -LO \" $(NF))}'\n</code></pre> <p>Once downloaded, untar the release using: <pre><code>tar -xvf &lt;PHGv2_release&gt;.tar\n</code></pre> ...where <code>&lt;PHGv2_release&gt;.tar</code> is the downloaded PHGv2 package. After the source has been decompressed, we can remove the initial tar file using:  <pre><code>rm &lt;PHGv2_release&gt;.tar\n</code></pre></p>"},{"location":"installation/#installation_1","title":"\"Installation\"","text":"<p>No traditional installation is required, as the precompiled jar  files are designed to function on any  POSIX platform meeting the  specified requirements. Just open the downloaded package and place  the folder containing the jar files and launch script in a preferred  directory on your hard drive or server filesystem.</p> <p>To run PHGv2, you can manually enter into the package directory and run the <code>phg</code> wrapper script from the <code>bin</code> directory. Another option is to add the wrapper script to your <code>PATH</code> variable. If you are using the <code>bash</code> terminal shell, the classic syntax is:</p> <pre><code>export PATH=\"/path/to/phgv2-package/:$PATH\"\n</code></pre> <p>...where <code>/path/to/phgv2-package/</code> is the path to the location of the <code>phg</code> executable wrapper script.</p> <p>Note</p> <p>The above path example must be the path to the <code>bin</code> subdirectory found in the <code>phg</code> directory.</p> <p>Note</p> <p>The Java JAR files (<code>.jar</code>) in the <code>lib</code> subdirectory must remain in the same directory as <code>phg</code> for it to work.</p> <p>Note</p> <p>Be sure to include the final <code>/</code> in your path.</p>"},{"location":"installation/#test-that-phgv2-works","title":"Test that PHGv2 works","text":"<p>To test that you can successfully run the <code>phg</code> executable. Run the following command:</p> <pre><code>./phg --help\n</code></pre> <p>Note</p> <p>This assumes that you have added <code>phg</code> to your <code>PATH</code> using the above example, or you are within the <code>bin</code> subdirectory.</p> <p>This should output summary text to the terminal including syntax help and a list of subcommands and descriptions.</p>"},{"location":"installation/#setting-memory","title":"Setting memory","text":"<p>The amount of data you wish to process will affect the amount of computational resources that you will need. Since PHGv2 leverages a Java virtual machine  (JVM) for a majority of its tasks, we can manually alter the maximum  amount of memory allocated to the JVM using the following command  prompt:</p> <pre><code>export JAVA_OPTS=\"-Xmx&lt;memory_amount&gt;\"\n</code></pre> <p>...where <code>&lt;memory_amount&gt;</code> is a specified unit of memory. For  example, if I want to allocate a maximum of 50 gigabytes (GB) of  memory for my operations, I would use the input <code>\"-Xmx50g\"</code>, where <code>g</code> stands for GB:</p> <pre><code>export JAVA_OPTS=\"-Xmx50g\"\n</code></pre> <p>Note</p> <p>In order for memory to properly be set, you must set this before running any of the PHGv2 commands.</p> <p>Note</p> <p>Setting JVM memory will only affect JVM-intensive commands. Since PGHv2 utilizes several external pieces of software several commands will not be affected by this. Currently, these are:</p> <ul> <li><code>setup-environment</code></li> <li><code>initdb</code></li> <li><code>align-assemblies</code></li> <li><code>agc-compress</code></li> </ul> <p>...which rely on conda, TileDB, AnchorWave, and AGC, respectively.</p>"},{"location":"ktor_specifications/","title":"Ktor Specifications","text":"<p>In this document, we will discuss the BrAPI  endpoints provided by the Ktor server implemented in the  <code>start-server</code> command.</p>"},{"location":"ktor_specifications/#overview","title":"Overview","text":"<p>As stated in the \"Export data\" documentation, <code>start-server</code> is a command that allows users to start a  \"RESTful\" web service. This  service can provide access to a centralized PHG database, allowing  multiple individuals in a team to simultaneously retrieve  PHG-relevant information. This service relies on two components: the Ktor framework and BrAPI-compliant endpoints.</p> <p>Ktor is an asynchronous framework for building  web applications and microservices in Kotlin. It is designed for  creating connected systems, focusing on scalability, high  performance, and ease of use. Ktor supports both server and  client-side development, allowing developers to create RESTful APIs,  web sockets, and various other server applications. It provides a  DSL (Domain-Specific Language) to define routing, request handling,  and response production, making it highly customizable and flexible  for a wide range of use cases.</p> <p>This framework aids in the deployment of BrAPI-compliant data  endpoints. BrAPI is a standardized RESTful web service API  specification for plant breeding data. It ensures interoperability  among breeding databases and tools by providing a common framework.  Additionally (and most importantly), BrAPI is developed by a global  community of contributors and is intended to be an open and  accessible standard for anyone involved in plant breeding data  management.</p>"},{"location":"ktor_specifications/#relevant-endpoints","title":"Relevant endpoints","text":"<p>While BrAPI comprises several \"modules\", PHGv2 leverages data endpoints found within the core  and genotyping modules:</p> <ul> <li><code>serverinfo</code><ul> <li>Usage: <code>&lt;host-url&gt;:&lt;port&gt;/brapi/v2/serverinfo</code></li> <li>Find all available BrAPI calls implemented for the server</li> </ul> </li> <li><code>samples</code><ul> <li>Usage: <code>&lt;host-url&gt;:&lt;port&gt;/brapi/v2/samples</code></li> <li>List all samples available in the PHG database</li> </ul> </li> <li><code>variants</code><ul> <li>Usage: <code>&lt;host-url&gt;:&lt;port&gt;/brapi/v2/variants</code></li> <li>List all reference ranges available in the PHG database</li> </ul> </li> <li><code>variantsets</code><ul> <li>Usage: <code>&lt;host-url&gt;:&lt;port&gt;/brapi/v2/variantsets</code></li> <li>Downloadable link for a composite hVCF   file. (Currently for all data)</li> </ul> </li> </ul>"},{"location":"ktor_specifications/#response-fields","title":"Response fields","text":"<p>Each of the prior BrAPI calls will contain \"key-value\" response fields which contain the actual data:</p> <ul> <li><code>serverinfo</code><ul> <li><code>calls</code> - array of available calls to the server<ul> <li><code>dataTypes</code> - possible data formats returned by the    available call (e.g., <code>\"APPLICATION_JSON\"</code>)</li> <li><code>methods</code> - HTTP methods to be used for each call (e.g., <code>GET</code>)</li> <li><code>service</code> - name of the call (e.g., <code>samples</code>)</li> <li><code>version</code> - supported versions for a given call (e.g., <code>\"_2\"</code>)</li> </ul> </li> </ul> </li> <li><code>samples</code><ul> <li><code>additionalInfo</code> - \"free\" space for further information (not   bound by any data type)</li> <li><code>sampleDbId</code> - internal ID value for given sample</li> <li><code>sampleDescription</code> - description of sample</li> <li><code>sampleName</code> - name of sample in PHG database</li> </ul> </li> <li><code>variants</code><ul> <li><code>additionalInfo</code> - an object for additional information</li> <li><code>alternateBases</code> - an array of alternate base sequences</li> <li><code>ciend</code> - an array of confidence interval end positions</li> <li><code>cipos</code> - an array of confidence interval start positions</li> <li><code>created</code> - the creation timestamp of the variant</li> <li><code>end</code> - the end position of the variant</li> <li><code>filtersApplied</code> - a boolean indicating if filters were applied</li> <li><code>filtersFailed</code> - an array of failed filters</li> <li><code>filtersPassed</code> - a boolean indicating if the variant passed filters</li> <li><code>referenceBases</code> - the reference base sequence</li> <li><code>referenceName</code> - the reference chromosome or scaffold name</li> <li><code>start</code> - the start position of the variant</li> <li><code>svlen</code> - the structural variant length</li> <li><code>updated</code> - the last update timestamp of the variant</li> <li><code>variantDbId</code> - the unique identifier of the variant</li> <li><code>variantNames</code> - an array of variant names</li> <li><code>variantSetDbId</code> - an array of associated variant set IDs</li> <li><code>variantType</code> - the type of variant (e.g., <code>\"REF_RANGE\"</code>)</li> </ul> </li> <li><code>variantsets</code><ul> <li><code>additionalInfo</code> - an object for additional information</li> <li><code>analysis</code> - an array of analysis related to the variant set</li> <li><code>availableFormats</code> - a list of available formats with details<ul> <li><code>dataFormat</code> - the data format (e.g., <code>\"VCF\"</code>)</li> <li><code>fileFormat</code> - the file format (e.g., <code>\"TEXT_TSV\"</code>)</li> <li><code>fileURL</code> - the URL to access the file</li> </ul> </li> <li><code>callSetCount</code> - the count of call sets</li> <li><code>referenceSetDbId</code> - the unique identifier of the reference set</li> <li><code>studyDbId</code> - the unique identifier of the study</li> <li><code>variantCount</code> - the count of variants</li> <li><code>variantSetDbId</code> - the unique identifier of the variant set</li> <li><code>variantSetName</code> - the name of the variant set</li> </ul> </li> </ul>"},{"location":"ps4g_specifications/","title":"PS4G - Positional Support for Gametes Specification","text":"<ul> <li>Specification version: <code>v2.0</code></li> <li>Date: 2025-10-24</li> </ul>"},{"location":"ps4g_specifications/#overview","title":"Overview","text":"<p>A PS4G (Positional Support for Gametes) file is a standardized tab-delimited format that tracks genomic support for reference panel gametes across binned genomic positions. This format provides positional tracking for genomic data from multiple sources (e.g., variants, read alignments), thereby enhancing pathfinding support and enabling integration with machine learning-based imputation engines.</p> <p>PS4G files aggregate evidence from sequencing reads or genotype calls to determine which reference panel haplotypes (gametes) are supported at different genomic locations. This compressed representation enables efficient downstream imputation and haplotype inference.</p>"},{"location":"ps4g_specifications/#file-format","title":"File Format","text":""},{"location":"ps4g_specifications/#structure","title":"Structure","text":"<p>A PS4G file consists of:</p> <ol> <li>Header section - Metadata lines prefixed with <code>#</code></li> <li>Column header line - Tab-delimited field names</li> <li>Data section - Tab-delimited data lines</li> </ol>"},{"location":"ps4g_specifications/#header-section","title":"Header Section","text":"<p>The header contains metadata about the file creation and reference gametes:</p> <pre><code>#PS4G\n#version=2.0\n#&lt;metadata lines&gt;\n#Command: &lt;CLI command used to generate this file&gt;\n#TotalUniqueCounts: &lt;sum of all counts in the file&gt;\n#gamete gameteIndex count\n#&lt;SampleGamete&gt; &lt;index&gt; &lt;total_count&gt;\n...\n</code></pre> <p>Header fields:</p> Field Description <code>#PS4G</code> File format identifier (required first line) <code>#version</code> PS4G format version (2.0 for current specification) <code>#Command</code> Full CLI command used to generate the file <code>#TotalUniqueCounts</code> Sum of all unique position counts in the file <code>#gamete</code> Reference panel gamete identifier (format: <code>SampleName</code> or <code>SampleName:gameteIndex</code>) <code>gameteIndex</code> Zero-based integer index assigned to each gamete <code>count</code> Total number of observations supporting this gamete across all positions"},{"location":"ps4g_specifications/#data-section","title":"Data Section","text":"<p>After the header, the column names are defined, followed by data rows:</p> <pre><code>gameteSet   refContig   refPosBinned    count\n&lt;comma-separated gamete indices&gt;    &lt;contig&gt;    &lt;binned position&gt;   &lt;count&gt;\n</code></pre> <p>Data fields:</p> Field Type Description <code>gameteSet</code> String Comma-separated list of gamete indices (from header) that are supported at this position <code>refContig</code> String Reference contig/chromosome identifier <code>refPosBinned</code> Integer Binned genomic position (actual position divided by 256) <code>count</code> Integer Number of reads/variants supporting this gamete set at this position"},{"location":"ps4g_specifications/#example","title":"Example","text":"<pre><code>#PS4G\n#version=2.0\n#Command: phg convert-rm2ps4g-file --read-mapping-file input.txt --hvcf-dir /path/to/hvcfs --output-dir output/\n#TotalUniqueCounts: 1234\n#gamete gameteIndex count\n#LineA  0   853\n#LineB  1   381\n#Ref    2   100\ngameteSet   refContig   refPosBinned    count\n0   chr1    1000    853\n0,1 chr1    2000    24\n1   chr2    500 15\n0,1,2   chr2    1500    5\n</code></pre> <p>In this example:</p> <ul> <li>Row 1: Gamete 0 (<code>LineA</code>) is supported at chr1 binned position 1000 (actual position ~256,000 bp) by 853 reads</li> <li>Row 2: Gametes 0 and 1 (<code>LineA</code> and <code>LineB</code>) are both supported at chr1 position 2000 (~512,000 bp) by 24 reads</li> <li>Row 3: Gamete 1 (<code>LineB</code>) is supported at chr2 position 500 (~128,000 bp) by 15 reads</li> <li>Row 4: All three gametes are supported at chr2 position 1500 (~384,000 bp) by 5 reads</li> </ul>"},{"location":"ps4g_specifications/#position-binning","title":"Position Binning","text":"<p>To reduce file size and provide efficient storage, genomic positions are binned into 256 bp windows:</p> <p>Binning process:</p> <p>The <code>refPosBinned</code> field stores the genomic position divided by 256 (integer division):</p> <pre><code>refPosBinned = genomicPosition / 256\n</code></pre> <p>Converting back to approximate genomic position:</p> <pre><code>approximateGenomicPosition = refPosBinned * 256\n</code></pre> <p>Resolution and Compression</p> <p>The 256 bp binning provides a balance between positional resolution and file compression. The actual genomic position is rounded down to the nearest 256 bp boundary during binning. This resolution is suitable for chromosome-scale imputation and pathfinding algorithms.</p> <p>Binning Example</p> <ul> <li>Genomic position 256,000 bp \u2192 Binned position 1,000</li> <li>Genomic position 256,255 bp \u2192 Binned position 1,000 (same bin)</li> <li>Genomic position 256,256 bp \u2192 Binned position 1,001 (next bin)</li> </ul>"},{"location":"ps4g_specifications/#generation-methods","title":"Generation Methods","text":"<p>PS4G files can be generated from three different sources:</p>"},{"location":"ps4g_specifications/#1-from-read-mapping-files-convert-rm2ps4g-file","title":"1. From Read Mapping Files (<code>convert-rm2ps4g-file</code>)","text":"<p>Converts PHG read mapping output (from k-mer or RopeBWT mapping) to PS4G format.</p> <p>Input: Read mapping file with format: <pre><code>HapIds  count\n&lt;comma-separated haplotype IDs&gt; &lt;count&gt;\n</code></pre></p> <p>Process:</p> <ol> <li>Maps haplotype IDs to reference ranges</li> <li>Identifies gametes at each reference range</li> <li>Determines position from reference range start</li> <li>Aggregates counts by gamete set and position</li> </ol> <p>Command: <pre><code>phg convert-rm2ps4g-file \\\n    --read-mapping-file mapping.txt \\\n    --hvcf-dir /path/to/hvcfs \\\n    --output-dir output/\n</code></pre></p>"},{"location":"ps4g_specifications/#2-from-ropebwt-bed-files-convert-ropebwt2ps4g-file","title":"2. From RopeBWT BED Files (<code>convert-ropebwt2ps4g-file</code>)","text":"<p>Converts RopeBWT3 maximal exact match (MEM) alignments to PS4G format using spline-based coordinate transformation.</p> <p>Input: BED file from RopeBWT3 with MEM alignments</p> <p>Process:</p> <ol> <li>Loads spline knots for coordinate transformation from assembly to reference coordinates</li> <li>Groups MEMs by read</li> <li>Filters by minimum MEM length and maximum hits</li> <li>Uses spline interpolation to map assembly positions to reference positions</li> <li>Creates consensus position from multiple MEMs</li> <li>Aggregates gamete support by position</li> </ol> <p>Command: <pre><code>phg convert-ropebwt2ps4g-file \\\n    --ropebwt-bed alignments.bed \\\n    --spline-knot-dir /path/to/splines \\\n    --output-dir output/ \\\n    --min-mem-length 148 \\\n    --max-num-hits 50\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>--min-mem-length</code>: Minimum MEM length to consider (default: 148)</li> <li><code>--max-num-hits</code>: Maximum number of haplotype hits allowed (default: 50)</li> <li><code>--sort-positions</code>: Sort output by genomic position (default: true)</li> </ul>"},{"location":"ps4g_specifications/#3-from-vcf-files-convert-vcf2ps4g-file","title":"3. From VCF Files (<code>convert-vcf2ps4g-file</code>)","text":"<p>Converts variant calls to PS4G format using a reference panel for gamete matching.</p> <p>Input:</p> <ul> <li>Sample VCF file (to be imputed)</li> <li>Reference panel VCF file</li> </ul> <p>Process:</p> <ol> <li>Builds allele-to-gamete lookup from reference panel</li> <li>For each variant in sample VCF:</li> <li>Matches alleles to reference panel gametes</li> <li>Records gamete support at that position</li> <li>Aggregates counts by gamete set and position</li> </ol> <p>Command: <pre><code>phg convert-vcf2ps4g-file \\\n    --to-impute-vcf sample.vcf \\\n    --ref-panel-vcf reference_panel.vcf \\\n    --output-dir output/\n</code></pre></p>"},{"location":"ps4g_specifications/#data-interpretation","title":"Data Interpretation","text":""},{"location":"ps4g_specifications/#gamete-sets","title":"Gamete Sets","text":"<p>The <code>gameteSet</code> field contains indices of gametes that share evidence at a position. Multiple gametes in a set indicate:</p> <ul> <li>From read data: Reads mapping ambiguously to multiple haplotypes</li> <li>From VCF data: Shared alleles across multiple reference samples</li> </ul>"},{"location":"ps4g_specifications/#position-accuracy","title":"Position Accuracy","text":"<p>Due to the 256 bp binning:</p> <ul> <li>Positions represent approximate genomic locations</li> <li>Multiple nearby variants/reads may contribute to the same bin</li> <li>Suitable for chromosome-scale imputation, not for fine-scale variant calling</li> </ul>"},{"location":"ps4g_specifications/#count-interpretation","title":"Count Interpretation","text":"<p>The <code>count</code> field represents:</p> <ul> <li>Read mapping: Number of reads supporting this gamete combination</li> <li>VCF conversion: Number of variants matching this pattern</li> <li>Higher counts indicate stronger evidence for those gametes</li> </ul>"},{"location":"ps4g_specifications/#use-cases","title":"Use Cases","text":"<p>PS4G files are designed for:</p> <ol> <li>Machine learning-based imputation: Provide feature vectors for ML models to predict haplotypes</li> <li>Pathfinding algorithms: Inform hidden Markov models about gamete support across the genome</li> <li>Quality control: Assess read mapping quality and reference panel coverage</li> <li>Comparative analysis: Compare imputation results across different methods</li> </ol>"},{"location":"ps4g_specifications/#file-naming-convention","title":"File Naming Convention","text":"<p>PHG generates PS4G files with the naming pattern: <pre><code>&lt;input_basename&gt;_&lt;sampleGamete&gt;_ps4g.txt\n</code></pre></p> <p>Examples:</p> <ul> <li><code>LineA_1_readMapping_ps4g.txt</code> - From read mapping</li> <li><code>sample_alignments_ps4g.txt</code> - From RopeBWT</li> <li><code>input_vcf_ps4g.txt</code> - From VCF conversion</li> </ul>"},{"location":"ps4g_specifications/#related-commands","title":"Related Commands","text":"<ul> <li><code>rope-bwt-chr-index</code> - Create RopeBWT index for alignment</li> <li><code>build-spline-knots</code> - Generate spline knots for coordinate transformation</li> <li><code>map-reads</code> - Align reads to generate mapping files</li> <li>See Imputation using Machine Learning for complete workflow</li> </ul>"},{"location":"ps4g_specifications/#specification-notes","title":"Specification Notes","text":""},{"location":"ps4g_specifications/#version-history","title":"Version History","text":"<ul> <li>v2.0 (2025-10-24): Major format update - removed position encoding, split position into separate <code>refContig</code> and <code>refPosBinned</code> columns for improved readability and flexibility</li> <li>v1.0 (2025-02-19): Initial complete specification</li> <li>v0.1 (2025-02-19): Draft specification</li> </ul>"},{"location":"ps4g_specifications/#implementation","title":"Implementation","text":"<p>PS4G files are generated by PHGv2 commands in the <code>net.maizegenetics.phgv2.pathing.ropebwt</code> package:</p> <ul> <li><code>ConvertRm2Ps4gFile.kt</code> - Read mapping conversion</li> <li><code>ConvertRopebwt2Ps4gFile.kt</code> - RopeBWT conversion</li> <li><code>ConvertVcf2Ps4gFile.kt</code> - VCF conversion</li> <li><code>PS4GUtils.kt</code> - Shared utilities for file writing and data formatting</li> </ul>"},{"location":"qc_metrics/","title":"Quality Control Metrics","text":"<p>In this document, we will discuss the built-in quality control  metrics available from the PHG. Metrics accompany several steps of building and using the PHG, and many can be run as either standalone  commands or as part of a related command.</p>"},{"location":"qc_metrics/#anchorwave-dot-plots","title":"AnchorWave dot plots","text":"<p>Dot plots  generated from the <code>align-assemblies</code> step  are a visual representation of the alignment between two assemblies.  They are useful for identifying large-scale structural differences between assemblies, such as  inversions,  translocations,  and large deletions.</p> <p>There are three methods to produce AnchorWave dot plots:</p> <p>1. As part of the <code>align-assemblies</code> command:</p> <pre><code>./phg align-assemblies \\\n    --gff data/anchors.gff \\\n    --reference-file output/updated_assemblies/Ref.fa \\\n    --assemblies data/assemblies_list.txt \\\n    --total-threads 20 \\\n    --in-parallel 2 \\\n    -o output/alignment_files\n</code></pre> <p>The <code>align-assemblies</code> command runs code to create the dot plots  from AnchorWave's <code>.anchorspro</code> files. For each <code>.anchorspro</code> file that is generated, a genome-wide dot plot image is created and stored in the same location where <code>.maf</code> and <code>.anchorspro</code>  files are placed (<code>-o</code> parameter directory). Each image that is generated will have the assembly name found in the <code>--assemblies</code> parameter followed by the suffix, <code>_dotplot.svg</code>. * For example, if your assemblies list file (in my case, this   would be <code>assemblies_list.txt</code>) has the following assemblies   to align:   <pre><code>output/updated_assemblies/LineA.fa\noutput/updated_assemblies/LineB.fa\n</code></pre>   The plot output that is generated by default would be:   <pre><code>output/alignment_files/LineA_dotplot.svg\noutput/alignment_files/LineB_dotplot.svg\n</code></pre></p> <p>Note</p> <p>As of PHGv2 version <code>2.2.74.123</code>, the plots generated with the  <code>align-assemblies</code> command are part of the default output and  is not optional.</p> <p>2. As a standalone command:</p> <pre><code>phg create-anchorwave-dotplot \\\n    --input-file my.anchorspro \\\n    --output-file myOutputFile.svg\n</code></pre> <p>The <code>create-anchorwave-dotplot</code> command creates a dot plot from  a user provided <code>.anchorspro</code> file generated from AnchorWave.  Users may use the output from either the <code>align-assemblies</code>  command, or from any user run AnchorWave alignment as the  <code>--input-file</code> parameter to <code>create-anchorwave-dotplot</code>.</p> <p>Output from <code>create-anchorwave-dotplot</code> is written to the file  specified by the <code>--output-file</code> parameter. Users may specify  output files with extension of <code>.svg</code> or <code>.png</code> and the  appropriate file type will be generated.</p> <p>3. Using <code>rPHG2</code>:</p> <p>For more \"granular\" inspection of our alignment data, we can also use  the complimentary R package, <code>rPHG2</code>. Check out the \"Load and visualize PHG metrics\" section for more information.</p>"},{"location":"qc_metrics/#vcf-metrics","title":"VCF metrics","text":"<p>Once the gVCF and hVCF files for the TileDB instances have been  created, we can produce a table summarizing metrics related to the  assemblies used to produce them. These metrics are useful for  identifying low-quality assemblies which you may wish to exclude from  the PHG, or detecting problems with the assembly alignments.</p> <p>There are three methods to produce or visualize VCF metrics:</p> <p>1. As part of <code>create-maf-vcf</code>:</p> <pre><code>phg create-maf-vcf \\\n    --db-path vcf_dbs \\\n    --bed output/ref_ranges.bed \\\n    --reference-file output/updated_assemblies/Ref.fa \\\n    --maf-dir output/alignment_files \\\n    -o output/vcf_files \\\n    --metrics-file output/vcf_files/VCFMetrics.tsv \\\n    --skip-metrics\n</code></pre> <p>By default, <code>create-maf-vcf</code> will write the VCF metrics table to the  same output directory as the VCF files (set with flag <code>-o</code>) and will  name the file <code>VCFMetrics.tsv</code>. The optional flag <code>--metrics-file</code>  can be used to change the destination of the table. </p> <p>If VCF metrics are not desired, the <code>--skip-metrics</code> flag can be used  to skip calculating and writing the metrics table altogether.  Generally, we recommend reviewing the built-in QC metrics at each  step of building the PHG, so the default behavior is to calculate  metrics.</p> <p>2. As a standalone command:</p> <pre><code>phg calc-vcf-metrics \\\n    --vcf-dir output/vcf_files \\\n    --output VCFMetrics.tsv\n</code></pre> <p>The <code>calc-vcf-metrics</code> command creates the VCF metrics table from a  directory of existing gVCF and hVCF files. It has two required  parameters: * <code>--vcf-dir</code> - The path to the directory containing assembly VCF    files. gVCF (<code>.g.vcf</code>) files are required, but the    corresponding hVCF files are optional. * <code>--output</code> - The name for the output metrics table</p> <p>Note</p> <p><code>calc-vcf-metrics</code> is not a general-purpose tool for VCF files.  It was designed specifically for the files produced by  <code>create-maf-vcf</code>, which are single-sample, haploid gVCFs and hVCFs. Metrics may not be accurate for VCF files that do not fit this  format.</p> <p>3. Visualize gVCF metrics: For more \"granular\" inspection of our gVCF data, we can also use  the complimentary R package, <code>rPHG2</code>. Check out the \"Load and visualize PHG metrics\" section for more information.</p>"},{"location":"qc_metrics/#output","title":"Output","text":"<p>For each gVCF file in the input directory, both chromosome-level and  assembly-level statistics are calculated, and each has its own row in  the table. The columns are as follows:</p> Column Description <code>taxa</code> The sample name in the gVCF file <code>chrom</code> The reference chromosome name. For assembly-level statistics this value is <code>ALL</code> <code>refLength</code> The length of the reference sequence <code>numSNPs</code> The number of SNP records in the gVCF file for the given chromosome <code>numIns</code> The number of insertion records in the gVCF file <code>numDel</code> The number of deletion records in the gVCF file <code>numNs</code> The number of N's/ambiguous bases in the assembly alignment <code>numBasesInserted</code> The number of bases inserted relative to the reference sequence <code>numBasesDeleted</code> The number of bases deleted relative to the reference sequence <code>percentIdentityWithRef</code> The proportion of bases relative to refLength that are the same base as the reference base <code>percentMappedToRef</code> The proportion of bases relative to refLength that are present in a gVCF record <code>meanInsertionSize</code> The mean size of insertion records <code>medianInsertionSize</code> The median size of insertion records <code>largestInsertion</code> The size of the largest insertion <code>meanDeletionSize</code> The mean size of deletion records <code>medianDeletionSize</code> The median size of deletion records <code>largestDeletion</code> The size of the largest deletion <code>refRangesWithHaplotype</code> The number of reference ranges with a non-missing haplotype in the hVCF file <code>haplotypesIdenticalToRef</code> The number of haplotypes identical to a reference haplotype in the hVCF file <p>The last two columns, <code>refRangesWithHaplotype</code> and  <code>haplotypesIdenticalToRef</code> require hVCF files to calculate, and are omitted if no hVCF files are present in the given directory. See the  hVCF specification documentation for more details about the hVCF  format.</p>"},{"location":"qc_metrics/#terminology-disambiguation","title":"Terminology disambiguation","text":"<ul> <li><code>percentIdentityWithRef</code> is roughly equivalent to the percentage of    bases contained in gVCF reference blocks with non-missing    genotypes. It also includes the padding base of each indel record    if that base is the same as the reference</li> <li><code>percentMappedToRef</code> includes reference blocks, SNP records, and    indel records. It is equivalent to the percentage of the reference    covered by alignment blocks in the AnchorWave MAF files used to    create the VCF files</li> <li><code>refRangesWithHaplotype</code> In general, most assemblies will produce    haplotypes at most reference ranges. However, large deletions may    span an entire reference range or more, resulting in a missing    range. A large number of missing ranges may indicate a problem with    the assembly or the alignment.</li> </ul>"},{"location":"qc_metrics/#imputation-metrics","title":"Imputation metrics","text":"<p>Calculate imputation metrics for an hvcf file created by the Imputation pipeline</p> <p>Command - <code>imputation-metrics</code></p> <p>Example</p> <pre><code>phg imputation-metrics \\\n    --sample-hvcf my/parent/hvcf/from/create-maf-vcf \\\n    --imputation-hvcf my/imputation/h.vcf \\\n    --bed-file path/to/ranges.bed \\\n    --read-mapping-files my/readMappingList.txt \\\n    --output-file output/imputation_metrics.txt \n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--sample-hvcf</code> Path to hvcf for the sample expected to be selected most often. <code>\"\"</code> <code>--imputation-hvcf</code> Path to hvcf created from imputation pipeline. <code>\"\"</code> <code>--bed-file</code> Path to bed file created in create-ranges. <code>\"\"</code> <code>--read-mapping-files</code> Path to a file that contains a list of read-mapping files, one per line, full path names. <code>\"\"</code> <code>--output-file</code> Path to a file where metrics will be printed. <code>\"\"</code> `--chrom' Comma-separated list of contigs to include in output (optional) <p>The <code>imputation-metrics</code> command calculates metrics for a given h.vcf file created as the output of find-paths. It requires input that is not available to the find-paths command, so must be run separately from that command.</p> <p>The intent of this command is to provide a way to evaluate the quality of the imputation process.  The metrics allow the user to see the path the imputation has taken, where it has diverged from the parent (sample) haplotype, and shows the read counts from the read mappings files that support each transition. The output is a tab-delimited file with data arranged in reference range order that can be opened in a  spreadsheet program for further analysis.</p>"},{"location":"qc_metrics/#read-mapping-metrics","title":"Read mapping metrics","text":""},{"location":"qc_metrics/#return-qc-metrics-for-read-mappings","title":"Return QC metrics for read mappings","text":"<p>Note</p> <p>Need clarification!</p> <p>Using FASTQ files as an input, this command creates a tab-delimited table reporting each k-mer generated from the FASTQ files and how well they map to haplotypes in the PHG database.</p> <p>Command - <code>qc-read-mapping</code></p> <p>Example</p> <pre><code>phg qc-read-mapping \\\n    --hvcf-dir path/to/hvcf_directory \\\n    --output-dir path/to/output_directory \\\n    --read-files reads1.fastq,reads2.fastq \\\n    --kmer-index path/to/kmerIndex.txt \\\n    --num-reads 20\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--hvcf-dir</code> Directory containing hVCF files used to build the HaplotypeGraph. <code>\"\"</code> <code>--output-dir</code> Output folder for the read mapping results. <code>\"\"</code> <code>--read-files</code> Comma separated list of FASTQ read files to map. <code>\"\"</code> <code>--kmer-index</code> K-mer index file created by <code>build-kmer-index</code>. If not provided, defaults to <code>&lt;hvcfDir&gt;/kmerIndex.txt</code>. <code>&lt;hvcfDir&gt;/kmerIndex.txt</code> <code>--num-reads</code> Number of reads to process. This value controls how many reads are processed from each file. <code>20</code> <p>Example output</p> <pre><code>Kmer                                Hash1                               Hash2                               RefRanges                      Hapids\nACGTACGTACGTACGTACGTACGTACGTACGT    d41d8cd98f00b204e9800998ecf8427e    0cc175b9c0f1b6a831c399e269772661    chr1:1000-2000,chr2:3000-4000    {0=[B73, Ki11]}\nTGCACTGATGCACTGATGCACTGATGCACTGA    900150983cd24fb0d6963f7d28e17f72    f96b697d7cb7938d525a2f31aaf161d0    None                           {}\n</code></pre> <ul> <li><code>Kmer</code>: A 32-nucleotide sequence extracted from the read.</li> <li><code>Hash1</code> / <code>Hash2</code>: MD5 checksum\u2013like hash values computed for the k-mer.</li> <li><code>RefRanges</code>: A comma-separated list of reference ranges (formatted as <code>contig:start-end</code>) where the k-mer was found.</li> <li><code>Hapids</code>: A mapping from reference range IDs to the corresponding haplotype IDs that were matched.</li> </ul>"},{"location":"qc_metrics/#return-qc-metrics-for-read-mapping-counts","title":"Return QC metrics for read mapping counts","text":"<p>Using a read mapping file as input, this command will generate a tab-delimited report of QC metrics of read mapping counts for a given sample ID found the PHG DB.</p> <p>Command - <code>read-mapping-count-qc</code></p> <p>Example</p> <pre><code>phg read-mapping-count-qc \\\n    --hvcf-dir path/to/hvcf_directory \\\n    --read-mapping-file path/to/read_mapping.txt \\\n    --target-sample-name LineA \\\n    --output-dir path/to/output_directory\n</code></pre> <p>Parameters</p> Parameter name Description Default value Required? <code>--hvcf-dir</code> Directory with Haplotype VCF files. <code>\"\"</code> <code>--read-mapping-file</code> Read mapping file containing the mapping data. <code>\"\"</code> <code>--target-sample-name</code> Target sample name for which counts will be computed. <code>\"\"</code> <code>--output-dir</code> Output directory for writing the count QC results. <code>\"\"</code> <p>Example output</p> <pre><code>refRange    LineA_HapID LineA_HapCount  HighestAltCount Difference  OtherHapCounts\n1:1-1000    12f0cec9102e84a161866e37072443b7    853 0   853 0_4fc7b8af32ddd74e07cb49d147ef1938, 0_546d1839623a5b0ea98bbff9a8a320e2\n1:1001-5500 3149b3144f93134eb29661bade697fc6    4378    70  4308    70_8967fabf10e55d881caa6fe192e7d4ca, 0_57705b1e2541c7634ea59a48fc52026f\n1:5501-6500 1b568197f6f329ec5b71f66e49a732fb    855 47  808 47_05efe15d97db33185b64821791b01b0f, 0_d896e9cc56e74f39fd3f3c665191d727\n1:6501-11000    369464a8743d2e40ad83d1375c196bdd    4355    101 4254    101_8f7de1a693aa15fb8fb7b85e7a8b5e95, 24_66465399052d8ebe48b06329c60fee03\n</code></pre> <ul> <li><code>refRange</code>: The reference range.</li> <li><code>LineA_HapID</code>: The haplotype ID corresponding to the target   sample (e.g., <code>--target-sample-name LineA</code>).</li> <li><code>LineA_HapCount</code>: The count of reads mapping to the target   haplotype.</li> <li><code>HighestAltCount</code>: The highest count among non-target   haplotypes.</li> <li><code>Difference</code>: The difference between the target haplotype count   and the highest alternative count.</li> <li><code>OtherHapCounts</code>: A comma-separated list of counts and haplotype   IDs for all other haplotypes in the reference range.</li> </ul>"},{"location":"resequencing/","title":"Resequencing / Rare Alleles Pipeline","text":"<p>In this document, we will discuss the proposed pipeline to identify  rare alleles in a PHG population. This pipeline is invoked once a user  has successfully run the PHG imputation pipeline and  has secured a haplotype VCF (hVCF) file. </p> <p>From the imputation hVCF file, a composite FASTA can be created. The  pipeline begins by re-aligning the WGS reads used in the imputation  pipeline against the composite FASTA derived above. The BAM file  created by this second alignment is filtered, then run through a  variant caller. We suggest using DeepVariant or  Octopus for this purpose.  The output of the variant caller is a VCF file that contains the  variants called against the composite fasta.</p> <p>Additional filtering is then performed to select variants based on  VCF quality scores, depth or other measures. This final VCF provides  a list of SNPs that may be translated to haplotype coordinates. To  perform this transformation, the user can use the  <code>composite-to-haplotype-coords</code> command.</p>"},{"location":"resequencing/#quick-start","title":"Quick start","text":"<ul> <li> <p>Create a composite FASTA file</p> <pre><code>phg create-fasta-from-hvcf \\\n    --hvcf-file output/vcf_files_imputed/LineA_B.h.vcf \\ \n    --fasta-type composite \\ \n    -o output/composite_assemblies/\n</code></pre> </li> <li> <p>Index composite FASTA file</p> <pre><code>samtools faidx output/composite_assemblies/LineA_B_composite.fa\n</code></pre> </li> <li> <p>Align short reads to composite FASTA</p> <pre><code># Run minimap2 to align the reads with the composite genome\nminimap2 -a -x sr -t 20 --secondary=no --eqx \\\n    output/composite_assemblies/LineA_B_composite.fa \\\n    data/short_reads/LineA_LineB_1.fq \\\n    data/short_reads/LineA_LineB_2.fq | \\\n    samtools view -b &gt; output/minimap2/LineA_B_composite_align.bam\n</code></pre> </li> <li> <p>Filter BAM files</p> <pre><code>samtools fixmate -m LineA_B_composite_align.bam LineA_B_composite_fm.bam\nsamtools sort --threads 8 -T ./tmp -o LineA_B_composite_sorted_fm.bam LineA_B_composite_fm.bam\nsamtools markdup -r LineA_B_composite_sorted_fm.bam LineA_B_composite_dedup.bam\nsamtools view -F 4 -f 2 -b LineA_B_composite_dedup.bam &gt; LineA_B_composite_filtered.bam\nsamtools index LineA_B_composite_filtered.bam\n</code></pre> </li> <li> <p>Perform variant calling</p> <pre><code>PHG_DIR=phg_v2_example/\n\ndocker run \\\n    -v ${PHG_DIR}:/workdir \\\n    google/deepvariant:1.6.1 \\\n    /opt/deepvariant/bin/run_deepvariant \\\n    --model_type=WGS \\\n    --ref=/workdir/output/composite_assemblies/LineA_B_composite.fa \\\n    --reads=/workdir/output/minimap2/LineA_B_composite_filtered.bam \\\n    --output_vcf=/workdir/output/vcf_files/LineA_B_composite_reseq.vcf \\\n    --output_gvcf=/workdir/output/vcf_files/LineA_B_composite_reseq.g.vcf \\\n    --intermediate_results_dir=/workdir/intermediate_results \\\n    --num_shards=4\n</code></pre> </li> <li> <p>Filter variants</p> <pre><code>bcftools view -m2 -M2 -v snps \\\n    -i 'QUAL&gt;20 &amp;&amp; GT=\"1/1\" &amp;&amp; DP&gt;4' \\\n    output/vcf_files/LineA_B_composite_reseq.vcf \\\n    -o output/vcf_files/LineA_B_composite_reseq_filt.vcf\n</code></pre> </li> <li> <p>Create final VCF in haplotype coordinates</p> <pre><code>phg composite-to-haplotype-coords \\\n    --path-hvcf output/vcf_files_imputed/LineA_B.h.vcf \\\n    --variant-vcf output/vcf_files/LineA_B_composite_reseq_filt.vcf \\\n    --sample-name LineA_B \\\n    --output-file output/vcf_files/LineA_B_reseq_hap_coords.vcf\n</code></pre> </li> </ul>"},{"location":"resequencing/#detailed-walkthrough","title":"Detailed walkthrough","text":""},{"location":"resequencing/#create-a-composite-fasta-file","title":"Create a composite FASTA file","text":"<p>Using the prior example outputs from the imputation section, we can first convert the hVCF output from the <code>find-paths</code> command to a FASTA format via the <code>create-fasta-from-hvcf</code> command:</p> <pre><code>phg create-fasta-from-hvcf \\\n    --hvcf-file output/vcf_files_imputed/LineA_B.h.vcf \\ \n    --fasta-type composite \\ \n    -o output/composite_assemblies/\n</code></pre> <p>This command takes 3 necessary parameters:</p> <ul> <li><code>--hvcf-file</code> - an hVCF file from the imputation pipeline</li> <li><code>--fasta-type</code> - how should the output for the FASTA file look like?<ul> <li><code>composite</code> - concatenates haplotype sequences together at the   chromosome level (needed for this pipeline)</li> <li><code>haplotype</code> - each FASTA entry is a haplotype sequence</li> </ul> </li> <li><code>-o</code> or <code>--output-dir</code> - an existing output directory for the    FASTA file</li> </ul> <p>After running this command, our example directory (based on the prior pipelines) will now have a FASTA file (<code>LineA_B_composite.fa</code>)  in the <code>composite_assemblies/</code> directory:</p> <pre><code>phg_v2_example/\n\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 short_reads\n\u2502   \u2502   \u251c\u2500\u2500 LineA_LineB_1.fq\n\u2502   \u2502   \u2514\u2500\u2500 LineA_LineB_2.fq\n\u2502   \u251c\u2500\u2500 anchors.gff\n\u2502   \u251c\u2500\u2500 Ref-v5.fa\n\u2502   \u251c\u2500\u2500 LineA-final-01.fa\n\u2502   \u2514\u2500\u2500 LineB-final-04.fa\n\u251c\u2500\u2500 output\n\u2502   \u251c\u2500\u2500 alignment_files/\n\u2502   \u251c\u2500\u2500 composite_assemblies *\n\u2502   \u2502   \u2514\u2500\u2500 LineA_B_composite.fa *\n\u2502   \u251c\u2500\u2500 ref_ranges.bed\n\u2502   \u251c\u2500\u2500 updated_assemblies/\n\u2502   \u251c\u2500\u2500 vcf_files/\n\u2502   \u251c\u2500\u2500 vcf_files_imputed\n\u2502   \u2502   \u2514\u2500\u2500 LineA_B.h.vcf \n\u2502   \u2514\u2500\u2500 read_mappings/\n\u2514\u2500\u2500 vcf_dbs\n    \u251c\u2500\u2500 assemblies.agc\n    \u251c\u2500\u2500 gvcf_dataset/\n    \u251c\u2500\u2500 hvcf_dataset/\n    \u251c\u2500\u2500 hvcf_files/\n    \u251c\u2500\u2500 reference/\n    \u2514\u2500\u2500 temp/\n</code></pre>"},{"location":"resequencing/#index-composite-fasta-file","title":"Index composite FASTA file","text":"<p>Once we create the composite FASTA file, we will need to create an <code>.fai</code> index file for the  subsequent DeepVariant procedure. This indexing step will significantly decrease lookup times into our new composite reference FASTA assembly. To create this file, we can use <code>faidx</code> command from <code>samtools</code>:</p> <pre><code>samtools faidx output/composite_assemblies/LineA_B_composite.fa\n</code></pre> <p>This will create an <code>.fai</code> index file in the same directory where the composite FASTA was created. In our case, this file will be named <code>LineA_B_composite.fa.fai</code>.</p> <p>Note</p> <p>For the DeepVariant step, this file will need to be in the same directory as your composite FASTA file. DeepVariant will assume that this file and the FASTA assembly are in the same directory. If not, an error will occur!</p>"},{"location":"resequencing/#align-short-reads-to-composite-fasta","title":"Align short reads to composite FASTA","text":"<p>After creating the composite FASTA, we can re-align the WGS reads used for the imputation steps to this new composite assembly. This can be accomplished using a short-read aligner such as  minimap2 (recommended) or other  comparable software. Since this step is dependent on the choices made  by the end-user or group, we will provide an example of how to run  this via minimap2 in the following code block:</p> <pre><code># Run minimap2 to align the reads with the composite genome\nminimap2 -a -x sr -t 20 --secondary=no --eqx \\\n    output/composite_assemblies/LineA_B_composite.fa \\\n    data/short_reads/LineA_LineB_1.fq \\\n    data/short_reads/LineA_LineB_2.fq | \\\n    samtools view -b &gt; output/minimap2/LineA_B_composite_align.bam\n</code></pre> <p>In summary, this command is using the following <code>minimap2</code> parameters:</p> <ul> <li><code>-a</code> - output alignments in SAM   format</li> <li><code>-x sr</code> - alignment mode is short-read alignment</li> <li><code>-t 20</code> - run this alignment using 20 threads on our example   machine (this can be modified to leverage the full potential of   your machine)</li> <li><code>--secondary=no</code> - suppress secondary alignments and report only   the primary alignment</li> <li><code>--eqx</code> - output the CIGAR   string in \"extended\" format (<code>=</code> for matches and <code>X</code> for mismatches)</li> </ul> <p>This minimap2 procedure will create a SAM file that is piped (<code>|</code>) into the SAMtools program which is converted (<code>view -b</code>) and saved (<code>&gt; output/minimap2/LineA_B_composite_align.bam</code>) as a BAM format for more efficient storage and downstream procedures.</p>"},{"location":"resequencing/#filter-bam-files","title":"Filter BAM files","text":"<p>BAM files can be filtered using SAMtools (recommended) or other  SAM/BAM processing tools. We suggest running the following SAMtools commands: </p> <ul> <li> <p>Fix potential issues in paired-end reads of a BAM file by adjusting   and correcting mate information by ensuring each pair of reads has   accurate information about its mate score (<code>-m</code>):</p> <pre><code>samtools fixmate -m LineA_B_composite_align.bam LineA_B_composite_fm.bam\n</code></pre> </li> <li> <p>Sort alignments by reference sequence coordinates which is   necessary before deduplication and indexing:</p> <pre><code>samtools sort --threads 8 -T ./tmp -o LineA_B_composite_sorted_fm.bam LineA_B_composite_fm.bam\n</code></pre> </li> <li> <p>Identify and remove duplicate reads:</p> <pre><code>samtools markdup -r LineA_B_composite_sorted_fm.bam LineA_B_composite_dedup.bam\n</code></pre> </li> <li> <p>Exclude unmapped reads or those that are not properly paired:</p> <pre><code>samtools view -F 4 -f 2 -b LineA_B_composite_dedup.bam &gt; LineA_B_composite_filtered.bam\n</code></pre> </li> <li> <p>Index alignments for fast random access:</p> <pre><code>samtools index LineA_B_composite_filtered.bam\n</code></pre> </li> </ul>"},{"location":"resequencing/#perform-variant-calling","title":"Perform variant calling","text":"<p>After the BAM files have been filtered, we can now run variant calling. Since there are many different variant callers available, this step can become \"highly opinionated\" for various groups and users. In our current tests, we have found the results produced by DeepVariant to be of high quality.</p> <p>Note</p> <p>For more information on comparisons between different variant callers we have tested, please refer to the notes found on this page.</p> <p>Here we show an example of running DeepVariant on the prior BAM file.  The output of DeepVariant is a VCF file that contains  the variants called against the composite fasta made in the first step. Assuming you have retrieved the Docker image for DeepVariant, the following command is an example of the recommended parameters to use for the program:</p> <pre><code>PHG_DIR=phg_v2_example/\n\ndocker run \\\n    -v ${PHG_DIR}:/workdir \\\n    google/deepvariant:1.6.1 \\\n    /opt/deepvariant/bin/run_deepvariant \\\n    --model_type=WGS \\\n    --ref=/workdir/output/composite_assemblies/LineA_B_composite.fa \\\n    --reads=/workdir/output/minimap2/LineA_B_composite_filtered.bam \\\n    --output_vcf=/workdir/output/vcf_files/LineA_B_composite_reseq.vcf \\\n    --output_gvcf=/workdir/output/vcf_files/LineA_B_composite_reseq.g.vcf \\\n    --intermediate_results_dir=/workdir/intermediate_results \\\n    --num_shards=4\n</code></pre> <p>If you have limited experience in what Docker and/or DeepVariant  commands are, here we explain in further detail the rationale for each of these parameters:</p> <p>Docker commands:</p> <ul> <li><code>docker run</code>: base Docker command to run a container</li> <li><code>-v ${PHG_DIR}:/workdir</code>: This mounts the host system's (i.e.,    your machine) working PHG directory (specified as the variable, <code>PHG_DIR</code>)    to the <code>/workdir</code> directory inside the container. This would be   the path on your machine that contains the necessary files    (discussed below). Here, I am mounting it to the \"example\" PHG   directory that was used previously.</li> </ul> <p>DeepVariant commands:</p> <ul> <li><code>google/deepvariant:1.6.1</code>: This specifies the version of the   DeepVariant Docker image to use (<code>1.6.1</code> in this case since this is   the latest version as of writing this documentation).</li> <li><code>/opt/deepvariant/bin/run_deepvariant</code>: This specifies the path   inside the Docker container to the DeepVariant executable script   that orchestrates the variant calling pipeline.</li> </ul> <p>DeepVariant input parameters:</p> <ul> <li><code>--model_type=WGS</code>: Specifies the model type to use for variant   calling. In this case, <code>WGS</code> stands for \"Whole Genome    Sequencing\".</li> <li><code>--ref=/workdir/output/composite_assemblies/LineA_B_composite.fa</code>:   The reference genome in FASTA format that DeepVariant will use for   alignment and variant calling. The reference file in this case   will be the composite FASTA file created earlier.</li> <li><code>--reads=/workdir/output/minimap2/LineA_B_composite_filtered.bam</code>:   Specifies the prior sorted and indexed BAM file containing the   aligned sequencing reads.</li> <li><code>--output_vcf=/workdir/output/vcf_files/LineA_B_composite_reseq.vcf</code>:   The output VCF file path where DeepVariant will write the called   variants.</li> <li><code>--output_gvcf=/workdir/output/vcf_files/LineA_B_composite_reseq.g.vcf</code>:   The output gVCF file path.</li> <li><code>--intermediate_results_dir=/workdir/intermediate_results</code>: The   directory where DeepVariant will write intermediate files generated   during the analysis. These files are useful for debugging or   optimizing performance, but can be deleted after run if not needed.</li> <li><code>--num_shards=4</code>: This specifies the number of CPU cores or threads   to parallelize the job across. In this case, we are using <code>4</code> CPU   threads. The higher number of shards, the faster the job, but will   require more computational resources.</li> </ul>"},{"location":"resequencing/#filter-variants","title":"Filter variants","text":"<p>To identify rare alleles, the VCF file output by the variant caller  should be filtered to select variants based on quality scores, depth,  or other measures. The following is an example of how to use  bcftools to filter the VCF  file created above:</p> <p><pre><code>bcftools view -m2 -M2 -v snps \\\n    -i 'QUAL&gt;20 &amp;&amp; GT=\"1/1\" &amp;&amp; DP&gt;4' \\\n    output/vcf_files/LineA_B_composite_reseq.vcf \\\n    -o output/vcf_files/LineA_B_composite_reseq_filt.vcf\n</code></pre> In summary, this command is using the following bcftools parameters:</p> <ul> <li><code>-m2</code>: Specifies that only diploid variants should be included.   The value <code>2</code> represents the ploidy level (diploid).</li> <li><code>-M2</code>: Ensures that variants with more than two alleles are   excluded. Using this in conjunction with <code>-m2</code> ensures that only   biallelic diploid variants are retained.</li> <li><code>-v snps</code>: This option restricts the output to only SNPs.</li> <li><code>-i 'QUAL&gt;20 &amp;&amp; GT=\"1/1\" &amp;&amp; DP&gt;4'</code>: This specifies the filtering   conditions in which variants have the qualities:<ul> <li><code>QUAL&gt;20</code>: only include variants with a quality score greater   than 20. The quality score indicates the confidence of the   variant call, and this filter ensures that only high-confidence   variants are retained.</li> <li><code>GT=\"1/1\"</code>: this ensures that only homozygous alternate   variants are included (where the genotype is <code>1/1</code>,    indicating) both alleles are the alternate allele).</li> <li><code>DP&gt;4</code>: retain only variants with a depth of coverage   greater than 4. The depth of coverage (<code>DP</code>) indicates the   number of reads covering the variant position.</li> </ul> </li> <li><code>output/vcf_files/LineA_B_composite_reseq.vcf</code>: the   input VCF file that contains the variants to be filtered.</li> <li><code>-o output/vcf_files/LineA_B_composite_reseq_filt.vcf</code>:    the output file where the filtered variants will be saved.</li> </ul>"},{"location":"resequencing/#create-final-vcf-in-haplotype-coordinates","title":"Create final VCF in haplotype coordinates","text":"<p>Now that we have retained \"high quality\" variants, we can finally convert the coordinates from the composite FASTA file to coordinates found in the haplotypes within PHG population. To perform this, we can use a PHGv2 command called <code>composite-to-haplotype-coords</code>:</p> <p>Note</p> <p>The VCF coordinates created in this step will not be based on reference coordinates or coordinates relative to any single assembly. They will be relative to the haplotypes used to make the composite assembly!</p> <pre><code>phg composite-to-haplotype-coords \\\n    --path-hvcf output/vcf_files_imputed/LineA_B.h.vcf \\\n    --variant-vcf output/vcf_files/LineA_B_composite_reseq_filt.vcf \\\n    --sample-name LineA_B \\\n    --output-file output/vcf_files/LineA_B_reseq_hap_coords.vcf\n</code></pre> <p>In summary, this command takes 4 parameters:</p> <ul> <li><code>--path-hvcf</code>: path to the imputed hVCF file created during the   imputation steps.</li> <li><code>--variant-vcf</code>: path to the filtered VCF file    created by the prior variant caller (e.g., DeepVariant).</li> <li><code>--sample-name</code>: sample ID to include in the VCF file.</li> <li><code>--output-file</code>: path and name of coordinate converted VCF file.</li> </ul>"},{"location":"resequencing/#miscellaneous","title":"Miscellaneous","text":"<p>In the following sections, we present possible alternate workflows that may arise due to the attributes of your starting data:</p>"},{"location":"resequencing/#wgs-reads-coming-from-multiple-libraries","title":"WGS reads coming from multiple libraries:","text":"<p>In certain cases, you may have reads that come from multiple flowcell or sequencing libraries. To align short reads to the composite, you can add read grouping information (<code>-R</code>) to minimap2. In the following example, we have three WGS libraries for the sample <code>P39</code>:</p> <pre><code># Run minimap2 to align the reads with the composite genome\nFASTA_DIR=/workdir/lcj34/phg_v2/testing/fastas\nFASTQ_DIR=/workdir/lcj34/phg_v2/testing/fastqFiles\nWORK_DIR=/workdir/lcj34/phg_v2/testing\n\nminimap2 -a -x sr -t 20 --secondary=no --eqx -R '@RG\\tID:READ_GRP1\\tSM:P39' \\\n    ${FASTA_DIR}/P39wgs_composite.fa \\\n    ${FASTQ_DIR}/P39/P39_HBEN2ADXX_GTGAAA_R1.fastq.gz \\\n    ${FASTQ_DIR}/P39/P39_HBEN2ADXX_GTGAAA_R2.fastq.gz | \\\n    samtools view -b &gt; ${WORK_DIR}/p39top39composite_bams/P39_HBEN2ADXX_GTGAAA.ori.bam\n\nminimap2 -a -x sr -t 20 --secondary=no --eqx -R '@RG\\tID:READ_GRP2\\tSM:P39' \\\n    ${FASTA_DIR}/P39wgs_composite.fa \\\n    ${FASTQ_DIR}/P39/P39__HFFGKADXX_GTGAAA_R1.fastq.gz \\\n    ${FASTQ_DIR}/P39/P39__HFFGKADXX_GTGAAA_R2.fastq.gz | \\\n    samtools view -b &gt; ${WORK_DIR}/p39top39composite_bams/P39_HFFGKADXX_GTGAAA.ori.bam\n\nminimap2 -a -x sr -t 20 --secondary=no --eqx -R '@RG\\tID:READ_GRP3\\tSM:P39'\n    ${FASTA_DIR}/P39wgs_composite.fa \\\n    ${FASTQ_DIR}/P39/P39_HL5WNCCXX_GTGAAA_R1.fastq.gz \\\n    ${FASTQ_DIR}/P39/P39_HL5WNCCXX_GTGAAA_R2.fastq.gz | \\\n    samtools view -b &gt; ${WORK_DIR}/p39top39composite_bams/P39_HL5WNCCXX_GTGAAA.ori.bam\n</code></pre> <p>In the above example, read groupings are added using the <code>-R</code>  parameter found in minimap2. Each WGS library will get its own read group:</p> <ul> <li><code>ID:READ_GRP1</code> - <code>ID:READ_GRP3</code>: specifies that the read group   identifier is <code>READ_GRP1</code> through <code>READ_GRP3</code></li> <li><code>SM:P39</code>: The sample name. In this case, it is <code>P39</code></li> </ul> <p>After alignment, the BAM files can be merged into one using the  <code>merge</code> command from samtools:</p> <pre><code>samtools merge -b /path/to/bam.list /path/to/merged.bam\n</code></pre> <ul> <li> <p><code>-b</code>: A list of BAM files to merge. This would be a plain-text   list of file paths. For example:</p> <pre><code>/path/to/sample1.bam\n/path/to/sample2.bam\n/path/to/sample3.bam\n</code></pre> </li> </ul> <p>After merging, the BAM file can be sorted and indexed using the prior guidelines:</p> <pre><code>samtools sort --threads 8 -T ./tmp -o /path/to/merged.sorted.bam /path/to/merged.bam\nsamtools index /path/to/merged.sorted.bam\n</code></pre>"},{"location":"slurm_usage/","title":"SLURM Usage Guidelines for <code>align-assemblies</code>","text":"<p>When running <code>align-assemblies</code> on a single machine,  the <code>--total-threads</code> and <code>--in-parallel</code> parameters are used to  determine how many threads to allocate for the alignment step and  how many genomes to run in parallel. Users only need to call the  <code>align-assemblies</code> command once and there is no pre-processing steps  necessary.</p> <p>However, for HPC systems leveraging the SLURM job scheduler, parallel processes operate differently which will require slight modifications to the alignment step. In this document,  we will discuss how to use the <code>align-assemblies</code> command for usage  with the SLURM work scheduler system.</p>"},{"location":"slurm_usage/#workflow-overview","title":"Workflow Overview","text":""},{"location":"slurm_usage/#1-reference-preparation","title":"1 - Reference preparation","text":"<p>When running jobs in a SLURM array, the first step is to run the  <code>align-assemblies</code> command with the <code>--just-ref-prep</code> parameter set  to <code>true</code>. This will run the preliminary steps of aligning the  reference genome to the GFF CDS, creating the <code>reference-sam</code> and  <code>reference-cds-fasta</code> files needed for aligning the individual  assemblies. The output of this step will be written to the user  supplied output directory.</p>"},{"location":"slurm_usage/#2-slurm-data-array-file-creation","title":"2 - SLURM data array file creation","text":"<p>The second step would be to create the SLURM data array file. This is  a text file which has an <code>align-assemblies</code> command, one per line,  for each assembly that will be run. It will use the <code>reference-sam</code>  and <code>reference-cds-fasta</code> files created in the first step as input to  the <code>align-assemblies</code> command. Users can create the file manually if  they prefer, but for long lists of assemblies, manual curation can be  tedious and prone to errors. PHGv2 provides a convenience command (<code>prepare-slurm-align-file</code>) to automatically generate these files. See  the \"Prepare SLURM File for Alignments\" section for further details.</p>"},{"location":"slurm_usage/#3-job-submission","title":"3 - Job submission","text":"<p>The third step is to submit the SLURM job. When running a SLURM job,  the <code>--in-parallel</code> parameter is not used as each <code>align-assemblies</code>  command is assigned to separate computer nodes in the HPC system.  Conversely, the number of threads to use is still specified by the  <code>--total-threads</code> parameter.</p>"},{"location":"slurm_usage/#prepare-slurm-file-for-alignments","title":"Prepare SLURM File for Alignments","text":"<p>The <code>prepare-slurm-align-file</code> command is a convenience method for  creating a file that can be submitted to SLURM as a  data array job:</p> <pre><code>phg prepare-slurm-align-file  \\\n    --phg-location /path/to/phg \\\n    --gff data/anchors.gff \\\n    --reference-file output/updated_assemblies/Ref.fa \\\n    --reference-sam output/alignment_files/Ref.sam \\\n    --reference-cds-fasta output/alignment_files/ref.cds.fasta \\\n    --asemblies data/assemblies_list.txt \\\n    --ref-max-align-cov 1 \\\n    --query-max-align-cov 1 \\\n    --total-threads 20 \\\n    --conda-env-prefix /path/to/conda/env \\\n    --output-dir /path/for/align-assemblies/output \\\n    --slurm-file output/slurm_align_file.txt \\\n    -o output/alignment_files\n</code></pre>"},{"location":"slurm_usage/#parameters","title":"Parameters","text":"<p>This command uses several parameters: * <code>--phg-location</code> - The location of the phg executable.  The full path should be provided. This is needed to run the align-assemblies command.   If it is not specified, the current directory, ie <code>./phg</code>, will be assumed.</p> <ul> <li> <p><code>--gff</code> - GFF file for the reference genome. This is used to   identify full-length coding sequences to use as anchors</p> </li> <li> <p><code>--reference-file</code> - The reference genome in   FASTA format.</p> <ul> <li> <p>\u2139\ufe0f Note   The path to the reference genome should be the updated version   that was created during the <code>prepare-assemblies</code> command.</p> </li> </ul> </li> <li> <p><code>--reference-sam</code> - Optional parameter. If this is specified, the    optional parameter <code>--reference-cds-fasta</code> must also be supplied.    When both are supplied, the software skips the creation of these    files and uses those supplied by the user. This is desirable when    the user is running multiple assembly alignments from a SLURM    data-array option and does not wish to realign the reference    multiple times. If specified, but <code>--reference-cds-fasta</code> is not,    the software will throw an exception.</p> </li> <li> <p><code>--reference-cds-fasta</code> - Optional parameter. If this is specified,    the optional parameter <code>--reference-sam</code> must also be supplied.    When both are supplied, the software skips the creation of these    files and uses those supplied by the user. This is desirable when    the user is running multiple assembly alignments from a SLURM    data-array option and does not wish to realign the reference    multiple times. If specified, but <code>reference-sam</code> is not, the    software will throw an exception.</p> </li> <li> <p><code>--assemblies</code> - A text file containing a list of annotated     assembly genomes (see the      \"Prepare Assembly FASTA files\"      section for further details). The contents of the assembly list      file should be either full or relative paths to each uncompressed      assembly you would like to align. For example, since I am following     the steps laid out in the      \"Build and Load Documentation\",     I can create a text file called <code>assemblies_list.txt</code> (placed in     the <code>data/</code> subdirectory) and populate it with the following lines:</p> <pre><code>output/updated_assemblies/LineA.fa\noutput/updated_assemblies/LineB.fa\n</code></pre> <p>Here, I am planning on aligning two genomes called <code>LineA</code> and <code>LineB</code>. Since these are created with the <code>prepare-assemblies</code> command and the output is located in a subdirectory called <code>output/updated_assemblies/</code> relative to my working directory, I  will also add that to the path.</p> <ul> <li> <p>\u26a0\ufe0f Warning   This text list should not contain the path to the reference   genome since this is recognized in the <code>--reference-file</code> flag.</p> </li> </ul> </li> <li> <p><code>--ref-max-align-cov</code> - The maximum reference genome alignment    coverage. This is used in the <code>proali</code> command.   The default value is <code>1</code>.</p> </li> <li> <p><code>--query-max-align-cov</code> - The maximum query genome alignment    coverage.  This is used in the <code>proali</code> command.   The default value is <code>1</code>.</p> </li> <li> <p><code>--total-threads</code> - How many threads would you like to allocate for   the alignment step?</p> </li> <li> <p><code>--conda-env-prefix</code> - Optional parameter that specifies the path   to the Conda directory that contains the conda environment needed   to run phg. If not set, conda env <code>phgv2-conda</code> in the default   location will be used.</p> </li> <li> <p><code>--slurm-file</code> - The name of the file that will be created that    contains the SLURM commands to run the <code>align-assemblies</code> command    for each assembly in the list of assemblies.</p> </li> <li> <p><code>-o</code>, <code>--output-dir</code> - The name of the directory for the alignment outputs.</p> </li> </ul>"},{"location":"slurm_usage/#example-output","title":"Example output","text":"<p>Following along with the example data shown in the \"Build and Load\" documentation, my example SLURM data-array file  (<code>output/slurm_align_file.txt</code>) will look like the following since I have two samples:</p> <pre><code>./phg align-assemblies --gff data/anchors.gff --output-dir output/alignment_files --reference-file output/update_assemblies/Ref.fa --reference-sam output/alignment_files/Ref.sam --reference-cds-fasta output/alignment_files/ref.cds.fasta --assembly-file data/test/smallseq/LineA.fa --total-threads 20 in-parallel 1  --ref-max-align-cov 1 --query-max-align-cov 1\n./phg align-assemblies --gff data/anchors.gff --output-dir output/alignment_files --reference-file output/update_assemblies/Ref.fa --reference-sam output/alignment_files/Ref.sam --reference-cds-fasta output/alignment_files/ref.cds.fasta --assembly-file data/test/smallseq/LineB.fa --total-threads 20 in-parallel 1  --ref-max-align-cov 1 --query-max-align-cov 1\n</code></pre> <p>Note</p> <p>If the file specified by the <code>--assemblies</code> parameter contains 10 assemblies, the output file will contain 10 lines, each with a  call to the <code>align-assemblies</code> command for a single assembly. If the  file specified by the <code>--assemblies</code> parameter contains 100  assemblies, the output file will contain 100 lines.</p>"},{"location":"slurm_usage/#integrating-into-slurm-jobs","title":"Integrating into SLURM jobs","text":"<p>Since the output from <code>prepare-slurm-align-file</code> is simply a list of individual <code>align-assemblies</code> commands (each representing an individual assembly), we must pass this along to an actual SLURM array job. Below we have added an example SLURM script detailing some example parameters and code setup you may want to use for your applications:</p> <pre><code>#!/bin/bash\n\n#SBATCH --time=10:30:00  # walltime limit (HH:MM:SS)\n#SBATCH --nodes=1  # number of nodes\n#SBATCH --ntasks-per-node=40  # 40 processor core(s) per node X 2 threads per core\n#SBATCH --mem=200G  # maximum memory per node\n#SBATCH --partition=short  # standard node(s)\n#SBATCH --job-name=\"10T_anchorwaveV2\"\n#SBATCH --mail-user=lcj34@cornell.edu  # email address\n#SBATCH --mail-type=BEGIN\n#SBATCH --mail-type=END\n#SBATCH --mail-type=FAIL\n#SBATCH --array=0-4\n# LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE\nmodule load miniconda\nmodule load java/17\n\neval \"$(conda shell.bash hook)\"\n\nconda activate /project/buckler_lab_panand/lynn.johnson/phgv2-conda\n\necho \"All jobs in this array have:\"\necho \"- SLURM array job id: ${SLURM_ARRAY_JOB_ID}\"\necho \"- SLURM array task count: ${SLURM_ARRAY_TASK_COUNT}\"\necho \"- SLURM array starting task: ${SLURM_ARRAY_TASK_MIN}\"\necho \"- SLURM array ending task: ${SLURM_ARRAY_TASK_MAX}\"\necho \"This job in the array has:\"\necho \"- SLURM job id: ${SLURM_JOB_ID}\"\necho \"- SLURM array task id: ${SLURM_ARRAY_TASK_ID}\"\n\nINPUTFILE=&lt;your_align_command_input&gt;\n\nIFS=$'\\n' read -d '' -r -a LINES &lt; ${INPUTFILE}\nLINE=${LINES[$SLURM_ARRAY_TASK_ID]}\neval ${LINE}\nif [ $? -eq 0 ]\n  then\n    echo -e \"$(date +\"%D  %r\")\\tSuccess: ${LINE}\"\n    exit 0\n  else\n    echo -e \"$(date +\"%D  %r\")\\tFailed\\t${LINE}\"\n    echo -e \"$(date +\"%D  %r\")\\tJobID\\t${SLURM_JOB_ID}\"\n    echo -e \"$(date +\"%D  %r\")\\tTaskID\\t${SLURM_ARRAY_TASK_ID}\"\n  exit 1\nfi\n</code></pre> <p>...where the line that contains the <code>INPUTFILE</code> variable declaration is the path to the <code>prepare-slurm-align-file</code> command output (in our case, this would be <code>output/slurm_align_file.txt</code>). For more information about how to get started with SLURM, please check out the official guides.</p>"},{"location":"terminology/","title":"PHGv2 Terminology","text":""},{"location":"terminology/#general-terms","title":"General terms","text":"Term Definition Reference genome genome used for initial alignment and base coordinates Reference range segment of the reference genome Haplotype sequence of part of an individual chromosome with its start and stop defined by the reference range Reference Haplotype haplotype from the reference genome Alternate genome high quality genomes used to identify alternate haplotypes Alternate haplotype haplotype derived from a genome assembly Composite genome inferred genome based on its composite set of alternate and reference haplotypes Haplotype ID MD5 checksum for the haplotype sequence Sample genotype (haploid or diploid or higher), taxon, individual Path phased set of haplotype ids through the pangenome graph"},{"location":"terminology/#file-types","title":"File types","text":"File Type Acronym definition Usage <code>.agc</code> Assembled Genomes Compressor Efficient genome sequence compression. <code>.bam</code> Binary Alignment Map Binary representation of a SAM file; useful for efficient processing. <code>.bed</code> Browser Extensible Data Genomic feature coordinate (e.g. reference ranges) storage. <code>.bcf</code> Binary Call Format Binary representation of a VCF file; useful for efficient processing. <code>.fasta</code> FAST-All Sequence representation and storage. <code>.g.VCF</code> genomic VCF file Variant and non-variant genomic storage. <code>.h.VCF</code> haplotype VCF file Haplotype information representation and storage. More information can be found here. <code>.maf</code> Multiple Alignment Format Multiple alignment storage; basis for gVCF and hVCF creation. <code>.sam</code> Sequence Alignment Map Sequence alignment to a reference sequence. <code>.vcf</code> Variant Call Format Genetic variant representation and storage."},{"location":"terminology/#software","title":"Software","text":"Software Purpose agc Performant FASTA genome compression AnchorWave Sensitive aligner for genomes with high sequence diversity bcftools Utilities for indexing VCF data samtools bgzip compression for VCF data TileDB Performant storage core for array data TileDB-VCF API for storing and querying VCF data"},{"location":"variant_comparisons/","title":"Variant Caller Comparisons","text":"<p>Currently, we have investigated 3 programs for calling variants from  BAM alignment files:</p> <ul> <li>DeepVariant</li> <li>Octopus</li> <li>Clair3</li> </ul> <p>While all 3 had their merits, we found DeepVariant and Octopus were more suited to the short-read variant calling that is often used  with PhgV2. We found DeepVariant easier to set up than Octopus, but  was less flexible in terms of parameters. We had difficulty pulling  Octopus forest models,  but found when running in the individual  calling model, they were not necessary. Additionally, we discovered  that leveraging Docker environments for these programs was more  efficient to set up than trying to link with a Conda-based  environment.</p> <p>Pulling from our experience and web comparisons, here is a synopsis  of each tool:</p> Feature DeepVariant Octopus Clair3 Accuracy High (short and long reads) High (especially in somatic) High (especially in long reads) Use Cases Germline variant calling Germline &amp; somatic variant calling Long-read variant calling Performance Best with GPUs, can be slow otherwise Fast even without GPU Efficient with long reads Ease of Use Easy but less customizable Flexible but complex setup Good for long-read, easier setup Supported Data Types Short &amp; long-read data Short &amp; long-read data Long-read focused Computational Requirements High if using GPUs Moderate Moderate (especially with GPUs) <p>For more in-depth notes, please take a look at the proceeding sections:</p>"},{"location":"variant_comparisons/#deepvariant","title":"DeepVariant","text":""},{"location":"variant_comparisons/#strengths","title":"Strengths","text":"<ul> <li>Accuracy: DeepVariant uses deep learning to call variants,    which leads to high accuracy in calling both SNPs and indels.</li> <li>Performance across platforms: Works well across different    sequencing technologies like Illumina, PacBio, and Oxford Nanopore,    making it versatile.</li> <li>Ease of Use: Pre-trained models for different platforms are    available, reducing the need for parameter tuning or custom    training.</li> <li>Automation: It is relatively easy to set up, and its automated    pipeline makes variant calling straightforward.</li> <li>Active Development: Continuous improvements are made by Google    and the open-source community.</li> </ul>"},{"location":"variant_comparisons/#weaknesses","title":"Weaknesses","text":"<ul> <li>Computational Resources: DeepVariant requires significant    computational resources, especially GPUs, to achieve optimal    performance. However, Buckler Lab was able to run successfully    without GPUs on a high performance computing machine.</li> <li>Limited Customizability: Since it's a pre-trained deep learning    model, there\u2019s less flexibility for users who want to tweak    algorithms or tailor the caller to niche applications.</li> <li>Not tailored for somatic mutation calling: It's more optimized    for germline variant calling, so may not perform as well on somatic    mutations compared to specialized callers.</li> </ul>"},{"location":"variant_comparisons/#octopus","title":"Octopus","text":""},{"location":"variant_comparisons/#strengths_1","title":"Strengths","text":"<ul> <li>Versatility: Octopus supports both germline and somatic variant    calling, making it suitable for diverse research applications.</li> <li>Haplotype-aware: It uses haplotype assembly for variant calling,    which improves accuracy, particularly in complex regions of the    genome.</li> <li>Speed: Compared to DeepVariant, Octopus is typically faster and    more computationally efficient, especially on multi-core machines.</li> <li>Supports multiple data types: Works with short reads and    long-read data, making it adaptable for mixed or evolving    sequencing approaches.</li> </ul>"},{"location":"variant_comparisons/#weaknesses_1","title":"Weaknesses","text":"<ul> <li>Less mature: While Octopus is powerful, it may not have as    large of a user base or community as DeepVariant or Clair3, meaning    less documentation or community support.</li> <li>Complex setup: Configuration and usage may be more complex due    to the high degree of flexibility, and tuning parameters for    specific datasets could be a challenge.</li> <li>Limited GPU support: It doesn\u2019t fully leverage GPUs for    acceleration, so it might not be as fast as GPU-accelerated tools    like DeepVariant when working on large datasets.</li> </ul>"},{"location":"variant_comparisons/#clair3","title":"Clair3","text":""},{"location":"variant_comparisons/#strengths_2","title":"Strengths","text":"<ul> <li>Accuracy for Long Reads: Clair3 is particularly designed to    excel with long-read sequencing data (e.g., PacBio HiFi, Oxford    Nanopore), showing superior performance in variant calling for such    data types.</li> <li>Phasing and Indel Detection: Clair3 shows good performance in    phasing and in detecting small indels, which are often problematic    for short-read focused callers.</li> <li>Deep Learning-based: Similar to DeepVariant, Clair3 also uses a    neural network model, but it has been particularly optimized for    the complexities of long-read data.</li> <li>Low Computational Cost: It\u2019s more computationally efficient    compared to DeepVariant, especially for long-read data, which makes    it faster while retaining accuracy.</li> </ul>"},{"location":"variant_comparisons/#weaknesses_2","title":"Weaknesses","text":"<ul> <li>Limited Short-Read Support: Clair3 is designed for long-read    sequencing and may not be as well-suited for short-read data, which    can make it less versatile than DeepVariant.</li> <li>Community Size: Clair3 is newer and has a smaller user base    than DeepVariant, so there may be less widespread support or    examples of usage in various research contexts.</li> <li>Requires specialized hardware: While not as resource-heavy as    DeepVariant, Clair3 still benefits from high-end hardware (e.g.,    GPUs) to maximize performance.</li> </ul>"},{"location":"variant_comparisons/#example-code-usage","title":"Example code usage","text":"DeepVariantOctopusClair3 <pre><code># Run DeepVariant from Docker, filter for chr9\nINPUT_DIR=/workdir/lcj34/deepVariant/p39toCompositeBamFastas\nOUTPUT_DIR=/workdir/lcj34/deepVariant/p39toComposite_dvOutput\n\ndocker run \\\n    -v ${INPUT_DIR}/:/input/ \\\n    -v ${OUTPUT_DIR}/:/output/ \\\n    google/deepvariant:1.6.1 \\\n    /opt/deepvariant/bin/run_deepvariant \\\n    --model_type=WGS \\\n    --ref=/input/P39wgs_composite.fa \\\n    --reads=/input/P39toCompositeMergedBams.bam \\\n    --regions \"chr9\" \\\n    --output_vcf=/output/p39Compositechr9_output.vcf.gz \\\n    --output_gvcf=/output/p39Compositechr9_output.g.vcf.gz \\\n    --intermediate_results_dir /output/intermediate_results \\\n    --num_shards=1\n</code></pre> <pre><code># Run Octopus from Docker, filter for chr9\nINPUT_DIR=/workdir/lcj34/octopus/input\nOUTPUT_DIR=/workdir/lcj34/octopus/output\n\ndocker run \\\n    -v ${INPUT_DIR}/:/input/ \\\n    -v ${OUTPUT_DIR}/:/output/ \\\n    dancooke/octopus:latest \\\n    octopus \\\n    -R /input/P39.fa \\\n    -I /input/P39toB73MergedBams.bam \\\n    -T chr9 \\\n    --sequence-error-model PCRF.X10 \\\n    --forest /opt/octopus/resources/forests/germline.v0.8.0.forest \\\n    -o /output/p39b73chr9_octopus.vcf.gz \\\n    --threads=16\n</code></pre> <pre><code># Run Octopus from Docker, filter for chr9\nINPUT_DIR=/workdir/lcj34/octopus/input\nOUTPUT_DIR=/workdir/lcj34/octopus/output\n\ndocker run \\\n    -v ${INPUT_DIR}/:/input/ \\\n    -v ${OUTPUT_DIR}/:/output/ \\\n    dancooke/octopus:latest \\\n    octopus \\\n    -R /input/P39.fa \\\n    -I /input/P39toB73MergedBams.bam \\\n    -T chr9 \\\n    --sequence-error-model PCRF.X10 \\\n    --forest /opt/octopus/resources/forests/germline.v0.8.0.forest \\\n    -o /output/p39b73chr9_octopus.vcf.gz \\\n    --threads=16\n</code></pre>"}]}